{
  "dataset_columns": [
    "Country",
    "Region",
    "Population",
    "Area (sq. mi.)",
    "Pop. Density (per sq. mi.)",
    "Coastline (coast/area ratio)",
    "Net migration",
    "Infant mortality (per 1000 births)",
    "GDP ($ per capita)",
    "Literacy (%)",
    "Phones (per 1000)",
    "Arable (%)",
    "Crops (%)",
    "Other (%)",
    "Climate",
    "Birthrate",
    "Deathrate",
    "Agriculture",
    "Industry",
    "Service"
  ],
  "questions": [
    {
      "question": "Which numeric feature in the dataset is the most skewed, and what are its mean and 95% confidence interval obtained via bootstrapping? Return as JSON with keys: column, skewness, mean, ci_lower, ci_upper, std_error, n_bootstrap.",
      "hint": "Identify numeric columns, compute absolute skewness for each, pick the highest, then bootstrap its values to estimate the mean, confidence interval, and standard error.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "bootstrap_ci_discovered",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (name of most skewed column), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (lower bound of 95% CI, rounded to 4 decimals), \"ci_upper\" (upper bound, rounded to 4 decimals), \"std_error\" (bootstrap standard error, rounded to 4 decimals), and \"n_bootstrap\" (integer, the number of bootstrap samples). Example: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}",
      "ground_truth_hash": "03c747c592ed46aa",
      "_ground_truth": {
        "column": "Population",
        "skewness": 9.2002,
        "mean": 28740284.3656,
        "ci_lower": 15377176.7744,
        "ci_upper": 46084893.1061,
        "std_error": 7857220.6309,
        "n_bootstrap": 1000
      },
      "_template": "bootstrap_ci_discovered"
    },
    {
      "question": "Which numeric variable most strongly predicts the most variable measurement in the dataset? Return as JSON with keys: target, best_predictor, correlation, r_squared, coefficient, p_value.",
      "hint": "Find the numeric column with highest variance, compute absolute correlations with other numeric columns, select the strongest, fit a simple OLS regression and report the stats.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "regression_most_predictive",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"best_predictor\" (column name), \"correlation\" (rounded to 3 decimals), \"r_squared\" (rounded to 4 decimals), \"coefficient\" (rounded to 4 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target\": \"price\", \"best_predictor\": \"sqft\", \"correlation\": 0.834, \"r_squared\": 0.6956, \"coefficient\": 135.2847, \"p_value\": 0.000001}",
      "ground_truth_hash": "ada972bdd5600cb4",
      "_ground_truth": {
        "target": "Population",
        "best_predictor": "Area (sq. mi.)",
        "correlation": 0.47,
        "r_squared": 0.2209,
        "coefficient": 30.9488,
        "p_value": 0.0
      },
      "_template": "regression_most_predictive"
    },
    {
      "question": "Is the numeric feature with the greatest variance distributed differently between two groups formed by a binary split of another variable? Return as JSON with keys: target_column, grouping_column, group1, group2, mean1, mean2, t_statistic, p_value, significant.",
      "hint": "Find the numeric column with the highest variance, create a binary grouping (e.g., based on median of another column), then compare the group means with a t\u2011test.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "ttest_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"mean1\" (rounded to 4 decimals), \"mean2\" (rounded to 4 decimals), \"t_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"significant\" (boolean, true if p < 0.05). Example: {\"target_column\": \"score\", \"grouping_column\": \"gender\", \"group1\": \"M\", \"group2\": \"F\", \"mean1\": 75.4321, \"mean2\": 78.1234, \"t_statistic\": -2.3456, \"p_value\": 0.019234, \"significant\": true}",
      "ground_truth_hash": "a096231e2515fc9b",
      "_ground_truth": {
        "target_column": "Population",
        "grouping_column": "_binary_group",
        "group1": "high",
        "group2": "low",
        "mean1": 55272245.292,
        "mean2": 2441059.9386,
        "t_statistic": 3.4567,
        "p_value": 0.000654,
        "significant": "True"
      },
      "_template": "ttest_discovered_groups"
    },
    {
      "question": "Which numeric variable in the country dataset appears most normally distributed? Return as JSON with keys: column, ks_statistic, p_value, is_normal, sample_mean, sample_std.",
      "hint": "Find the numeric column with the smallest absolute skewness, compute its mean and std, standardize the data, and run a Kolmogorov\u2011Smirnov test against a standard normal distribution.",
      "n_steps": 7,
      "difficulty": "MEDIUM",
      "template_name": "ks_normality_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (tested column), \"ks_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"is_normal\" (boolean, true if p >= 0.05), \"sample_mean\" (rounded to 4 decimals), and \"sample_std\" (rounded to 4 decimals). Example: {\"column\": \"height\", \"ks_statistic\": 0.0456, \"p_value\": 0.234567, \"is_normal\": true, \"sample_mean\": 170.1234, \"sample_std\": 10.5678}",
      "ground_truth_hash": "7fd3219406b3ed9b",
      "_ground_truth": {
        "column": "GDP ($ per capita)",
        "ks_statistic": 0.1852,
        "p_value": 0.0,
        "is_normal": "False",
        "sample_mean": 9689.823,
        "sample_std": 10049.1385
      },
      "_template": "ks_normality_test"
    },
    {
      "question": "Do the variances of the most variable numeric measurement differ across climate categories? Return as JSON with keys: target_column, grouping_column, n_groups, levene_statistic, p_value, variances_equal, group_variances.",
      "hint": "Identify the numeric column with the highest variance, select a categorical column with 2\u20116 distinct values, compute each group's variance, and run Levene's test for equality of variances.",
      "n_steps": 7,
      "difficulty": "HARD",
      "template_name": "levene_variance_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"levene_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"variances_equal\" (boolean, true if p >= 0.05), and \"group_variances\" (dict mapping group names to variance rounded to 4 decimals). Example: {\"target_column\": \"score\", \"grouping_column\": \"class\", \"n_groups\": 3, \"levene_statistic\": 2.3456, \"p_value\": 0.098765, \"variances_equal\": true, \"group_variances\": {\"A\": 123.4567, \"B\": 145.6789, \"C\": 112.3456}}",
      "ground_truth_hash": "cf515345b33b98ee",
      "_ground_truth": {
        "target_column": "Population",
        "grouping_column": "Climate",
        "n_groups": 6,
        "levene_statistic": 9.5592,
        "p_value": 0.0,
        "variances_equal": "False",
        "group_variances": {
          "1": 1173026347819977.0,
          "1,5": 2.0266062545354634e+17,
          "2": 1241741130242716.5,
          "2,5": 3.9762093497277037e+17,
          "3": 2294400256142744.0,
          "4": 520620083246153.8
        }
      },
      "_template": "levene_variance_test"
    },
    {
      "question": "Is there a significant difference in population sizes between two groups formed by splitting another numeric variable at its median? Return as JSON with keys: target_column, grouping_column, group1, group2, median1, median2, u_statistic, p_value.",
      "hint": "Find the most skewed numeric column to use as the target, create a binary grouping column (or use an existing two\u2011level categorical), then compare the groups with a Mann\u2011Whitney U test.",
      "n_steps": 6,
      "difficulty": "MEDIUM",
      "template_name": "mann_whitney_u_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 8 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"median1\" (rounded to 4 decimals), \"median2\" (rounded to 4 decimals), \"u_statistic\" (rounded to 2 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target_column\": \"score\", \"grouping_column\": \"treatment\", \"group1\": \"control\", \"group2\": \"experimental\", \"median1\": 72.5000, \"median2\": 78.0000, \"u_statistic\": 1234.50, \"p_value\": 0.034567}",
      "ground_truth_hash": "a9d93d57a5571043",
      "_ground_truth": {
        "target_column": "Population",
        "grouping_column": "_binary_group",
        "group1": "high",
        "group2": "low",
        "median1": 16134219.0,
        "median2": 513319.5,
        "u_statistic": 11761.0,
        "p_value": 0.0
      },
      "_template": "mann_whitney_u_test"
    },
    {
      "question": "Which two numeric variables in this country dataset exhibit the strongest monotonic relationship? Return as JSON with keys: columns, spearman_rho, p_value, interpretation.",
      "hint": "Compute Spearman correlations for all numeric columns, find the pair with the highest absolute correlation, and report the rho, p\u2011value, and strength interpretation.",
      "n_steps": 6,
      "difficulty": "MEDIUM",
      "template_name": "spearman_rank_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"spearman_rho\" (correlation coefficient rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"interpretation\" (string: \"strong\", \"moderate\", \"weak\", or \"negligible\"). Example: {\"columns\": [\"age\", \"income\"], \"spearman_rho\": 0.7234, \"p_value\": 0.000001, \"interpretation\": \"strong\"}",
      "ground_truth_hash": "949c093a7cb45210",
      "_ground_truth": {
        "columns": [
          "Area (sq. mi.)",
          "Population"
        ],
        "spearman_rho": 0.8274,
        "p_value": 0.0,
        "interpretation": "strong"
      },
      "_template": "spearman_rank_correlation"
    },
    {
      "question": "Which two most skewed numeric variables have a stronger Pearson correlation after applying a log transformation? Return as JSON with keys: columns, original_correlation, log_correlation, improvement, transformation_helpful.",
      "hint": "Find the two numeric columns with the highest absolute skewness, compute their Pearson correlation on the raw data, then log\u2011transform both columns (e.g., log1p) and recompute the correlation to see if it improves.",
      "n_steps": 6,
      "difficulty": "HARD",
      "template_name": "correlation_change_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 5 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (Pearson, rounded to 4 decimals), \"log_correlation\" (after log transform, rounded to 4 decimals), \"improvement\" (absolute difference, rounded to 4 decimals), and \"transformation_helpful\" (boolean, true if |log_corr| > |orig_corr|). Example: {\"columns\": [\"income\", \"spending\"], \"original_correlation\": 0.4567, \"log_correlation\": 0.7234, \"improvement\": 0.2667, \"transformation_helpful\": true}",
      "ground_truth_hash": "4388489315e142a2",
      "_ground_truth": {
        "columns": [
          "Area (sq. mi.)",
          "Population"
        ],
        "original_correlation": 0.47,
        "log_correlation": 0.8315,
        "improvement": 0.3615,
        "transformation_helpful": "True"
      },
      "_template": "correlation_change_analysis"
    },
    {
      "question": "Which numeric column in the dataset has the largest variance and what are its Q1, Q3, IQR, lower and upper fences, and outlier count? Return as JSON with keys: column, q1, q3, iqr, lower_fence, upper_fence, n_outliers.",
      "hint": "Identify the numeric column with the highest variance, then compute its quartiles, inter\u2011quartile range, fences (1.5\u202f\u00d7\u202fIQR), and count observations outside the fences.",
      "n_steps": 5,
      "difficulty": "EASY",
      "template_name": "iqr_outlier_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (analyzed column), \"q1\" (25th percentile rounded to 4 decimals), \"q3\" (75th percentile rounded to 4 decimals), \"iqr\" (rounded to 4 decimals), \"lower_fence\" (rounded to 4 decimals), \"upper_fence\" (rounded to 4 decimals), and \"n_outliers\" (integer count). Example: {\"column\": \"salary\", \"q1\": 45000.0000, \"q3\": 75000.0000, \"iqr\": 30000.0000, \"lower_fence\": 0.0000, \"upper_fence\": 120000.0000, \"n_outliers\": 23}",
      "ground_truth_hash": "67e5ff1134241e9c",
      "_ground_truth": {
        "column": "Population",
        "q1": 437624.0,
        "q3": 17497772.5,
        "iqr": 17060148.5,
        "lower_fence": -25152598.75,
        "upper_fence": 43087995.25,
        "n_outliers": 28
      },
      "_template": "iqr_outlier_analysis"
    },
    {
      "question": "Which numeric feature varies the most across countries and does its distribution appear normal? Return as JSON with keys: distribution, median, iqr (or mean, std if normal).",
      "hint": "Identify the numeric column with the highest variance, perform a Shapiro\u2011Wilk test for normality, then report the appropriate summary statistics.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "conditional_normality",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 3 keys. If normal: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}. If non-normal: {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. The \"iqr\" value is Q3 minus Q1 as a single number. All numeric values rounded to 3 decimal places.",
      "ground_truth_hash": "622725fc73e91025",
      "_ground_truth": {
        "distribution": "non-normal",
        "median": 4786994.0,
        "iqr": 17060148.5
      },
      "_template": "conditional_normality"
    },
    {
      "question": "Which geographic grouping shows the largest average number of inhabitants? Return as JSON with keys: category_column, best_category, target_column, mean_value.",
      "hint": "Identify the numeric column with the highest variance, compute its mean for each categorical group, and report the group with the highest mean.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "category_highest_target_mean",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"category_column\" (the grouping column name), \"best_category\" (the category value with highest mean), \"target_column\" (the numeric column analyzed), and \"mean_value\" (rounded to 3 decimal places). Example: {\"category_column\": \"region\", \"best_category\": \"West\", \"target_column\": \"sales\", \"mean_value\": 1234.567}",
      "ground_truth_hash": "5aa02c2dff73bb75",
      "_ground_truth": {
        "category_column": "Region",
        "best_category": "ASIA (EX. NEAR EAST)         ",
        "target_column": "Population",
        "mean_value": 131713651.286
      },
      "_template": "category_highest_target_mean"
    },
    {
      "question": "Which two numeric variables in the country dataset have the strongest correlation, and how does that relationship change after removing extreme outliers? Return as JSON with keys: columns, original_correlation, outliers_removed, clean_correlation.",
      "hint": "Calculate absolute Pearson correlations for all numeric columns, pick the pair with the highest value, drop rows where either column is more than three standard deviations from its mean, then recompute the correlation.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "bcc1d237c89ab6be",
      "_ground_truth": {
        "columns": [
          "Area (sq. mi.)",
          "Population"
        ],
        "original_correlation": 0.47,
        "outliers_removed": 7,
        "clean_correlation": 0.446
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "I wonder how many numeric variables contain extreme values and how many outliers exist in total. Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "Use a 3\u2011standard\u2011deviation rule on each numeric column to flag outliers, then count columns with any and sum all outlier occurrences.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "b0974b64b49df39e",
      "_ground_truth": {
        "columns_with_outliers": 3,
        "total_outliers": 9
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "How many numeric variables have outliers and what is the total count of outlier values? Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "For each numeric column, flag values that lie beyond 2.5 standard deviations from the mean, then tally columns with any outliers and the overall outlier count.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "7e255242c0b0d351",
      "_ground_truth": {
        "columns_with_outliers": 3,
        "total_outliers": 13
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "Which numeric attribute in this country dataset shows the greatest relative variability? Return as JSON with keys: column, cv, mean, std.",
      "hint": "Calculate the coefficient of variation (std/mean * 100) for each numeric column and identify the one with the highest value.",
      "n_steps": 4,
      "difficulty": "EASY",
      "template_name": "coefficient_of_variation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"column\" (name of column with highest CV), \"cv\" (coefficient of variation as percentage rounded to 2 decimals), \"mean\" (rounded to 4 decimals), and \"std\" (rounded to 4 decimals). Example: {\"column\": \"price\", \"cv\": 45.23, \"mean\": 1234.5678, \"std\": 558.4567}",
      "ground_truth_hash": "a8d153bbf9ff1625",
      "_ground_truth": {
        "column": "Population",
        "cv": 410.2,
        "mean": 28740284.3656,
        "std": 117891326.5435
      },
      "_template": "coefficient_of_variation"
    },
    {
      "question": "Which two numeric variables in this dataset are most strongly related? Return as JSON with keys: columns, correlation.",
      "hint": "Compute the absolute correlation matrix of numeric columns and identify the pair with the highest off\u2011diagonal correlation.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "1b6322838ef0b286",
      "_ground_truth": {
        "columns": [
          "Area (sq. mi.)",
          "Population"
        ],
        "correlation": 0.47
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Which two numeric variables in this dataset are the least linearly related? Return as JSON with keys: columns, correlation.",
      "hint": "Calculate the absolute correlation matrix for all numeric columns and identify the pair with the smallest non\u2011zero correlation.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "5d3de179b8a9c402",
      "_ground_truth": {
        "columns": [
          "GDP ($ per capita)",
          "Population"
        ],
        "correlation": 0.039
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "Which numeric column varies the most in this dataset, and what is its average value? Return as JSON with keys: mean.",
      "hint": "Identify the column with the highest variance, then compute its mean.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "cb67f8d823aea6d5",
      "_ground_truth": 28740284.366,
      "_template": "max_variance_mean"
    },
    {
      "question": "What is the standard deviation of the numeric variable that has the lowest mean? Return as JSON with key: result.",
      "hint": "Select numeric columns, find the one with the smallest mean, then compute its standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "6199bc1b44709e7d",
      "_ground_truth": 10049.139,
      "_template": "min_mean_column_std"
    },
    {
      "question": "Which columns in this dataset have a high proportion of missing values (over 5%)? Return as JSON with keys: count, columns.",
      "hint": "Calculate the percentage of missing entries per column and list those exceeding the 5% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "8bab36f3c6412f56",
      "_ground_truth": {
        "count": 5,
        "columns": [
          "Agriculture",
          "Climate",
          "Industry",
          "Literacy (%)",
          "Service"
        ]
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "I wonder which columns have a large proportion of missing values; are any above a 10% threshold? Return as JSON with keys: count, columns.",
      "hint": "Calculate missing percentages for each column, filter those >10%, count them and list their names alphabetically.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Which numeric variable in the dataset spans the widest range, and what are its key percentile values? Return as JSON with keys: column, p10, p25, p50, p75, p90.",
      "hint": "Identify the numeric column with the largest max\u2011min difference, then compute its 10th, 25th, 50th, 75th and 90th percentiles.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "percentile_ranking",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (analyzed column), \"p10\" (10th percentile rounded to 4 decimals), \"p25\" (25th percentile), \"p50\" (median), \"p75\" (75th percentile), and \"p90\" (90th percentile). Example: {\"column\": \"income\", \"p10\": 25000.0000, \"p25\": 40000.0000, \"p50\": 55000.0000, \"p75\": 75000.0000, \"p90\": 100000.0000}",
      "ground_truth_hash": "949552a0a3b97021",
      "_ground_truth": {
        "column": "Population",
        "p10": 67655.2,
        "p25": 437624.0,
        "p50": 4786994.0,
        "p75": 17497772.5,
        "p90": 52561497.4
      },
      "_template": "percentile_ranking"
    },
    {
      "question": "Which numeric attribute in the dataset varies the most, and what are its key descriptive statistics? Return as JSON with keys: column, count, mean, std, min, max, median, skewness, kurtosis.",
      "hint": "Identify the numeric column with the highest variance, then compute its count, mean, standard deviation, min, max, median, skewness, and kurtosis.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "descriptive_summary",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"column\", \"count\" (integer), \"mean\" (rounded to 4 decimals), \"std\", \"min\", \"max\", \"median\", \"skewness\", and \"kurtosis\" (all rounded to 4 decimals). Example: {\"column\": \"age\", \"count\": 1000, \"mean\": 35.4567, \"std\": 12.3456, \"min\": 18.0000, \"max\": 85.0000, \"median\": 34.0000, \"skewness\": 0.5678, \"kurtosis\": -0.2345}",
      "ground_truth_hash": "acae3f23eb11388e",
      "_ground_truth": {
        "column": "Population",
        "count": 227,
        "mean": 28740284.3656,
        "std": 117891326.5435,
        "min": 7026.0,
        "max": 1313973713.0,
        "median": 4786994.0,
        "skewness": 9.2002,
        "kurtosis": 91.8057
      },
      "_template": "descriptive_summary"
    }
  ]
}