{
  "dataset_columns": [
    "Store ID ",
    "Store_Area",
    "Items_Available",
    "Daily_Customer_Count",
    "Store_Sales"
  ],
  "questions": [
    {
      "question": "Which numeric measurement that shows the greatest variability can be explained by other store metrics? Return as JSON with keys: target, predictors, r_squared, adj_r_squared, n_significant, coefficients, p_values.",
      "hint": "Pick the numeric column with the highest variance as the target, find the three most correlated numeric columns, fit an OLS regression, and report R\u2011squared, adjusted R\u2011squared, significant predictors, coefficients and p\u2011values.",
      "n_steps": 10,
      "difficulty": "VERY_HARD",
      "template_name": "multiple_regression_top_predictors",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"predictors\" (list of 3 column names), \"r_squared\" (rounded to 4 decimals), \"adj_r_squared\" (rounded to 4 decimals), \"n_significant\" (integer count of predictors with p < 0.05), \"coefficients\" (dict mapping predictor names to coefficients rounded to 4 decimals), and \"p_values\" (dict mapping predictor names to p-values rounded to 6 decimals). Example: {\"target\": \"price\", \"predictors\": [\"sqft\", \"bedrooms\", \"age\"], \"r_squared\": 0.7234, \"adj_r_squared\": 0.7156, \"n_significant\": 2, \"coefficients\": {\"sqft\": 123.45, \"bedrooms\": 5000.12, \"age\": -200.34}, \"p_values\": {\"sqft\": 0.000001, \"bedrooms\": 0.023456, \"age\": 0.156789}}",
      "ground_truth_hash": "f374915ffc72fbfa",
      "_ground_truth": {
        "target": "Store_Sales",
        "predictors": [
          "Items_Available",
          "Store_Area",
          "Store ID "
        ],
        "r_squared": 0.0162,
        "adj_r_squared": 0.0129,
        "n_significant": 1,
        "coefficients": {
          "Items_Available": 32.6156,
          "Store_Area": -32.0397,
          "Store ID ": 5.0351
        },
        "p_values": {
          "Items_Available": 0.420968,
          "Store_Area": 0.509505,
          "Store ID ": 0.023197
        }
      },
      "_template": "multiple_regression_top_predictors"
    },
    {
      "question": "Which numeric column in the store dataset shows the greatest skewness, and what are its mean, 95% confidence interval, and bootstrap standard error? Return as JSON with keys: column, skewness, mean, ci_lower, ci_upper, std_error, n_bootstrap.",
      "hint": "Calculate absolute skewness for each numeric column, select the highest, then perform bootstrap resampling of that column to estimate the mean, its confidence interval, and standard error.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "bootstrap_ci_discovered",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (name of most skewed column), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (lower bound of 95% CI, rounded to 4 decimals), \"ci_upper\" (upper bound, rounded to 4 decimals), \"std_error\" (bootstrap standard error, rounded to 4 decimals), and \"n_bootstrap\" (integer, the number of bootstrap samples). Example: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}",
      "ground_truth_hash": "7024bc0f40bd0566",
      "_ground_truth": {
        "column": "Store_Sales",
        "skewness": 0.1488,
        "mean": 59351.3058,
        "ci_lower": 58240.9403,
        "ci_upper": 60595.8803,
        "std_error": 580.8914,
        "n_bootstrap": 1000
      },
      "_template": "bootstrap_ci_discovered"
    },
    {
      "question": "Which numeric variable best predicts the most variable outcome in the store data? Return as JSON with keys: target, best_predictor, correlation, r_squared, coefficient, p_value.",
      "hint": "Identify the column with the highest variance, compute absolute correlations with other numeric columns, pick the strongest, and fit a simple OLS regression to report the stats.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "regression_most_predictive",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"best_predictor\" (column name), \"correlation\" (rounded to 3 decimals), \"r_squared\" (rounded to 4 decimals), \"coefficient\" (rounded to 4 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target\": \"price\", \"best_predictor\": \"sqft\", \"correlation\": 0.834, \"r_squared\": 0.6956, \"coefficient\": 135.2847, \"p_value\": 0.000001}",
      "ground_truth_hash": "17f76c327e3c1813",
      "_ground_truth": {
        "target": "Store_Sales",
        "best_predictor": "Items_Available",
        "correlation": 0.099,
        "r_squared": 0.0098,
        "coefficient": 5.6667,
        "p_value": 0.003056
      },
      "_template": "regression_most_predictive"
    },
    {
      "question": "I wonder whether the most variable numeric measurement in the store dataset shows a significant difference between two naturally formed groups. Return as JSON with keys: target_column, grouping_column, group1, group2, mean1, mean2, t_statistic, p_value, significant.",
      "hint": "Identify the numeric column with highest variance, create a binary grouping (e.g., median split of another numeric column), then compare the groups' means with a t-test.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "ttest_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"mean1\" (rounded to 4 decimals), \"mean2\" (rounded to 4 decimals), \"t_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"significant\" (boolean, true if p < 0.05). Example: {\"target_column\": \"score\", \"grouping_column\": \"gender\", \"group1\": \"M\", \"group2\": \"F\", \"mean1\": 75.4321, \"mean2\": 78.1234, \"t_statistic\": -2.3456, \"p_value\": 0.019234, \"significant\": true}",
      "ground_truth_hash": "66f25bcab81f325e",
      "_ground_truth": {
        "target_column": "Store_Sales",
        "grouping_column": "_binary_group",
        "group1": "low",
        "group2": "high",
        "mean1": 58446.5625,
        "mean2": 60256.0491,
        "t_statistic": -1.5767,
        "p_value": 0.115222,
        "significant": "False"
      },
      "_template": "ttest_discovered_groups"
    },
    {
      "question": "Which numeric variable in the store dataset most closely follows a normal distribution? Return as JSON with keys: column, ks_statistic, p_value, is_normal, sample_mean, sample_std.",
      "hint": "Calculate skewness for each numeric column, select the one with the smallest absolute skewness, standardize its values, and run a Kolmogorov\u2011Smirnov test against a standard normal distribution.",
      "n_steps": 7,
      "difficulty": "MEDIUM",
      "template_name": "ks_normality_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (tested column), \"ks_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"is_normal\" (boolean, true if p >= 0.05), \"sample_mean\" (rounded to 4 decimals), and \"sample_std\" (rounded to 4 decimals). Example: {\"column\": \"height\", \"ks_statistic\": 0.0456, \"p_value\": 0.234567, \"is_normal\": true, \"sample_mean\": 170.1234, \"sample_std\": 10.5678}",
      "ground_truth_hash": "d6a5235e8f096755",
      "_ground_truth": {
        "column": "Store ID ",
        "ks_statistic": 0.0576,
        "p_value": 0.004987,
        "is_normal": "False",
        "sample_mean": 448.5,
        "sample_std": 258.7972
      },
      "_template": "ks_normality_test"
    },
    {
      "question": "Which two numeric variables in the store data show the strongest monotonic relationship? Return as JSON with keys: columns, spearman_rho, p_value, interpretation.",
      "hint": "Compute the Spearman correlation matrix for all numeric columns, then find the pair with the highest absolute rho and report its value and p\u2011value.",
      "n_steps": 6,
      "difficulty": "MEDIUM",
      "template_name": "spearman_rank_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"spearman_rho\" (correlation coefficient rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"interpretation\" (string: \"strong\", \"moderate\", \"weak\", or \"negligible\"). Example: {\"columns\": [\"age\", \"income\"], \"spearman_rho\": 0.7234, \"p_value\": 0.000001, \"interpretation\": \"strong\"}",
      "ground_truth_hash": "04fb17d6870e2ee2",
      "_ground_truth": {
        "columns": [
          "Items_Available",
          "Store_Area"
        ],
        "spearman_rho": 0.9987,
        "p_value": 0.0,
        "interpretation": "strong"
      },
      "_template": "spearman_rank_correlation"
    },
    {
      "question": "Do the most skewed numeric variables show a stronger relationship after a log transformation? Return as JSON with keys: columns, original_correlation, log_correlation, improvement, transformation_helpful.",
      "hint": "Find the two numeric columns with highest absolute skewness, compute their Pearson correlation, apply a log1p transform to both, recompute the correlation, and compare the absolute values.",
      "n_steps": 6,
      "difficulty": "HARD",
      "template_name": "correlation_change_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 5 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (Pearson, rounded to 4 decimals), \"log_correlation\" (after log transform, rounded to 4 decimals), \"improvement\" (absolute difference, rounded to 4 decimals), and \"transformation_helpful\" (boolean, true if |log_corr| > |orig_corr|). Example: {\"columns\": [\"income\", \"spending\"], \"original_correlation\": 0.4567, \"log_correlation\": 0.7234, \"improvement\": 0.2667, \"transformation_helpful\": true}",
      "ground_truth_hash": "a1da2774c3945d8f",
      "_ground_truth": {
        "columns": [
          "Daily_Customer_Count",
          "Store_Sales"
        ],
        "original_correlation": 0.0086,
        "log_correlation": -0.0147,
        "improvement": 0.0061,
        "transformation_helpful": "True"
      },
      "_template": "correlation_change_analysis"
    },
    {
      "question": "Which numeric variable in the store dataset shows the greatest variability, and does it contain any extreme outliers? Return as JSON with keys: column, q1, q3, iqr, lower_fence, upper_fence, n_outliers.",
      "hint": "Find the column with the highest variance, compute its Q1, Q3, IQR, derive the lower and upper fences (1.5*IQR), and count points outside these fences.",
      "n_steps": 5,
      "difficulty": "EASY",
      "template_name": "iqr_outlier_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (analyzed column), \"q1\" (25th percentile rounded to 4 decimals), \"q3\" (75th percentile rounded to 4 decimals), \"iqr\" (rounded to 4 decimals), \"lower_fence\" (rounded to 4 decimals), \"upper_fence\" (rounded to 4 decimals), and \"n_outliers\" (integer count). Example: {\"column\": \"salary\", \"q1\": 45000.0000, \"q3\": 75000.0000, \"iqr\": 30000.0000, \"lower_fence\": 0.0000, \"upper_fence\": 120000.0000, \"n_outliers\": 23}",
      "ground_truth_hash": "08f7e7c2066ef40b",
      "_ground_truth": {
        "column": "Store_Sales",
        "q1": 46530.0,
        "q3": 71872.5,
        "iqr": 25342.5,
        "lower_fence": 8516.25,
        "upper_fence": 109886.25,
        "n_outliers": 1
      },
      "_template": "iqr_outlier_analysis"
    },
    {
      "question": "Which numeric column varies the most and how is its distribution characterized?",
      "hint": "Find the column with the highest variance, test for normality, then report median and IQR if non\u2011normal (or mean and std if normal).",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "conditional_normality",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 3 keys. If normal: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}. If non-normal: {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. The \"iqr\" value is Q3 minus Q1 as a single number. All numeric values rounded to 3 decimal places.",
      "ground_truth_hash": "db84757a9e97746e",
      "_ground_truth": {
        "distribution": "non-normal",
        "median": 58605.0,
        "iqr": 25342.5
      },
      "_template": "conditional_normality"
    },
    {
      "question": "Which two numeric variables in the store dataset are most strongly correlated, and does that relationship change after removing extreme outliers? Return as JSON with keys: columns, original_correlation, outliers_removed, clean_correlation.",
      "hint": "Compute the absolute correlation matrix, identify the highest pair, filter out rows beyond 3 standard deviations for both columns, then recompute the correlation.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "3cc0178449bd5aac",
      "_ground_truth": {
        "columns": [
          "Items_Available",
          "Store_Area"
        ],
        "original_correlation": 0.999,
        "outliers_removed": 0,
        "clean_correlation": 0.999
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "Are there any outlier values in the dataset, and how many columns contain them? Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "For each numeric column, compare values to the mean \u00b1 3\u202f\u00d7\u202fstd and count the points that fall outside.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "72dd2629edfa55e7",
      "_ground_truth": {
        "columns_with_outliers": 1,
        "total_outliers": 1
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "How many numeric variables have outliers and what is the total number of outlier values? Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "For each numeric column, flag values beyond 2.5\u202f\u00d7\u202fstd from the mean, then count affected columns and total flagged rows.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "c47f7574ed0294b0",
      "_ground_truth": {
        "columns_with_outliers": 4,
        "total_outliers": 26
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "Which numeric feature in the store dataset exhibits the greatest relative variability? Return as JSON with keys: column, cv, mean, std.",
      "hint": "Calculate the coefficient of variation (std/mean*100) for each numeric column and identify the one with the highest value.",
      "n_steps": 4,
      "difficulty": "EASY",
      "template_name": "coefficient_of_variation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"column\" (name of column with highest CV), \"cv\" (coefficient of variation as percentage rounded to 2 decimals), \"mean\" (rounded to 4 decimals), and \"std\" (rounded to 4 decimals). Example: {\"column\": \"price\", \"cv\": 45.23, \"mean\": 1234.5678, \"std\": 558.4567}",
      "ground_truth_hash": "a346316a0cb914f0",
      "_ground_truth": {
        "column": "Store ID ",
        "cv": 57.7,
        "mean": 448.5,
        "std": 258.7972
      },
      "_template": "coefficient_of_variation"
    },
    {
      "question": "Which two numeric variables in the store dataset show the strongest linear relationship? Return as JSON with keys: columns, correlation.",
      "hint": "Calculate the absolute correlation matrix for numeric columns and identify the largest off\u2011diagonal correlation.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "e629c3aafc39a1dc",
      "_ground_truth": {
        "columns": [
          "Items_Available",
          "Store_Area"
        ],
        "correlation": 0.999
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Which pair of numeric variables in the store dataset shows the weakest correlation? Return as JSON with keys: columns, correlation.",
      "hint": "Compute the absolute correlation matrix, ignore the diagonal, and locate the minimum non\u2011zero value.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "a7d7b2d9008a78d4",
      "_ground_truth": {
        "columns": [
          "Daily_Customer_Count",
          "Store_Sales"
        ],
        "correlation": 0.009
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "Which numeric feature shows the greatest variability across stores, and what is its overall mean? Return as JSON with key: mean.",
      "hint": "Find the column with the highest variance, then calculate its mean.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "82cc83b83a920894",
      "_ground_truth": 59351.306,
      "_template": "max_variance_mean"
    },
    {
      "question": "Which numeric feature in the stores dataset has the smallest mean, and what is its standard deviation? Return as JSON with keys: result.",
      "hint": "Identify the column with the lowest mean and then compute its standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "fc60402d1571843f",
      "_ground_truth": 258.797,
      "_template": "min_mean_column_std"
    },
    {
      "question": "Are there any columns in the dataset with a substantial amount of missing values? Return as JSON with keys: count, columns.",
      "hint": "Compute the percentage of missing values for each column and count those exceeding a chosen threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Are there any columns with a high proportion of missing values? Return as JSON with keys: count, columns.",
      "hint": "Compute the percentage of missing entries for each column and list those above a 10% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Which numeric column varies the most across stores, and what are its 10th, 25th, 50th, 75th, and 90th percentiles? Return as JSON with keys: column, p10, p25, p50, p75, p90.",
      "hint": "Identify the column with the largest range (max\u2011min) and then compute the specified quantiles.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "percentile_ranking",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (analyzed column), \"p10\" (10th percentile rounded to 4 decimals), \"p25\" (25th percentile), \"p50\" (median), \"p75\" (75th percentile), and \"p90\" (90th percentile). Example: {\"column\": \"income\", \"p10\": 25000.0000, \"p25\": 40000.0000, \"p50\": 55000.0000, \"p75\": 75000.0000, \"p90\": 100000.0000}",
      "ground_truth_hash": "9927480cfa921ca3",
      "_ground_truth": {
        "column": "Store_Sales",
        "p10": 37820.0,
        "p25": 46530.0,
        "p50": 58605.0,
        "p75": 71872.5,
        "p90": 82045.0
      },
      "_template": "percentile_ranking"
    },
    {
      "question": "Which numeric column in the store data shows the greatest variability, and what are its key descriptive statistics? Return as JSON with keys: column, count, mean, std, min, max, median, skewness, kurtosis.",
      "hint": "Identify the column with the highest variance, then compute its count, mean, standard deviation, min, max, median, skewness, and kurtosis.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "descriptive_summary",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"column\", \"count\" (integer), \"mean\" (rounded to 4 decimals), \"std\", \"min\", \"max\", \"median\", \"skewness\", and \"kurtosis\" (all rounded to 4 decimals). Example: {\"column\": \"age\", \"count\": 1000, \"mean\": 35.4567, \"std\": 12.3456, \"min\": 18.0000, \"max\": 85.0000, \"median\": 34.0000, \"skewness\": 0.5678, \"kurtosis\": -0.2345}",
      "ground_truth_hash": "3e8ee113984fb7d4",
      "_ground_truth": {
        "column": "Store_Sales",
        "count": 896,
        "mean": 59351.3058,
        "std": 17190.7419,
        "min": 14920.0,
        "max": 116320.0,
        "median": 58605.0,
        "skewness": 0.1488,
        "kurtosis": -0.4561
      },
      "_template": "descriptive_summary"
    }
  ]
}