{
  "dataset_columns": [
    "Rank",
    "CCA3",
    "Country/Territory",
    "Capital",
    "Continent",
    "2022 Population",
    "2020 Population",
    "2015 Population",
    "2010 Population",
    "2000 Population",
    "1990 Population",
    "1980 Population",
    "1970 Population",
    "Area (km\u00b2)",
    "Density (per km\u00b2)",
    "Growth Rate",
    "World Population Percentage"
  ],
  "questions": [
    {
      "question": "Which variable in the world population dataset shows the greatest variability, and can its values be predicted by three other numeric measurements? Return as JSON with keys: target, predictors, r_squared, adj_r_squared, n_significant, coefficients, p_values.",
      "hint": "Find the numeric column with the highest variance, compute absolute correlations with the other numeric columns, select the three strongest correlates, fit an OLS regression, and report the model statistics.",
      "n_steps": 10,
      "difficulty": "VERY_HARD",
      "template_name": "multiple_regression_top_predictors",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"predictors\" (list of 3 column names), \"r_squared\" (rounded to 4 decimals), \"adj_r_squared\" (rounded to 4 decimals), \"n_significant\" (integer count of predictors with p < 0.05), \"coefficients\" (dict mapping predictor names to coefficients rounded to 4 decimals), and \"p_values\" (dict mapping predictor names to p-values rounded to 6 decimals). Example: {\"target\": \"price\", \"predictors\": [\"sqft\", \"bedrooms\", \"age\"], \"r_squared\": 0.7234, \"adj_r_squared\": 0.7156, \"n_significant\": 2, \"coefficients\": {\"sqft\": 123.45, \"bedrooms\": 5000.12, \"age\": -200.34}, \"p_values\": {\"sqft\": 0.000001, \"bedrooms\": 0.023456, \"age\": 0.156789}}",
      "ground_truth_hash": "2fbf3592b7aede6c",
      "_ground_truth": {
        "target": "2022 Population",
        "predictors": [
          "World Population Percentage",
          "2020 Population",
          "2015 Population"
        ],
        "r_squared": 1.0,
        "adj_r_squared": 1.0,
        "n_significant": 3,
        "coefficients": {
          "World Population Percentage": 59511255.2279,
          "2020 Population": 0.3721,
          "2015 Population": -0.1208
        },
        "p_values": {
          "World Population Percentage": 0.0,
          "2020 Population": 0.0,
          "2015 Population": 0.0
        }
      },
      "_template": "multiple_regression_top_predictors"
    },
    {
      "question": "Which numeric column in the world population dataset shows the greatest skewness, and what are its mean and 95% confidence interval?",
      "hint": "Calculate absolute skewness for each numeric column, select the most skewed one, then use bootstrap resampling to estimate the mean and its confidence interval.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "bootstrap_ci_discovered",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (name of most skewed column), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (lower bound of 95% CI, rounded to 4 decimals), \"ci_upper\" (upper bound, rounded to 4 decimals), \"std_error\" (bootstrap standard error, rounded to 4 decimals), and \"n_bootstrap\" (integer, the number of bootstrap samples). Example: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}",
      "ground_truth_hash": "e215152dc712e01c",
      "_ground_truth": {
        "column": "1980 Population",
        "skewness": 9.6576,
        "mean": 18984616.9701,
        "ci_lower": 10450557.8332,
        "ci_upper": 30797987.1635,
        "std_error": 5457792.5459,
        "n_bootstrap": 1000
      },
      "_template": "bootstrap_ci_discovered"
    },
    {
      "question": "Does the most variable numeric measurement differ significantly across continents? Return as JSON with keys: target_column, grouping_column, n_groups, f_statistic, p_value, significant, best_group, best_mean, worst_group, worst_mean, eta_squared.",
      "hint": "Find the numeric column with the highest variance, choose a categorical column with a moderate number of groups, and compare group means with a one\u2011way ANOVA.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "anova_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 11 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"f_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"significant\" (boolean), \"best_group\" (category with highest mean), \"best_mean\" (rounded to 4 decimals), \"worst_group\" (category with lowest mean), \"worst_mean\" (rounded to 4 decimals), and \"eta_squared\" (effect size, rounded to 4 decimals). Example: {\"target_column\": \"sales\", \"grouping_column\": \"region\", \"n_groups\": 4, \"f_statistic\": 15.2345, \"p_value\": 0.000012, \"significant\": true, \"best_group\": \"West\", \"best_mean\": 1234.5678, \"worst_group\": \"East\", \"worst_mean\": 890.1234, \"eta_squared\": 0.1523}",
      "ground_truth_hash": "a6da2f538abc70cf",
      "_ground_truth": {
        "target_column": "2022 Population",
        "grouping_column": "Continent",
        "n_groups": 6,
        "f_statistic": 2.6999,
        "p_value": 0.021569,
        "significant": "True",
        "best_group": "Asia",
        "best_mean": 94427665.48,
        "worst_group": "Oceania",
        "worst_mean": 1958198.0,
        "eta_squared": 0.0559
      },
      "_template": "anova_discovered_groups"
    },
    {
      "question": "Which numeric column in the world population dataset is the most variable, and which other numeric feature best predicts it? Return as JSON with keys: target, best_predictor, correlation, r_squared, coefficient, p_value.",
      "hint": "Find the numeric column with the highest variance, compute absolute correlations with the others, pick the strongest, then fit a simple OLS regression.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "regression_most_predictive",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"best_predictor\" (column name), \"correlation\" (rounded to 3 decimals), \"r_squared\" (rounded to 4 decimals), \"coefficient\" (rounded to 4 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target\": \"price\", \"best_predictor\": \"sqft\", \"correlation\": 0.834, \"r_squared\": 0.6956, \"coefficient\": 135.2847, \"p_value\": 0.000001}",
      "ground_truth_hash": "517be0518b05ccc0",
      "_ground_truth": {
        "target": "2022 Population",
        "best_predictor": "World Population Percentage",
        "correlation": 1.0,
        "r_squared": 1.0,
        "coefficient": 79748166.4538,
        "p_value": 0.0
      },
      "_template": "regression_most_predictive"
    },
    {
      "question": "Is the most variable numeric measurement in the dataset significantly different between two groups defined by a binary split? Return as JSON with keys: target_column, grouping_column, group1, group2, mean1, mean2, t_statistic, p_value, significant.",
      "hint": "Find the numeric column with the highest variance, create a binary grouping variable (e.g., based on median of another column), then compare group means with a t\u2011test.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "ttest_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"mean1\" (rounded to 4 decimals), \"mean2\" (rounded to 4 decimals), \"t_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"significant\" (boolean, true if p < 0.05). Example: {\"target_column\": \"score\", \"grouping_column\": \"gender\", \"group1\": \"M\", \"group2\": \"F\", \"mean1\": 75.4321, \"mean2\": 78.1234, \"t_statistic\": -2.3456, \"p_value\": 0.019234, \"significant\": true}",
      "ground_truth_hash": "8a337ba4ffe2739a",
      "_ground_truth": {
        "target_column": "2022 Population",
        "grouping_column": "_binary_group",
        "group1": "low",
        "group2": "high",
        "mean1": 66844111.4017,
        "mean2": 1304718.0171,
        "t_statistic": 3.7676,
        "p_value": 0.000209,
        "significant": "True"
      },
      "_template": "ttest_discovered_groups"
    },
    {
      "question": "Is there any numeric column in the world population dataset that appears to follow a normal distribution? Return as JSON with keys: column, ks_statistic, p_value, is_normal, sample_mean, sample_std.",
      "hint": "Find the numeric column with the lowest absolute skewness, compute its mean and std, standardize the values, and perform a Kolmogorov\u2011Smirnov test against a standard normal distribution.",
      "n_steps": 7,
      "difficulty": "MEDIUM",
      "template_name": "ks_normality_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (tested column), \"ks_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"is_normal\" (boolean, true if p >= 0.05), \"sample_mean\" (rounded to 4 decimals), and \"sample_std\" (rounded to 4 decimals). Example: {\"column\": \"height\", \"ks_statistic\": 0.0456, \"p_value\": 0.234567, \"is_normal\": true, \"sample_mean\": 170.1234, \"sample_std\": 10.5678}",
      "ground_truth_hash": "bc80dd5c559f31c7",
      "_ground_truth": {
        "column": "Rank",
        "ks_statistic": 0.0588,
        "p_value": 0.377828,
        "is_normal": "True",
        "sample_mean": 117.5,
        "sample_std": 67.6942
      },
      "_template": "ks_normality_test"
    },
    {
      "question": "I wonder whether the most variable numeric measurement shows different variances across continents. Return as JSON with keys: target_column, grouping_column, n_groups, levene_statistic, p_value, variances_equal, group_variances.",
      "hint": "Find the numeric column with the highest variance, pick a categorical column with 2\u20136 groups, then run Levene's test to compare group variances.",
      "n_steps": 7,
      "difficulty": "HARD",
      "template_name": "levene_variance_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"levene_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"variances_equal\" (boolean, true if p >= 0.05), and \"group_variances\" (dict mapping group names to variance rounded to 4 decimals). Example: {\"target_column\": \"score\", \"grouping_column\": \"class\", \"n_groups\": 3, \"levene_statistic\": 2.3456, \"p_value\": 0.098765, \"variances_equal\": true, \"group_variances\": {\"A\": 123.4567, \"B\": 145.6789, \"C\": 112.3456}}",
      "ground_truth_hash": "51b6f0bb2b77a524",
      "_ground_truth": {
        "target_column": "2022 Population",
        "grouping_column": "Continent",
        "n_groups": 6,
        "levene_statistic": 2.5012,
        "p_value": 0.031463,
        "variances_equal": "False",
        "group_variances": {
          "Africa": 1411677713816726.0,
          "Asia": 7.824367504604048e+16,
          "Europe": 743375725622747.9,
          "North America": 3182997880975658.5,
          "Oceania": 33119271461767.184,
          "South America": 3105366454936978.0
        }
      },
      "_template": "levene_variance_test"
    },
    {
      "question": "Is there a significant difference in the most skewed numeric measurement between two groups formed by a binary split of another variable? Return as JSON with keys: target_column, grouping_column, group1, group2, median1, median2, u_statistic, p_value.",
      "hint": "Find the numeric column with highest absolute skewness, define a binary grouping (or use an existing two\u2011level categorical), compute group medians and apply a Mann\u2011Whitney U test.",
      "n_steps": 6,
      "difficulty": "MEDIUM",
      "template_name": "mann_whitney_u_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 8 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"median1\" (rounded to 4 decimals), \"median2\" (rounded to 4 decimals), \"u_statistic\" (rounded to 2 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target_column\": \"score\", \"grouping_column\": \"treatment\", \"group1\": \"control\", \"group2\": \"experimental\", \"median1\": 72.5000, \"median2\": 78.0000, \"u_statistic\": 1234.50, \"p_value\": 0.034567}",
      "ground_truth_hash": "2984ddbacf4c1abb",
      "_ground_truth": {
        "target_column": "1980 Population",
        "grouping_column": "_binary_group",
        "group1": "low",
        "group2": "high",
        "median1": 9828986.0,
        "median2": 228263.0,
        "u_statistic": 13458.0,
        "p_value": 0.0
      },
      "_template": "mann_whitney_u_test"
    },
    {
      "question": "Which two numeric variables in the world population dataset show the strongest monotonic correlation? Return as JSON with keys: columns, spearman_rho, p_value, interpretation.",
      "hint": "Calculate Spearman correlations for all numeric columns and pick the pair with the highest absolute correlation.",
      "n_steps": 6,
      "difficulty": "MEDIUM",
      "template_name": "spearman_rank_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"spearman_rho\" (correlation coefficient rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"interpretation\" (string: \"strong\", \"moderate\", \"weak\", or \"negligible\"). Example: {\"columns\": [\"age\", \"income\"], \"spearman_rho\": 0.7234, \"p_value\": 0.000001, \"interpretation\": \"strong\"}",
      "ground_truth_hash": "897ffd5e929bf858",
      "_ground_truth": {
        "columns": [
          "2022 Population",
          "Rank"
        ],
        "spearman_rho": -1.0,
        "p_value": 0.0,
        "interpretation": "strong"
      },
      "_template": "spearman_rank_correlation"
    },
    {
      "question": "Which two numeric population columns are the most skewed, and does applying a log transformation change their Pearson correlation? Return as JSON with keys: columns, original_correlation, log_correlation, improvement, transformation_helpful.",
      "hint": "Find the two most positively skewed numeric columns, compute their original Pearson correlation, apply a log1p transform, recompute the correlation, and compare the absolute values.",
      "n_steps": 6,
      "difficulty": "HARD",
      "template_name": "correlation_change_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 5 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (Pearson, rounded to 4 decimals), \"log_correlation\" (after log transform, rounded to 4 decimals), \"improvement\" (absolute difference, rounded to 4 decimals), and \"transformation_helpful\" (boolean, true if |log_corr| > |orig_corr|). Example: {\"columns\": [\"income\", \"spending\"], \"original_correlation\": 0.4567, \"log_correlation\": 0.7234, \"improvement\": 0.2667, \"transformation_helpful\": true}",
      "ground_truth_hash": "99014dc213e7cb30",
      "_ground_truth": {
        "columns": [
          "1970 Population",
          "1980 Population"
        ],
        "original_correlation": 0.9992,
        "log_correlation": 0.9983,
        "improvement": 0.0009,
        "transformation_helpful": "False"
      },
      "_template": "correlation_change_analysis"
    },
    {
      "question": "Which numeric column in the world population dataset has the largest variability, and how many extreme values does it contain? Return as JSON with keys: column, q1, q3, iqr, lower_fence, upper_fence, n_outliers.",
      "hint": "Identify the numeric column with the highest variance, compute its Q1, Q3, IQR, then apply the 1.5*IQR rule to find lower/upper fences and count outliers.",
      "n_steps": 5,
      "difficulty": "EASY",
      "template_name": "iqr_outlier_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (analyzed column), \"q1\" (25th percentile rounded to 4 decimals), \"q3\" (75th percentile rounded to 4 decimals), \"iqr\" (rounded to 4 decimals), \"lower_fence\" (rounded to 4 decimals), \"upper_fence\" (rounded to 4 decimals), and \"n_outliers\" (integer count). Example: {\"column\": \"salary\", \"q1\": 45000.0000, \"q3\": 75000.0000, \"iqr\": 30000.0000, \"lower_fence\": 0.0000, \"upper_fence\": 120000.0000, \"n_outliers\": 23}",
      "ground_truth_hash": "2b819eac9bdb4b5c",
      "_ground_truth": {
        "column": "2022 Population",
        "q1": 419738.5,
        "q3": 22476504.75,
        "iqr": 22056766.25,
        "lower_fence": -32665410.875,
        "upper_fence": 55561654.125,
        "n_outliers": 25
      },
      "_template": "iqr_outlier_analysis"
    },
    {
      "question": "Which numeric measurement in the world population dataset shows the greatest variability and how is it distributed? Return as JSON with keys: distribution, median, iqr.",
      "hint": "Find the numeric column with the highest variance, test its normality (e.g., Shapiro-Wilk), and if non\u2011normal report the median and interquartile range.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "conditional_normality",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 3 keys. If normal: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}. If non-normal: {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. The \"iqr\" value is Q3 minus Q1 as a single number. All numeric values rounded to 3 decimal places.",
      "ground_truth_hash": "cdda198d8f725b82",
      "_ground_truth": {
        "distribution": "non-normal",
        "median": 5559944.5,
        "iqr": 22056766.25
      },
      "_template": "conditional_normality"
    },
    {
      "question": "Which category shows the highest average for the most variable numeric measurement in the dataset? Return as JSON with keys: category_column, best_category, target_column, mean_value.",
      "hint": "Identify the numeric column with the greatest variance, select a categorical column with 2\u201120 unique values, compute the mean of the numeric column for each category, and report the category with the highest mean.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "category_highest_target_mean",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"category_column\" (the grouping column name), \"best_category\" (the category value with highest mean), \"target_column\" (the numeric column analyzed), and \"mean_value\" (rounded to 3 decimal places). Example: {\"category_column\": \"region\", \"best_category\": \"West\", \"target_column\": \"sales\", \"mean_value\": 1234.567}",
      "ground_truth_hash": "3879a2869f126886",
      "_ground_truth": {
        "category_column": "Continent",
        "best_category": "Asia",
        "target_column": "2022 Population",
        "mean_value": 94427665.48
      },
      "_template": "category_highest_target_mean"
    },
    {
      "question": "Which two numeric attributes in the world population dataset are most strongly linearly related, and does that relationship change after removing extreme values? Return as JSON with keys: columns, original_correlation, outliers_removed, clean_correlation.",
      "hint": "Calculate the absolute correlation matrix for all numeric columns, pick the pair with the highest correlation, drop rows where either value is more than 3 standard deviations from its mean, then recompute the correlation for the cleaned data.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "a52e7adadc0e0006",
      "_ground_truth": {
        "columns": [
          "2022 Population",
          "World Population Percentage"
        ],
        "original_correlation": 1.0,
        "outliers_removed": 2,
        "clean_correlation": 1.0
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "Are there many outlier values across the numeric attributes in this world population dataset? Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "For each numeric column, flag values that lie more than 3 standard deviations from the mean, then count columns with any outliers and total outlier occurrences.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "308df922a5d8623f",
      "_ground_truth": {
        "columns_with_outliers": 12,
        "total_outliers": 30
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "I wonder how many numeric columns contain outliers and the overall number of outlier values in the dataset. Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "Flag values beyond ~2.5 standard deviations from each column's mean, then count columns with any flagged values and the total flagged entries.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "87bf9cd475db6b87",
      "_ground_truth": {
        "columns_with_outliers": 12,
        "total_outliers": 31
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "Which numeric measurement in the world population dataset shows the greatest relative variability? Return as JSON with keys: column, cv, mean, std.",
      "hint": "Calculate the coefficient of variation (std/mean\u202f\u00d7\u202f100) for each numeric column and identify the one with the highest value.",
      "n_steps": 4,
      "difficulty": "EASY",
      "template_name": "coefficient_of_variation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"column\" (name of column with highest CV), \"cv\" (coefficient of variation as percentage rounded to 2 decimals), \"mean\" (rounded to 4 decimals), and \"std\" (rounded to 4 decimals). Example: {\"column\": \"price\", \"cv\": 45.23, \"mean\": 1234.5678, \"std\": 558.4567}",
      "ground_truth_hash": "19e1dbc0329d451e",
      "_ground_truth": {
        "column": "Density (per km\u00b2)",
        "cv": 456.98,
        "mean": 452.127,
        "std": 2066.1219
      },
      "_template": "coefficient_of_variation"
    },
    {
      "question": "Which two numeric variables in the world population dataset show the strongest linear relationship? Return as JSON with keys: columns, correlation.",
      "hint": "Compute the absolute correlation matrix of all numeric columns, zero out the diagonal, then locate the pair with the highest value.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "7adcac5d95b470b1",
      "_ground_truth": {
        "columns": [
          "2022 Population",
          "World Population Percentage"
        ],
        "correlation": 1.0
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Which pair of numeric measurements in the world population dataset have the weakest linear relationship? Return as JSON with keys: columns, correlation.",
      "hint": "Calculate the absolute correlation matrix for all numeric columns and identify the smallest non\u2011zero off\u2011diagonal correlation.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "f6cc8be4ab7a2191",
      "_ground_truth": {
        "columns": [
          "Area (km\u00b2)",
          "Growth Rate"
        ],
        "correlation": 0.014
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "Which numeric attribute in the world population dataset varies the most, and what is its average value? Return as JSON with keys: mean.",
      "hint": "Identify the numeric column with the highest variance, then compute its mean.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "fc789abf3ac97226",
      "_ground_truth": 34074414.709,
      "_template": "max_variance_mean"
    },
    {
      "question": "Which numeric column has the lowest average value, and what is its standard deviation? Return as JSON with keys: result.",
      "hint": "Calculate the mean of each numeric column, identify the column with the smallest mean, then compute its standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "ff2a378d99934046",
      "_ground_truth": 1.715,
      "_template": "min_mean_column_std"
    },
    {
      "question": "Are there any columns in this dataset that have a high proportion of missing values? Return as JSON with keys: count, columns.",
      "hint": "Calculate the percentage of missing values per column and count those exceeding a 5% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Are there any columns in this dataset that contain a high proportion of missing values? Return as JSON with keys: count, columns.",
      "hint": "Compute the percentage of missing entries per column, then count and list those where the percentage exceeds 10%.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Which numeric variable in the dataset varies the most, and what are its key percentile values? Return as JSON with keys: column, p10, p25, p50, p75, p90.",
      "hint": "Identify the numeric column with the largest range, then compute its 10th, 25th, 50th, 75th, and 90th percentiles.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "percentile_ranking",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (analyzed column), \"p10\" (10th percentile rounded to 4 decimals), \"p25\" (25th percentile), \"p50\" (median), \"p75\" (75th percentile), and \"p90\" (90th percentile). Example: {\"column\": \"income\", \"p10\": 25000.0000, \"p25\": 40000.0000, \"p50\": 55000.0000, \"p75\": 75000.0000, \"p90\": 100000.0000}",
      "ground_truth_hash": "ff5034290e90c6cb",
      "_ground_truth": {
        "column": "2022 Population",
        "p10": 48225.2,
        "p25": 419738.5,
        "p50": 5559944.5,
        "p75": 22476504.75,
        "p90": 59636961.7
      },
      "_template": "percentile_ranking"
    },
    {
      "question": "Which numeric variable in the world population dataset shows the greatest variability, and what are its key summary statistics? Return as JSON with keys: column, count, mean, std, min, max, median, skewness, kurtosis.",
      "hint": "Identify the numeric column with the highest variance, then compute its descriptive statistics.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "descriptive_summary",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"column\", \"count\" (integer), \"mean\" (rounded to 4 decimals), \"std\", \"min\", \"max\", \"median\", \"skewness\", and \"kurtosis\" (all rounded to 4 decimals). Example: {\"column\": \"age\", \"count\": 1000, \"mean\": 35.4567, \"std\": 12.3456, \"min\": 18.0000, \"max\": 85.0000, \"median\": 34.0000, \"skewness\": 0.5678, \"kurtosis\": -0.2345}",
      "ground_truth_hash": "44b67091a08bf65d",
      "_ground_truth": {
        "column": "2022 Population",
        "count": 234,
        "mean": 34074414.7094,
        "std": 136766424.8048,
        "min": 510.0,
        "max": 1425887337.0,
        "median": 5559944.5,
        "skewness": 9.1512,
        "kurtosis": 90.4647
      },
      "_template": "descriptive_summary"
    }
  ]
}