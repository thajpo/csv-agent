{
  "dataset_columns": [
    "Row ID",
    "Order ID",
    "Order Date",
    "Ship Date",
    "Ship Mode",
    "Customer ID",
    "Customer Name",
    "Segment",
    "Country",
    "City",
    "State",
    "Postal Code",
    "Region",
    "Product ID",
    "Category",
    "Sub-Category",
    "Product Name",
    "Sales"
  ],
  "questions": [
    {
      "question": "Which numeric column in the dataset is the most skewed and what is its mean with a 95% confidence interval using bootstrapping? Return as JSON with keys: column, skewness, mean, ci_lower, ci_upper, std_error, n_bootstrap.",
      "hint": "Compute absolute skewness for all numeric columns, select the highest, then bootstrap the mean to obtain the confidence interval and standard error.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "bootstrap_ci_discovered",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (name of most skewed column), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (lower bound of 95% CI, rounded to 4 decimals), \"ci_upper\" (upper bound, rounded to 4 decimals), \"std_error\" (bootstrap standard error, rounded to 4 decimals), and \"n_bootstrap\" (integer, the number of bootstrap samples). Example: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}",
      "ground_truth_hash": "235d24c729adfd0a",
      "_ground_truth": {
        "column": "Sales",
        "skewness": 12.9835,
        "mean": 230.7691,
        "ci_lower": 218.3881,
        "ci_upper": 244.3992,
        "std_error": 6.2857,
        "n_bootstrap": 1000
      },
      "_template": "bootstrap_ci_discovered"
    },
    {
      "question": "Is the numeric variable with the greatest variance significantly different across a small set of categories? Return as JSON with keys: target_column, grouping_column, n_groups, f_statistic, p_value, significant, best_group, best_mean, worst_group, worst_mean, eta_squared.",
      "hint": "Find the numeric column with the highest variance, select a categorical column that has between 3 and 10 unique values, run a one\u2011way ANOVA to compare group means, compute effect size, and output the results using the specified JSON keys.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "anova_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 11 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"f_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"significant\" (boolean), \"best_group\" (category with highest mean), \"best_mean\" (rounded to 4 decimals), \"worst_group\" (category with lowest mean), \"worst_mean\" (rounded to 4 decimals), and \"eta_squared\" (effect size, rounded to 4 decimals). Example: {\"target_column\": \"sales\", \"grouping_column\": \"region\", \"n_groups\": 4, \"f_statistic\": 15.2345, \"p_value\": 0.000012, \"significant\": true, \"best_group\": \"West\", \"best_mean\": 1234.5678, \"worst_group\": \"East\", \"worst_mean\": 890.1234, \"eta_squared\": 0.1523}",
      "ground_truth_hash": "eefa2a620a252cfb",
      "_ground_truth": {
        "target_column": "Postal Code",
        "grouping_column": "Ship Mode",
        "n_groups": 4,
        "f_statistic": 1.0722,
        "p_value": 0.359483,
        "significant": "False",
        "best_group": "Same Day",
        "best_mean": 57459.9684,
        "worst_group": "First Class",
        "worst_mean": 54937.7055,
        "eta_squared": 0.0003
      },
      "_template": "anova_discovered_groups"
    },
    {
      "question": "Does the most variable numeric measurement differ between two naturally defined groups in the dataset? Return as JSON with keys: target_column, grouping_column, group1, group2, mean1, mean2, t_statistic, p_value, significant.",
      "hint": "Find the numeric column with the highest variance, create a binary grouping (e.g., from a categorical column or a median split of another numeric column), then compare the group means with a t-test.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "ttest_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"mean1\" (rounded to 4 decimals), \"mean2\" (rounded to 4 decimals), \"t_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"significant\" (boolean, true if p < 0.05). Example: {\"target_column\": \"score\", \"grouping_column\": \"gender\", \"group1\": \"M\", \"group2\": \"F\", \"mean1\": 75.4321, \"mean2\": 78.1234, \"t_statistic\": -2.3456, \"p_value\": 0.019234, \"significant\": true}",
      "ground_truth_hash": "de44e4f1537abb83",
      "_ground_truth": {
        "target_column": "Postal Code",
        "grouping_column": "_binary_group",
        "group1": "low",
        "group2": "high",
        "mean1": 55043.0765,
        "mean2": 55503.992,
        "t_statistic": -0.7116,
        "p_value": 0.476725,
        "significant": "False"
      },
      "_template": "ttest_discovered_groups"
    },
    {
      "question": "I wonder which numeric column in this sales dataset is closest to a normal distribution. Return as JSON with keys: column, ks_statistic, p_value, is_normal, sample_mean, sample_std.",
      "hint": "Find the numeric column with the smallest absolute skewness, standardize its values, and run a Kolmogorov\u2011Smirnov test against a standard normal.",
      "n_steps": 7,
      "difficulty": "MEDIUM",
      "template_name": "ks_normality_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (tested column), \"ks_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"is_normal\" (boolean, true if p >= 0.05), \"sample_mean\" (rounded to 4 decimals), and \"sample_std\" (rounded to 4 decimals). Example: {\"column\": \"height\", \"ks_statistic\": 0.0456, \"p_value\": 0.234567, \"is_normal\": true, \"sample_mean\": 170.1234, \"sample_std\": 10.5678}",
      "ground_truth_hash": "c35fb1062d06f2da",
      "_ground_truth": {
        "column": "Row ID",
        "ks_statistic": 0.0572,
        "p_value": 0.0,
        "is_normal": "False",
        "sample_mean": 4900.5,
        "sample_std": 2829.1607
      },
      "_template": "ks_normality_test"
    },
    {
      "question": "Do the variances of the most variable numeric measurement differ across shipping modes? Return as JSON with keys: target_column, grouping_column, n_groups, levene_statistic, p_value, variances_equal, group_variances.",
      "hint": "Find the numeric column with the highest variance, pick a categorical column with 2\u20116 groups, then compare group variances using Levene's test.",
      "n_steps": 7,
      "difficulty": "HARD",
      "template_name": "levene_variance_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"levene_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"variances_equal\" (boolean, true if p >= 0.05), and \"group_variances\" (dict mapping group names to variance rounded to 4 decimals). Example: {\"target_column\": \"score\", \"grouping_column\": \"class\", \"n_groups\": 3, \"levene_statistic\": 2.3456, \"p_value\": 0.098765, \"variances_equal\": true, \"group_variances\": {\"A\": 123.4567, \"B\": 145.6789, \"C\": 112.3456}}",
      "ground_truth_hash": "fe0d656ef4ae9f6d",
      "_ground_truth": {
        "target_column": "Postal Code",
        "grouping_column": "Ship Mode",
        "n_groups": 4,
        "levene_statistic": 3.451,
        "p_value": 0.015828,
        "variances_equal": "False",
        "group_variances": {
          "First Class": 1072933425.0159,
          "Same Day": 983161464.9618,
          "Second Class": 1026199452.9069,
          "Standard Class": 1018863432.2207
        }
      },
      "_template": "levene_variance_test"
    },
    {
      "question": "Is there an association between the shipping method and the customer segment in this sales dataset? Return as JSON with keys: column1, column2, chi_squared, p_value, degrees_of_freedom, expected_min, independent.",
      "hint": "Build a contingency table for two low\u2011cardinality categorical columns and run a chi\u2011squared test of independence.",
      "n_steps": 6,
      "difficulty": "MEDIUM",
      "template_name": "chi_squared_independence",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column1\" (first categorical column), \"column2\" (second categorical column), \"chi_squared\" (test statistic rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"degrees_of_freedom\" (integer), \"expected_min\" (minimum expected frequency rounded to 2 decimals), and \"independent\" (boolean, true if p >= 0.05). Example: {\"column1\": \"gender\", \"column2\": \"product\", \"chi_squared\": 15.2345, \"p_value\": 0.004321, \"degrees_of_freedom\": 4, \"expected_min\": 5.23, \"independent\": false}",
      "ground_truth_hash": "99a5d4cdcc29af75",
      "_ground_truth": {
        "column1": "Ship Mode",
        "column2": "Segment",
        "chi_squared": 25.8399,
        "p_value": 0.000238,
        "degrees_of_freedom": 6,
        "expected_min": 95.85,
        "independent": "False"
      },
      "_template": "chi_squared_independence"
    },
    {
      "question": "Is the most skewed numeric measurement different between two groups created by a median split of another numeric variable? Return as JSON with keys: target_column, grouping_column, group1, group2, median1, median2, u_statistic, p_value.",
      "hint": "Find the numeric column with highest absolute skewness, create a binary group using the median of another numeric column, then compare group medians with a Mann\u2011Whitney U test.",
      "n_steps": 6,
      "difficulty": "MEDIUM",
      "template_name": "mann_whitney_u_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 8 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"median1\" (rounded to 4 decimals), \"median2\" (rounded to 4 decimals), \"u_statistic\" (rounded to 2 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target_column\": \"score\", \"grouping_column\": \"treatment\", \"group1\": \"control\", \"group2\": \"experimental\", \"median1\": 72.5000, \"median2\": 78.0000, \"u_statistic\": 1234.50, \"p_value\": 0.034567}",
      "ground_truth_hash": "7667182134e86a44",
      "_ground_truth": {
        "target_column": "Sales",
        "grouping_column": "_binary_group",
        "group1": "low",
        "group2": "high",
        "median1": 52.48,
        "median2": 56.064,
        "u_statistic": 11908745.0,
        "p_value": 0.49186
      },
      "_template": "mann_whitney_u_test"
    },
    {
      "question": "Do the two most skewed numeric columns exhibit a stronger correlation after applying a log transformation? Return as JSON with keys: question, hint.",
      "hint": "Find the numeric columns with highest absolute skewness, compute their Pearson correlation, then log\u2011transform both (e.g., log1p) and recompute the correlation to see if the absolute value improves.",
      "n_steps": 6,
      "difficulty": "HARD",
      "template_name": "correlation_change_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 5 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (Pearson, rounded to 4 decimals), \"log_correlation\" (after log transform, rounded to 4 decimals), \"improvement\" (absolute difference, rounded to 4 decimals), and \"transformation_helpful\" (boolean, true if |log_corr| > |orig_corr|). Example: {\"columns\": [\"income\", \"spending\"], \"original_correlation\": 0.4567, \"log_correlation\": 0.7234, \"improvement\": 0.2667, \"transformation_helpful\": true}",
      "ground_truth_hash": "f3ad556e66bfa5e9",
      "_ground_truth": {
        "columns": [
          "Row ID",
          "Sales"
        ],
        "original_correlation": 0.0012,
        "log_correlation": -0.0049,
        "improvement": 0.0037,
        "transformation_helpful": "True"
      },
      "_template": "correlation_change_analysis"
    },
    {
      "question": "Which numeric attribute shows the greatest spread in the dataset, and what are its quartile values and outlier counts? Return as JSON with keys: column, q1, q3, iqr, lower_fence, upper_fence, n_outliers.",
      "hint": "Identify the numeric column with the highest variance, then compute its Q1, Q3, IQR, fences, and count of outliers.",
      "n_steps": 5,
      "difficulty": "EASY",
      "template_name": "iqr_outlier_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (analyzed column), \"q1\" (25th percentile rounded to 4 decimals), \"q3\" (75th percentile rounded to 4 decimals), \"iqr\" (rounded to 4 decimals), \"lower_fence\" (rounded to 4 decimals), \"upper_fence\" (rounded to 4 decimals), and \"n_outliers\" (integer count). Example: {\"column\": \"salary\", \"q1\": 45000.0000, \"q3\": 75000.0000, \"iqr\": 30000.0000, \"lower_fence\": 0.0000, \"upper_fence\": 120000.0000, \"n_outliers\": 23}",
      "ground_truth_hash": "d0424a3f04c0dab3",
      "_ground_truth": {
        "column": "Postal Code",
        "q1": 23223.0,
        "q3": 90008.0,
        "iqr": 66785.0,
        "lower_fence": -76954.5,
        "upper_fence": 190185.5,
        "n_outliers": 0
      },
      "_template": "iqr_outlier_analysis"
    },
    {
      "question": "Which numeric column shows the greatest variability and does its distribution appear normal? Return as JSON with keys: distribution, median, iqr.",
      "hint": "Identify the numeric column with highest variance, test normality with Shapiro\u2011Wilk, then report median and IQR if non\u2011normal.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "conditional_normality",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 3 keys. If normal: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}. If non-normal: {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. The \"iqr\" value is Q3 minus Q1 as a single number. All numeric values rounded to 3 decimal places.",
      "ground_truth_hash": "1c6300b9687efc87",
      "_ground_truth": {
        "distribution": "non-normal",
        "median": 58103.0,
        "iqr": 66785.0
      },
      "_template": "conditional_normality"
    },
    {
      "question": "Which categorical group has the highest average for the most variable numeric column? Return as JSON with keys: category_column, best_category, target_column, mean_value.",
      "hint": "Identify the numeric column with the largest variance, select a categorical column with moderate cardinality, compute means per category, and report the top one.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "category_highest_target_mean",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"category_column\" (the grouping column name), \"best_category\" (the category value with highest mean), \"target_column\" (the numeric column analyzed), and \"mean_value\" (rounded to 3 decimal places). Example: {\"category_column\": \"region\", \"best_category\": \"West\", \"target_column\": \"sales\", \"mean_value\": 1234.567}",
      "ground_truth_hash": "719e6ee224103c43",
      "_ground_truth": {
        "category_column": "Ship Mode",
        "best_category": "Same Day",
        "target_column": "Postal Code",
        "mean_value": 57459.968
      },
      "_template": "category_highest_target_mean"
    },
    {
      "question": "Which two numeric columns show the strongest correlation, and how does that relationship change after removing extreme outliers? Return as JSON with keys: columns, original_correlation, outliers_removed, clean_correlation.",
      "hint": "Calculate absolute Pearson correlations, pick the highest pair, filter rows where either value is more than 3 standard deviations from its mean, then recompute the correlation on the cleaned data.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "6f72328a6b0c22d3",
      "_ground_truth": {
        "columns": [
          "Postal Code",
          "Sales"
        ],
        "original_correlation": 0.024,
        "outliers_removed": 133,
        "clean_correlation": -0.004
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "How many numeric columns have extreme outlier values and what is the total count of those outliers? Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "For each numeric column, compare values to mean \u00b1 3\u00b7std to identify outliers, then aggregate counts.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "3d794bfd071c0861",
      "_ground_truth": {
        "columns_with_outliers": 1,
        "total_outliers": 123
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "How many numeric columns have outlier values and what is the total number of outliers across all numeric columns? Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "For each numeric column, compare values to the mean and flag those beyond 2.5 standard deviations; then aggregate counts.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "382955c92f496201",
      "_ground_truth": {
        "columns_with_outliers": 1,
        "total_outliers": 168
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "Which numeric field in this sales dataset shows the greatest relative variability? Return as JSON with keys: column, cv, mean, std.",
      "hint": "Calculate the coefficient of variation (std/mean*100) for each numeric column and identify the one with the highest value.",
      "n_steps": 4,
      "difficulty": "EASY",
      "template_name": "coefficient_of_variation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"column\" (name of column with highest CV), \"cv\" (coefficient of variation as percentage rounded to 2 decimals), \"mean\" (rounded to 4 decimals), and \"std\" (rounded to 4 decimals). Example: {\"column\": \"price\", \"cv\": 45.23, \"mean\": 1234.5678, \"std\": 558.4567}",
      "ground_truth_hash": "fc392644e05b3966",
      "_ground_truth": {
        "column": "Sales",
        "cv": 271.55,
        "mean": 230.7691,
        "std": 626.6519
      },
      "_template": "coefficient_of_variation"
    },
    {
      "question": "Which two numeric fields are most closely related in this sales dataset? Return as JSON with keys: columns, correlation.",
      "hint": "Compute the absolute correlation matrix of numeric columns and identify the pair with the highest off\u2011diagonal correlation.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "79d5b07da12886a3",
      "_ground_truth": {
        "columns": [
          "Postal Code",
          "Sales"
        ],
        "correlation": 0.024
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Which pair of numeric variables in the dataset have the weakest linear relationship? Return as JSON with keys: columns, correlation.",
      "hint": "Compute the absolute correlation matrix for numeric columns, mask the diagonal, and find the minimum non\u2011zero correlation.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "494d6a1cc64d4eb4",
      "_ground_truth": {
        "columns": [
          "Row ID",
          "Sales"
        ],
        "correlation": 0.001
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "Which numeric column in the sales dataset exhibits the highest variability, and what is its mean? Return as JSON with keys: mean.",
      "hint": "Identify the numeric column with the greatest variance, then calculate its average.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "f504996530714ec5",
      "_ground_truth": 55273.322,
      "_template": "max_variance_mean"
    },
    {
      "question": "Which numeric column has the lowest mean and what is its standard deviation? Return as JSON with keys: std.",
      "hint": "Calculate the mean of each numeric column, select the one with the smallest mean, then compute its standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "ea4a01dfa1870e33",
      "_ground_truth": 626.652,
      "_template": "min_mean_column_std"
    },
    {
      "question": "Are there any columns in this sales dataset that contain a high proportion of missing values? Return as JSON with keys: count, columns.",
      "hint": "Compute the percentage of missing values for each column and identify those exceeding a 5% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Are there any columns in this dataset that have a high proportion of missing values (over 10%)? Return as JSON with keys: count, columns.",
      "hint": "Compute missing percentage for each column, compare to a 10% threshold, count and list columns exceeding it.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Which numeric column shows the greatest range, and what are its 10th, 25th, 50th, 75th, and 90th percentiles? Return as JSON with keys: column, p10, p25, p50, p75, p90.",
      "hint": "Identify the numeric column with the largest max\u2011min difference, then compute the requested percentiles.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "percentile_ranking",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (analyzed column), \"p10\" (10th percentile rounded to 4 decimals), \"p25\" (25th percentile), \"p50\" (median), \"p75\" (75th percentile), and \"p90\" (90th percentile). Example: {\"column\": \"income\", \"p10\": 25000.0000, \"p25\": 40000.0000, \"p50\": 55000.0000, \"p75\": 75000.0000, \"p90\": 100000.0000}",
      "ground_truth_hash": "1ef58342a85b8e87",
      "_ground_truth": {
        "column": "Postal Code",
        "p10": 10024.0,
        "p25": 23223.0,
        "p50": 58103.0,
        "p75": 90008.0,
        "p90": 94122.0
      },
      "_template": "percentile_ranking"
    },
    {
      "question": "Which numeric column has the highest variability, and what are its basic descriptive statistics? Return as JSON with keys: column, count, mean, std, min, max, median, skewness, kurtosis.",
      "hint": "Find the numeric column with the largest variance, then compute count, mean, std, min, max, median, skewness, and kurtosis for that column.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "descriptive_summary",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"column\", \"count\" (integer), \"mean\" (rounded to 4 decimals), \"std\", \"min\", \"max\", \"median\", \"skewness\", and \"kurtosis\" (all rounded to 4 decimals). Example: {\"column\": \"age\", \"count\": 1000, \"mean\": 35.4567, \"std\": 12.3456, \"min\": 18.0000, \"max\": 85.0000, \"median\": 34.0000, \"skewness\": 0.5678, \"kurtosis\": -0.2345}",
      "ground_truth_hash": "5cf91ae3c50eac5c",
      "_ground_truth": {
        "column": "Postal Code",
        "count": 9789,
        "mean": 55273.3224,
        "std": 32041.2234,
        "min": 1040.0,
        "max": 99301.0,
        "median": 58103.0,
        "skewness": -0.1313,
        "kurtosis": -1.4927
      },
      "_template": "descriptive_summary"
    }
  ]
}