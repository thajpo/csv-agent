{
  "dataset_columns": [
    "longitude",
    "latitude",
    "housing_median_age",
    "total_rooms",
    "total_bedrooms",
    "population",
    "households",
    "median_income",
    "median_house_value",
    "ocean_proximity"
  ],
  "questions": [
    {
      "question": "Which numeric variable with the greatest variability can be explained by other measurements, and how well do its top three correlated features predict it? Return as JSON with keys: target, predictors, r_squared, adj_r_squared, n_significant, coefficients, p_values.",
      "hint": "Find the numeric column with the highest variance, compute absolute correlations to all other numeric columns, pick the three strongest, fit an OLS regression, and report R\u2011squared, adjusted R\u2011squared, significant predictor count, coefficients, and p\u2011values.",
      "n_steps": 10,
      "difficulty": "VERY_HARD",
      "template_name": "multiple_regression_top_predictors",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"predictors\" (list of 3 column names), \"r_squared\" (rounded to 4 decimals), \"adj_r_squared\" (rounded to 4 decimals), \"n_significant\" (integer count of predictors with p < 0.05), \"coefficients\" (dict mapping predictor names to coefficients rounded to 4 decimals), and \"p_values\" (dict mapping predictor names to p-values rounded to 6 decimals). Example: {\"target\": \"price\", \"predictors\": [\"sqft\", \"bedrooms\", \"age\"], \"r_squared\": 0.7234, \"adj_r_squared\": 0.7156, \"n_significant\": 2, \"coefficients\": {\"sqft\": 123.45, \"bedrooms\": 5000.12, \"age\": -200.34}, \"p_values\": {\"sqft\": 0.000001, \"bedrooms\": 0.023456, \"age\": 0.156789}}",
      "ground_truth_hash": "7430c6f2c55139ed",
      "_ground_truth": {
        "target": "median_house_value",
        "predictors": [
          "median_income",
          "latitude",
          "total_rooms"
        ],
        "r_squared": 0.4815,
        "adj_r_squared": 0.4814,
        "n_significant": 2,
        "coefficients": {
          "median_income": 41407.4262,
          "latitude": -4856.9636,
          "total_rooms": -0.2172
        },
        "p_values": {
          "median_income": 0.0,
          "latitude": 0.0,
          "total_rooms": 0.422214
        }
      },
      "_template": "multiple_regression_top_predictors"
    },
    {
      "question": "Which numeric attribute in the California housing data is the most skewed, and what are its mean, skewness, 95% confidence interval, bootstrap standard error, and number of bootstrap samples? Return as JSON with keys: column, skewness, mean, ci_lower, ci_upper, std_error, n_bootstrap.",
      "hint": "Compute absolute skewness for all numeric columns, select the highest, then bootstrap the mean to obtain the CI and standard error.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "bootstrap_ci_discovered",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (name of most skewed column), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (lower bound of 95% CI, rounded to 4 decimals), \"ci_upper\" (upper bound, rounded to 4 decimals), \"std_error\" (bootstrap standard error, rounded to 4 decimals), and \"n_bootstrap\" (integer, the number of bootstrap samples). Example: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}",
      "ground_truth_hash": "f3108337b663aae5",
      "_ground_truth": {
        "column": "population",
        "skewness": 4.9359,
        "mean": 1425.4767,
        "ci_lower": 1410.9832,
        "ci_upper": 1439.7346,
        "std_error": 7.563,
        "n_bootstrap": 1000
      },
      "_template": "bootstrap_ci_discovered"
    },
    {
      "question": "Which numeric measurement shows the greatest variability and does it differ significantly across the geographic proximity categories? Return as JSON with keys: target_column, grouping_column, n_groups, f_statistic, p_value, significant, best_group, best_mean, worst_group, worst_mean, eta_squared.",
      "hint": "Identify the most variable numeric column, then perform a one\u2011way ANOVA across a categorical column with a moderate number of groups.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "anova_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 11 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"f_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"significant\" (boolean), \"best_group\" (category with highest mean), \"best_mean\" (rounded to 4 decimals), \"worst_group\" (category with lowest mean), \"worst_mean\" (rounded to 4 decimals), and \"eta_squared\" (effect size, rounded to 4 decimals). Example: {\"target_column\": \"sales\", \"grouping_column\": \"region\", \"n_groups\": 4, \"f_statistic\": 15.2345, \"p_value\": 0.000012, \"significant\": true, \"best_group\": \"West\", \"best_mean\": 1234.5678, \"worst_group\": \"East\", \"worst_mean\": 890.1234, \"eta_squared\": 0.1523}",
      "ground_truth_hash": "78cefdd298f56b0d",
      "_ground_truth": {
        "target_column": "median_house_value",
        "grouping_column": "ocean_proximity",
        "n_groups": 5,
        "f_statistic": 1612.1407,
        "p_value": 0.0,
        "significant": "True",
        "best_group": "ISLAND",
        "best_mean": 380440.0,
        "worst_group": "INLAND",
        "worst_mean": 124805.392,
        "eta_squared": 0.2381
      },
      "_template": "anova_discovered_groups"
    },
    {
      "question": "Which numeric attribute in this California housing dataset is closest to a normal distribution? Return as JSON with keys: column, ks_statistic, p_value, is_normal, sample_mean, sample_std.",
      "hint": "Compute skewness for each numeric column, select the one with the smallest absolute skewness, then standardize its values and apply a Kolmogorov\u2011Smirnov test against a standard normal distribution.",
      "n_steps": 7,
      "difficulty": "MEDIUM",
      "template_name": "ks_normality_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (tested column), \"ks_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"is_normal\" (boolean, true if p >= 0.05), \"sample_mean\" (rounded to 4 decimals), and \"sample_std\" (rounded to 4 decimals). Example: {\"column\": \"height\", \"ks_statistic\": 0.0456, \"p_value\": 0.234567, \"is_normal\": true, \"sample_mean\": 170.1234, \"sample_std\": 10.5678}",
      "ground_truth_hash": "7a903f52a2be3bd9",
      "_ground_truth": {
        "column": "housing_median_age",
        "ks_statistic": 0.0605,
        "p_value": 0.0,
        "is_normal": "False",
        "sample_mean": 28.6395,
        "sample_std": 12.5856
      },
      "_template": "ks_normality_test"
    },
    {
      "question": "Do the values of the most variable numeric feature show different variances across the ocean proximity categories? Return as JSON with keys: target_column, grouping_column, n_groups, levene_statistic, p_value, variances_equal, group_variances.",
      "hint": "Find the numeric column with the highest variance, use the categorical column with 2\u20116 unique values as groups, then apply Levene's test to compare variances.",
      "n_steps": 7,
      "difficulty": "HARD",
      "template_name": "levene_variance_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"levene_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"variances_equal\" (boolean, true if p >= 0.05), and \"group_variances\" (dict mapping group names to variance rounded to 4 decimals). Example: {\"target_column\": \"score\", \"grouping_column\": \"class\", \"n_groups\": 3, \"levene_statistic\": 2.3456, \"p_value\": 0.098765, \"variances_equal\": true, \"group_variances\": {\"A\": 123.4567, \"B\": 145.6789, \"C\": 112.3456}}",
      "ground_truth_hash": "59cb5e09c4f064db",
      "_ground_truth": {
        "target_column": "median_house_value",
        "grouping_column": "ocean_proximity",
        "n_groups": 5,
        "levene_statistic": 439.2306,
        "p_value": 0.0,
        "variances_equal": "False",
        "group_variances": {
          "<1H OCEAN": 11262365397.7042,
          "INLAND": 4901107251.7205,
          "ISLAND": 6489843000.0,
          "NEAR BAY": 15084393046.5161,
          "NEAR OCEAN": 15000651274.3856
        }
      },
      "_template": "levene_variance_test"
    },
    {
      "question": "Is the most skewed numeric measurement significantly different between low and high groups defined by a median split of another numeric feature? Return as JSON with keys: target_column, grouping_column, group1, group2, median1, median2, u_statistic, p_value.",
      "hint": "Find the column with highest absolute skewness, create a binary group using the median of a different numeric column, then compare group medians with a Mann\u2011Whitney U test.",
      "n_steps": 6,
      "difficulty": "MEDIUM",
      "template_name": "mann_whitney_u_test",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 8 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"median1\" (rounded to 4 decimals), \"median2\" (rounded to 4 decimals), \"u_statistic\" (rounded to 2 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target_column\": \"score\", \"grouping_column\": \"treatment\", \"group1\": \"control\", \"group2\": \"experimental\", \"median1\": 72.5000, \"median2\": 78.0000, \"u_statistic\": 1234.50, \"p_value\": 0.034567}",
      "ground_truth_hash": "475312e47b635670",
      "_ground_truth": {
        "target_column": "population",
        "grouping_column": "_binary_group",
        "group1": "low",
        "group2": "high",
        "median1": 1096.0,
        "median2": 1236.0,
        "u_statistic": 46623227.0,
        "p_value": 0.0
      },
      "_template": "mann_whitney_u_test"
    },
    {
      "question": "Do the two most skewed numeric variables in this housing dataset show a stronger relationship after a log transformation? Return as JSON with keys: columns, original_correlation, log_correlation, improvement, transformation_helpful.",
      "hint": "Find the most skewed positive columns, compute their Pearson correlation, apply a log1p transform, recompute correlation, and compare absolute values.",
      "n_steps": 6,
      "difficulty": "HARD",
      "template_name": "correlation_change_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 5 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (Pearson, rounded to 4 decimals), \"log_correlation\" (after log transform, rounded to 4 decimals), \"improvement\" (absolute difference, rounded to 4 decimals), and \"transformation_helpful\" (boolean, true if |log_corr| > |orig_corr|). Example: {\"columns\": [\"income\", \"spending\"], \"original_correlation\": 0.4567, \"log_correlation\": 0.7234, \"improvement\": 0.2667, \"transformation_helpful\": true}",
      "ground_truth_hash": "1710d688f65f3e52",
      "_ground_truth": {
        "columns": [
          "population",
          "total_rooms"
        ],
        "original_correlation": 0.8571,
        "log_correlation": 0.865,
        "improvement": 0.0079,
        "transformation_helpful": "True"
      },
      "_template": "correlation_change_analysis"
    },
    {
      "question": "Which numeric feature in the California housing data has the greatest variability, and what are its quartile and outlier statistics? Return as JSON with keys: column, q1, q3, iqr, lower_fence, upper_fence, n_outliers.",
      "hint": "Identify the numeric column with the highest variance, then compute its Q1, Q3, IQR, the lower/upper fences, and count the outliers.",
      "n_steps": 5,
      "difficulty": "EASY",
      "template_name": "iqr_outlier_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (analyzed column), \"q1\" (25th percentile rounded to 4 decimals), \"q3\" (75th percentile rounded to 4 decimals), \"iqr\" (rounded to 4 decimals), \"lower_fence\" (rounded to 4 decimals), \"upper_fence\" (rounded to 4 decimals), and \"n_outliers\" (integer count). Example: {\"column\": \"salary\", \"q1\": 45000.0000, \"q3\": 75000.0000, \"iqr\": 30000.0000, \"lower_fence\": 0.0000, \"upper_fence\": 120000.0000, \"n_outliers\": 23}",
      "ground_truth_hash": "7f882e0aaddc798c",
      "_ground_truth": {
        "column": "median_house_value",
        "q1": 119600.0,
        "q3": 264725.0,
        "iqr": 145125.0,
        "lower_fence": -98087.5,
        "upper_fence": 482412.5,
        "n_outliers": 1071
      },
      "_template": "iqr_outlier_analysis"
    },
    {
      "question": "Which numeric measurement in this housing dataset shows the greatest variability, and does its distribution appear normal or skewed? Return as JSON with keys: distribution, median, iqr (or mean, std).",
      "hint": "Find the numeric column with the highest variance, sample up to 5000 rows if needed, run a Shapiro\u2011Wilk test, and report mean/std for normal or median/IQR for non\u2011normal.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "conditional_normality",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 3 keys. If normal: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}. If non-normal: {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. The \"iqr\" value is Q3 minus Q1 as a single number. All numeric values rounded to 3 decimal places.",
      "ground_truth_hash": "c88078bdead6124c",
      "_ground_truth": {
        "distribution": "non-normal",
        "median": 179700.0,
        "iqr": 145125.0
      },
      "_template": "conditional_normality"
    },
    {
      "question": "Which ocean proximity category shows the highest average house value? Return as JSON with keys: category_column, best_category, target_column, mean_value.",
      "hint": "Identify the most variable numeric column, group by a suitable categorical column, compute means, and select the category with the highest mean.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "category_highest_target_mean",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"category_column\" (the grouping column name), \"best_category\" (the category value with highest mean), \"target_column\" (the numeric column analyzed), and \"mean_value\" (rounded to 3 decimal places). Example: {\"category_column\": \"region\", \"best_category\": \"West\", \"target_column\": \"sales\", \"mean_value\": 1234.567}",
      "ground_truth_hash": "0c5b6f6c02890afe",
      "_ground_truth": {
        "category_column": "ocean_proximity",
        "best_category": "ISLAND",
        "target_column": "median_house_value",
        "mean_value": 380440.0
      },
      "_template": "category_highest_target_mean"
    },
    {
      "question": "Which pair of numeric variables in this housing dataset are most strongly correlated, and how does that relationship change after removing extreme outliers? Return as JSON with keys: columns, original_correlation, outliers_removed, clean_correlation.",
      "hint": "Find the highest absolute correlation among numeric columns, drop rows where either variable is beyond 3 standard deviations, then recompute the correlation.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "c1ff91595bb2b720",
      "_ground_truth": {
        "columns": [
          "households",
          "total_bedrooms"
        ],
        "original_correlation": 0.98,
        "outliers_removed": 605,
        "clean_correlation": 0.975
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "How many numeric features contain extreme values and what is the total number of such outliers? Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "For each numeric column, compute its mean and standard deviation and count values farther than 3\u202f\u00d7\u202fstd from the mean.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "d415771c4544e43a",
      "_ground_truth": {
        "columns_with_outliers": 5,
        "total_outliers": 1791
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "I wonder how many numeric features contain outliers and what the total number of outlier values is across the dataset. Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "Flag values in each numeric column that lie beyond 2.5\u202f\u00d7\u202fstandard deviation from the mean, then count how many columns have any such values and sum all flagged instances.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "97465cb282b9db18",
      "_ground_truth": {
        "columns_with_outliers": 8,
        "total_outliers": 3689
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "Which numeric column has the highest coefficient of variation, and what are its mean and standard deviation? Return as JSON with keys: column, cv, mean, std.",
      "hint": "Calculate CV = (std / mean) * 100 for each numeric column, identify the column with the largest CV, then report its mean and std.",
      "n_steps": 4,
      "difficulty": "EASY",
      "template_name": "coefficient_of_variation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"column\" (name of column with highest CV), \"cv\" (coefficient of variation as percentage rounded to 2 decimals), \"mean\" (rounded to 4 decimals), and \"std\" (rounded to 4 decimals). Example: {\"column\": \"price\", \"cv\": 45.23, \"mean\": 1234.5678, \"std\": 558.4567}",
      "ground_truth_hash": "3809cc7dbaa67efd",
      "_ground_truth": {
        "column": "total_rooms",
        "cv": 82.77,
        "mean": 2635.7631,
        "std": 2181.6153
      },
      "_template": "coefficient_of_variation"
    },
    {
      "question": "Which two numeric variables in this California housing dataset show the strongest linear relationship? Return as JSON with keys: columns, correlation.",
      "hint": "Compute the absolute correlation matrix for numeric columns and identify the off\u2011diagonal pair with the highest value.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "bb63250bec55407b",
      "_ground_truth": {
        "columns": [
          "households",
          "total_bedrooms"
        ],
        "correlation": 0.98
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Which pair of numeric variables in the dataset have the weakest correlation? Return as JSON with keys: columns, correlation.",
      "hint": "Compute the absolute correlation matrix for numeric columns, ignore the diagonal and zero entries, then locate the minimum value.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "4e8cc0d63fdf520a",
      "_ground_truth": {
        "columns": [
          "median_income",
          "population"
        ],
        "correlation": 0.005
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "Which numeric variable shows the greatest variability and what is its average? Return as JSON with keys: mean.",
      "hint": "Identify the numeric column with the highest variance, then compute its mean.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "c9a09278573be259",
      "_ground_truth": 206855.817,
      "_template": "max_variance_mean"
    },
    {
      "question": "Which numeric column has the smallest mean, and what is its standard deviation? Return as JSON with keys: result.",
      "hint": "Find the numeric column with the lowest average value, then compute its standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "7a9748017639979f",
      "_ground_truth": 2.004,
      "_template": "min_mean_column_std"
    },
    {
      "question": "Are there any columns in the dataset that have a high proportion of missing values? Return as JSON with keys: count, columns.",
      "hint": "Compute missing percentages for each column and list those where the missing rate exceeds 5%.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "I wonder if any columns in this housing dataset have a high proportion of missing values. Return as JSON with keys: count, columns.",
      "hint": "Calculate missing percentages for each column, count how many exceed 10%, and list those column names.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Which numeric attribute varies the most across districts, and what are its key percentile values? Return as JSON with keys: column, p10, p25, p50, p75, p90.",
      "hint": "Find the numeric column with the largest range, then calculate its 10th, 25th, 50th, 75th, and 90th percentiles.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "percentile_ranking",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"column\" (analyzed column), \"p10\" (10th percentile rounded to 4 decimals), \"p25\" (25th percentile), \"p50\" (median), \"p75\" (75th percentile), and \"p90\" (90th percentile). Example: {\"column\": \"income\", \"p10\": 25000.0000, \"p25\": 40000.0000, \"p50\": 55000.0000, \"p75\": 75000.0000, \"p90\": 100000.0000}",
      "ground_truth_hash": "5f70607297933a75",
      "_ground_truth": {
        "column": "median_house_value",
        "p10": 82300.0,
        "p25": 119600.0,
        "p50": 179700.0,
        "p75": 264725.0,
        "p90": 376600.0
      },
      "_template": "percentile_ranking"
    },
    {
      "question": "Which numeric attribute varies the most across the California districts, and what are its basic descriptive statistics? Return as JSON with keys: column, count, mean, std, min, max, median, skewness, kurtosis.",
      "hint": "Find the numeric column with the highest variance, then compute its count, mean, standard deviation, minimum, maximum, median, skewness, and kurtosis.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "descriptive_summary",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"column\", \"count\" (integer), \"mean\" (rounded to 4 decimals), \"std\", \"min\", \"max\", \"median\", \"skewness\", and \"kurtosis\" (all rounded to 4 decimals). Example: {\"column\": \"age\", \"count\": 1000, \"mean\": 35.4567, \"std\": 12.3456, \"min\": 18.0000, \"max\": 85.0000, \"median\": 34.0000, \"skewness\": 0.5678, \"kurtosis\": -0.2345}",
      "ground_truth_hash": "c3e56fea623dc9f4",
      "_ground_truth": {
        "column": "median_house_value",
        "count": 20640,
        "mean": 206855.8169,
        "std": 115395.6159,
        "min": 14999.0,
        "max": 500001.0,
        "median": 179700.0,
        "skewness": 0.9778,
        "kurtosis": 0.3279
      },
      "_template": "descriptive_summary"
    }
  ]
}