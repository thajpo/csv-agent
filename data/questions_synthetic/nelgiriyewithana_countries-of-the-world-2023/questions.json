{
  "dataset_columns": [
    "Country",
    "Density\n(P/Km2)",
    "Abbreviation",
    "Agricultural Land( %)",
    "Land Area(Km2)",
    "Armed Forces size",
    "Birth Rate",
    "Calling Code",
    "Capital/Major City",
    "Co2-Emissions",
    "CPI",
    "CPI Change (%)",
    "Currency-Code",
    "Fertility Rate",
    "Forested Area (%)",
    "Gasoline Price",
    "GDP",
    "Gross primary education enrollment (%)",
    "Gross tertiary education enrollment (%)",
    "Infant mortality",
    "Largest city",
    "Life expectancy",
    "Maternal mortality ratio",
    "Minimum wage",
    "Official language",
    "Out of pocket health expenditure",
    "Physicians per thousand",
    "Population",
    "Population: Labor force participation (%)",
    "Tax revenue (%)",
    "Total tax rate",
    "Unemployment rate",
    "Urban_population",
    "Latitude",
    "Longitude"
  ],
  "questions": [
    {
      "question": "Which two numeric variables in the dataset show the greatest change in correlation after applying a log transformation? Return as JSON with keys: columns, original_correlation, log_correlation, improvement, transformation_helpful.",
      "hint": "Select the two most skewed positive numeric columns, compute their Pearson correlation, apply a log transformation, recompute the correlation, and compare the absolute values.",
      "n_steps": 6,
      "difficulty": "HARD",
      "template_name": "correlation_change_analysis",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 5 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (Pearson, rounded to 4 decimals), \"log_correlation\" (after log transform, rounded to 4 decimals), \"improvement\" (absolute difference, rounded to 4 decimals), and \"transformation_helpful\" (boolean, true if |log_corr| > |orig_corr|). Example: {\"columns\": [\"income\", \"spending\"], \"original_correlation\": 0.4567, \"log_correlation\": 0.7234, \"improvement\": 0.2667, \"transformation_helpful\": true}",
      "ground_truth_hash": "4f999b9bd0787c8c",
      "_ground_truth": {
        "columns": [
          "Birth Rate",
          "Calling Code"
        ],
        "original_correlation": 0.0721,
        "log_correlation": 0.2577,
        "improvement": 0.1856,
        "transformation_helpful": "True"
      },
      "_template": "correlation_change_analysis"
    },
    {
      "question": "Which two numeric variables in this dataset exhibit the strongest correlation, and how does that relationship shift after discarding extreme outliers? Return as JSON with keys: columns, original_correlation, outliers_removed, clean_correlation.",
      "hint": "Calculate absolute Pearson correlations among all numeric columns, pick the pair with the highest value, remove rows where either variable lies beyond three standard deviations from its mean, then recompute the correlation.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "bc5ca1a00d8bbc30",
      "_ground_truth": {
        "columns": [
          "Birth Rate",
          "Fertility Rate"
        ],
        "original_correlation": 0.981,
        "outliers_removed": 8,
        "clean_correlation": 0.982
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "Are there many extreme values in the numeric data, and how many columns actually contain outliers? Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "Compute mean and standard deviation for each numeric column and flag values farther than 3\u202f\u00d7\u202fstd from the mean; then count affected columns and total flagged rows.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "54a35f2bff3ee3f7",
      "_ground_truth": {
        "columns_with_outliers": 5,
        "total_outliers": 9
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "I\u2019m curious how many numeric columns have any extreme values and how many outlier records exist overall. Return as JSON with keys: columns_with_outliers, total_outliers.",
      "hint": "For each numeric column, flag values that lie more than 2.5 standard deviations away from the mean, then count columns with at least one flag and sum all flags.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "f9b6f8b6d1ac3c23",
      "_ground_truth": {
        "columns_with_outliers": 9,
        "total_outliers": 27
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "Which numeric measurement in the dataset shows the greatest relative variability? Return as JSON with keys: column, cv, mean, std.",
      "hint": "Calculate the coefficient of variation (std/mean*100) for each numeric column and pick the one with the highest value.",
      "n_steps": 4,
      "difficulty": "EASY",
      "template_name": "coefficient_of_variation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"column\" (name of column with highest CV), \"cv\" (coefficient of variation as percentage rounded to 2 decimals), \"mean\" (rounded to 4 decimals), and \"std\" (rounded to 4 decimals). Example: {\"column\": \"price\", \"cv\": 45.23, \"mean\": 1234.5678, \"std\": 558.4567}",
      "ground_truth_hash": "c1a2df2dd59c01fc",
      "_ground_truth": {
        "column": "Longitude",
        "cv": 329.75,
        "mean": 20.2324,
        "std": 66.7161
      },
      "_template": "coefficient_of_variation"
    },
    {
      "question": "Which two numeric measurements in this dataset are most strongly linearly related? Return as JSON with keys: columns, correlation.",
      "hint": "Compute the absolute correlation matrix for all numeric columns and identify the pair with the highest off\u2011diagonal correlation.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "4fd2607a40550b2d",
      "_ground_truth": {
        "columns": [
          "Birth Rate",
          "Fertility Rate"
        ],
        "correlation": 0.981
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Which two numeric columns have the weakest correlation? Return as JSON with keys: columns, correlation.",
      "hint": "Compute the absolute correlation matrix for numeric columns, set the diagonal to NaN, then find the minimum non\u2011zero correlation.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "d36b9abcbe9d322e",
      "_ground_truth": {
        "columns": [
          "Calling Code",
          "Latitude"
        ],
        "correlation": 0.013
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "Which numeric measurement shows the greatest variability across countries, and what is its average value? Return as JSON with key: mean.",
      "hint": "Calculate variances of all numeric columns, identify the highest, then compute its mean.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "00b9501bc8c3fee7",
      "_ground_truth": 360.546,
      "_template": "max_variance_mean"
    },
    {
      "question": "Which numeric variable has the lowest mean and what is its standard deviation? Return as JSON with keys: result.",
      "hint": "Identify the numeric column with the smallest average, then compute its standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "4d57664587737b92",
      "_ground_truth": 1.684,
      "_template": "min_mean_column_std"
    },
    {
      "question": "Which columns in the dataset have more than 5% missing values? Return as JSON with keys: count, columns.",
      "hint": "Compute the missing\u2011value percentage for each column and select those exceeding a 5% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "bd0807e4c4419932",
      "_ground_truth": {
        "count": 12,
        "columns": [
          "Armed Forces size",
          "CPI",
          "CPI Change (%)",
          "Currency-Code",
          "Gasoline Price",
          "Gross tertiary education enrollment (%)",
          "Maternal mortality ratio",
          "Minimum wage",
          "Population: Labor force participation (%)",
          "Tax revenue (%)",
          "Total tax rate",
          "Unemployment rate"
        ]
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Which columns in the dataset have a relatively high amount of missing data?",
      "hint": "Calculate the percentage of missing values per column and list those exceeding 10%.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "772d517f7e2a362d",
      "_ground_truth": {
        "count": 4,
        "columns": [
          "Armed Forces size",
          "Gasoline Price",
          "Minimum wage",
          "Tax revenue (%)"
        ]
      },
      "_template": "count_high_missing_columns"
    }
  ]
}