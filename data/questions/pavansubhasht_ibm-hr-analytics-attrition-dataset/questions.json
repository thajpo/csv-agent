{
  "dataset_columns": [
    "Age",
    "Attrition",
    "BusinessTravel",
    "DailyRate",
    "Department",
    "DistanceFromHome",
    "Education",
    "EducationField",
    "EmployeeCount",
    "EmployeeNumber",
    "EnvironmentSatisfaction",
    "Gender",
    "HourlyRate",
    "JobInvolvement",
    "JobLevel",
    "JobRole",
    "JobSatisfaction",
    "MaritalStatus",
    "MonthlyIncome",
    "MonthlyRate",
    "NumCompaniesWorked",
    "Over18",
    "OverTime",
    "PercentSalaryHike",
    "PerformanceRating",
    "RelationshipSatisfaction",
    "StandardHours",
    "StockOptionLevel",
    "TotalWorkingYears",
    "TrainingTimesLastYear",
    "WorkLifeBalance",
    "YearsAtCompany",
    "YearsInCurrentRole",
    "YearsSinceLastPromotion",
    "YearsWithCurrManager"
  ],
  "questions": [
    {
      "question": "Identify the numeric column that exhibits the highest variance in the dataset and treat this column as the target variable. Among the remaining numeric columns, select the three that have the largest absolute correlation with the target column. Using these three columns as predictors, fit an ordinary least squares (OLS) multiple linear regression model to predict the target. Provide the results as a JSON object with exactly the following keys and formatting:\n\n- \"target\": the name of the target column (string)\n- \"predictors\": a list of the three predictor column names (list of strings, order does not matter)\n- \"r_squared\": the model's R-squared value rounded to 4 decimal places (number)\n- \"adj_r_squared\": the adjusted R-squared value rounded to 4 decimal places (number)\n- \"n_significant\": the count of predictors whose p\u2011value is less than 0.05 (integer)\n- \"coefficients\": an object mapping each predictor name to its regression coefficient, rounded to 4 decimal places (e.g., {\"col1\": 123.4567, \"col2\": -0.8910, \"col3\": 45.6789})\n- \"p_values\": an object mapping each predictor name to its p\u2011value, rounded to 6 decimal places (e.g., {\"col1\": 0.012345, \"col2\": 0.678901, \"col3\": 0.000123})\n\nReturn only the JSON object described above.",
      "hint": "Use pandas to compute variances and identify the column with the maximum variance, then calculate absolute correlations between this target and the other numeric columns to choose the top three. Fit the regression with statsmodels OLS and extract R\u2011squared, adjusted R\u2011squared, coefficients, and p\u2011values.",
      "n_steps": 10,
      "difficulty": "VERY_HARD",
      "template_name": "multiple_regression_top_predictors",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"predictors\" (list of 3 column names), \"r_squared\" (rounded to 4 decimals), \"adj_r_squared\" (rounded to 4 decimals), \"n_significant\" (integer count of predictors with p < 0.05), \"coefficients\" (dict mapping predictor names to coefficients rounded to 4 decimals), and \"p_values\" (dict mapping predictor names to p-values rounded to 6 decimals). Example: {\"target\": \"price\", \"predictors\": [\"sqft\", \"bedrooms\", \"age\"], \"r_squared\": 0.7234, \"adj_r_squared\": 0.7156, \"n_significant\": 2, \"coefficients\": {\"sqft\": 123.45, \"bedrooms\": 5000.12, \"age\": -200.34}, \"p_values\": {\"sqft\": 0.000001, \"bedrooms\": 0.023456, \"age\": 0.156789}}",
      "ground_truth_hash": "5e99b9f2c15d5b88",
      "_ground_truth": {
        "target": "MonthlyRate",
        "predictors": [
          "JobLevel",
          "EnvironmentSatisfaction",
          "YearsWithCurrManager"
        ],
        "r_squared": 0.006,
        "adj_r_squared": 0.004,
        "n_significant": 2,
        "coefficients": {
          "JobLevel": 398.4434,
          "EnvironmentSatisfaction": 242.4003,
          "YearsWithCurrManager": -119.3182
        },
        "p_values": {
          "JobLevel": 0.027553,
          "EnvironmentSatisfaction": 0.153042,
          "YearsWithCurrManager": 0.033403
        }
      },
      "_template": "multiple_regression_top_predictors"
    },
    {
      "question": "Identify the numeric column that has the greatest absolute skewness. For that column, calculate its absolute skewness, the sample mean, a 95% confidence interval for the mean obtained via bootstrap resampling with 1000 samples (report the lower and upper bounds), and the bootstrap standard error of the mean. Provide your answer as a JSON object with exactly the following keys: \"column\" (the column name), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (rounded to 4 decimals), \"ci_upper\" (rounded to 4 decimals), \"std_error\" (rounded to 4 decimals), and \"n_bootstrap\" (the integer 1000).",
      "hint": "Compute the absolute skewness for each numeric column, select the column with the highest value, then perform 1000 bootstrap resamples on its non\u2011missing values to obtain the mean, its 95% CI, and the standard error.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "bootstrap_ci_discovered",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (name of most skewed column), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (lower bound of 95% CI, rounded to 4 decimals), \"ci_upper\" (upper bound, rounded to 4 decimals), \"std_error\" (bootstrap standard error, rounded to 4 decimals), and \"n_bootstrap\" (integer, the number of bootstrap samples). Example: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}",
      "ground_truth_hash": "7e9e4be36e12d9ac",
      "_ground_truth": {
        "column": "YearsSinceLastPromotion",
        "skewness": 1.9843,
        "mean": 2.1878,
        "ci_lower": 2.0265,
        "ci_upper": 2.3545,
        "std_error": 0.0841,
        "n_bootstrap": 1000
      },
      "_template": "bootstrap_ci_discovered"
    },
    {
      "question": "Using the given dataset, first locate the numeric column that has the highest variance among all numeric columns. Then identify a categorical column that contains between 3 and 10 distinct categories. Perform a one\u2011way ANOVA to test whether the means of the high\u2011variance numeric column differ across the groups defined by the selected categorical column. Report the results as a JSON object with exactly the following 11 keys:\n\n- \"target_column\": name of the numeric column with highest variance\n- \"grouping_column\": name of the categorical column with 3\u201110 groups\n- \"n_groups\": number of distinct groups (integer)\n- \"f_statistic\": F\u2011statistic rounded to 4 decimal places\n- \"p_value\": p\u2011value rounded to 6 decimal places\n- \"significant\": boolean indicating whether p\u202f<\u202f0.05\n- \"best_group\": the category with the highest mean of the target column\n- \"best_mean\": that highest mean rounded to 4 decimal places\n- \"worst_group\": the category with the lowest mean of the target column\n- \"worst_mean\": that lowest mean rounded to 4 decimal places\n- \"eta_squared\": eta\u2011squared effect size rounded to 4 decimal places\n\nProvide the answer exactly in this JSON format.",
      "hint": "Calculate the variance for each numeric column to find the one with the largest value, then count unique values for each categorical column to pick one with 3\u201110 categories. Group the data by that categorical column, compute each group's mean of the target column, and use scipy.stats.f_oneway for the ANOVA. From the group means, identify the highest and lowest, and compute eta\u2011squared as (SS_between / SS_total).",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "anova_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 11 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"f_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"significant\" (boolean), \"best_group\" (category with highest mean), \"best_mean\" (rounded to 4 decimals), \"worst_group\" (category with lowest mean), \"worst_mean\" (rounded to 4 decimals), and \"eta_squared\" (effect size, rounded to 4 decimals). Example: {\"target_column\": \"sales\", \"grouping_column\": \"region\", \"n_groups\": 4, \"f_statistic\": 15.2345, \"p_value\": 0.000012, \"significant\": true, \"best_group\": \"West\", \"best_mean\": 1234.5678, \"worst_group\": \"East\", \"worst_mean\": 890.1234, \"eta_squared\": 0.1523}",
      "ground_truth_hash": "0c4cfbf1ae9a5d1e",
      "_ground_truth": {
        "target_column": "MonthlyRate",
        "grouping_column": "BusinessTravel",
        "n_groups": 3,
        "f_statistic": 0.1773,
        "p_value": 0.837557,
        "significant": "False",
        "best_group": "Non-Travel",
        "best_mean": 14635.6,
        "worst_group": "Travel_Rarely",
        "worst_mean": 14265.373,
        "eta_squared": 0.0002
      },
      "_template": "anova_discovered_groups"
    },
    {
      "question": "Determine the numeric column that exhibits the highest variance and treat it as the target variable. From the remaining numeric columns, identify the column that has the largest absolute correlation with this target. Using only these two columns (and discarding any rows with missing values), fit a simple ordinary least squares regression of the target on the identified predictor. Report the following information: the name of the target column, the name of the predictor column, the absolute correlation between them (rounded to 3 decimal places), the R-squared of the regression model (rounded to 4 decimal places), the regression coefficient for the predictor (rounded to 4 decimal places), and the p\u2011value associated with that coefficient (rounded to 6 decimal places). Provide your answer as a JSON object with exactly these six keys: \"target\", \"best_predictor\", \"correlation\", \"r_squared\", \"coefficient\", and \"p_value\".",
      "hint": "Compute variances to find the column with the highest variance, then calculate absolute correlations of all other numeric columns with it, pick the strongest, and run a simple OLS regression to obtain R\u2011squared, coefficient, and p\u2011value.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "regression_most_predictive",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"best_predictor\" (column name), \"correlation\" (rounded to 3 decimals), \"r_squared\" (rounded to 4 decimals), \"coefficient\" (rounded to 4 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target\": \"price\", \"best_predictor\": \"sqft\", \"correlation\": 0.834, \"r_squared\": 0.6956, \"coefficient\": 135.2847, \"p_value\": 0.000001}",
      "ground_truth_hash": "224cbd2a84bebb8a",
      "_ground_truth": {
        "target": "MonthlyRate",
        "best_predictor": "JobLevel",
        "correlation": 0.04,
        "r_squared": 0.0016,
        "coefficient": 254.3956,
        "p_value": 0.129476
      },
      "_template": "regression_most_predictive"
    },
    {
      "question": "Find the numeric column that exhibits the highest variance in the dataset. Next, locate a categorical column that contains exactly two distinct categories (if no such column exists, create a binary grouping by splitting any other numeric column at its median). Using the two groups defined by this binary column, calculate the mean of the previously identified high\u2011variance numeric column for each group, perform an independent two\u2011sample t\u2011test comparing the groups, and determine whether the difference is statistically significant at \u03b1 = 0.05. Report your findings as a JSON object with exactly the following keys: \"target_column\" (the name of the high\u2011variance numeric column), \"grouping_column\" (the name of the binary categorical column or the name you gave to the created binary column), \"group1\" (the first group label), \"group2\" (the second group label), \"mean1\" (mean for group1 rounded to 4 decimal places), \"mean2\" (mean for group2 rounded to 4 decimal places), \"t_statistic\" (t\u2011statistic rounded to 4 decimal places), \"p_value\" (p\u2011value rounded to 6 decimal places), and \"significant\" (boolean, true if p\u202f<\u202f0.05).",
      "hint": "Start by computing variances for all numeric columns, then look for a categorical column with exactly two unique values; if none, turn a numeric column into a high/low binary split using its median.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "ttest_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"mean1\" (rounded to 4 decimals), \"mean2\" (rounded to 4 decimals), \"t_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"significant\" (boolean, true if p < 0.05). Example: {\"target_column\": \"score\", \"grouping_column\": \"gender\", \"group1\": \"M\", \"group2\": \"F\", \"mean1\": 75.4321, \"mean2\": 78.1234, \"t_statistic\": -2.3456, \"p_value\": 0.019234, \"significant\": true}",
      "ground_truth_hash": "98a59b74ffe56123",
      "_ground_truth": {
        "target_column": "MonthlyRate",
        "grouping_column": "Attrition",
        "group1": "Yes",
        "group2": "No",
        "mean1": 14559.308,
        "mean2": 14265.7794,
        "t_statistic": 0.5813,
        "p_value": 0.561124,
        "significant": "False"
      },
      "_template": "ttest_discovered_groups"
    },
    {
      "question": "Identify the numeric column that has the highest variance in the dataset and assess whether its distribution is normal. If the distribution is normal, provide its mean and standard deviation; if it is non\u2011normal, provide its median and inter\u2011quartile range (IQR). Return the result as a JSON object with exactly three keys: for a normal distribution use {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}; for a non\u2011normal distribution use {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. Round all numeric values to three decimal places.",
      "hint": "First locate the numeric column with the greatest variance, then perform a normality test (e.g., Shapiro\u2011Wilk). Based on the test outcome, compute either mean/std or median/IQR.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "conditional_normality",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 3 keys. If normal: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}. If non-normal: {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. The \"iqr\" value is Q3 minus Q1 as a single number. All numeric values rounded to 3 decimal places.",
      "ground_truth_hash": "2625ebc728ae5889",
      "_ground_truth": {
        "distribution": "non-normal",
        "median": 14235.5,
        "iqr": 12414.5
      },
      "_template": "conditional_normality"
    },
    {
      "question": "Identify the numeric column that has the highest variance. Then choose a categorical column whose number of distinct values is between 2 and 20. For each category of that chosen categorical column, compute the mean of the selected numeric column and determine which category yields the highest mean. Provide your answer as a JSON object with exactly four keys: \"category_column\" (the name of the categorical column you used), \"best_category\" (the category value with the highest mean), \"target_column\" (the name of the numeric column with highest variance), and \"mean_value\" (the highest mean rounded to three decimal places). Example format: {\"category_column\": \"...\", \"best_category\": \"...\", \"target_column\": \"...\", \"mean_value\": 123.456}",
      "hint": "First calculate the variance for all numeric columns to find the most variable one, then inspect the distinct counts of categorical columns to pick one with a moderate number of categories before aggregating.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "category_highest_target_mean",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"category_column\" (the grouping column name), \"best_category\" (the category value with highest mean), \"target_column\" (the numeric column analyzed), and \"mean_value\" (rounded to 3 decimal places). Example: {\"category_column\": \"region\", \"best_category\": \"West\", \"target_column\": \"sales\", \"mean_value\": 1234.567}",
      "ground_truth_hash": "2f065bb940a4440e",
      "_ground_truth": {
        "category_column": "Attrition",
        "best_category": "Yes",
        "target_column": "MonthlyRate",
        "mean_value": 14559.308
      },
      "_template": "category_highest_target_mean"
    },
    {
      "question": "Find the pair of numeric columns that exhibit the strongest absolute Pearson correlation in the dataset. Report the original correlation between this pair, then remove any rows where the value in either of the two columns lies more than three standard deviations away from its column mean. After removing those outlier rows, recompute the correlation for the same pair and count how many rows were removed. Provide your answer as a JSON object with exactly four keys: \"columns\" (a list of the two column names, alphabetically sorted), \"original_correlation\" (the original correlation rounded to three decimal places), \"outliers_removed\" (the integer number of rows removed), and \"clean_correlation\" (the correlation after outlier removal rounded to three decimal places).",
      "hint": "Compute the correlation matrix for all numeric columns, locate the maximum off\u2011diagonal absolute value to identify the pair, then apply a 3\u2011standard\u2011deviation filter on both columns before recalculating the correlation.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "0911ed6f7f1080e1",
      "_ground_truth": {
        "columns": [
          "JobLevel",
          "MonthlyIncome"
        ],
        "original_correlation": 0.95,
        "outliers_removed": 0,
        "clean_correlation": 0.95
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "Which pair of numeric columns exhibits the strongest absolute correlation? Provide your answer as a JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places).",
      "hint": "Calculate the absolute correlation matrix for all numeric columns and identify the largest off\u2011diagonal value.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "da82dde3f2359318",
      "_ground_truth": {
        "columns": [
          "JobLevel",
          "MonthlyIncome"
        ],
        "correlation": 0.95
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Which two numeric columns have the smallest absolute Pearson correlation coefficient among all distinct pairs of numeric columns (ignore self\u2011correlation)? Provide your answer as a JSON object with keys \"columns\" (a list of the two column names sorted alphabetically) and \"correlation\" (the absolute correlation value rounded to three decimal places).",
      "hint": "Compute the correlation matrix for numeric columns, mask the diagonal, then find the minimum non\u2011zero correlation value.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "2053df44aa573c0e",
      "_ground_truth": {
        "columns": [
          "DailyRate",
          "PerformanceRating"
        ],
        "correlation": 0.0
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "For the dataset, treat each numeric column separately and define an outlier as any value whose absolute distance from the column's mean is greater than three times that column's standard deviation. How many numeric columns contain at least one outlier, and what is the total number of outlier values across all numeric columns? Provide your answer as a JSON object with the keys \"columns_with_outliers\" (integer) and \"total_outliers\" (integer).",
      "hint": "Iterate over all numeric columns, compute mean and standard deviation, count values beyond 3\u202f\u00d7\u202fstd for each column, then sum the counts and count how many columns have a non\u2011zero count.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "1b8da3bbbab887c7",
      "_ground_truth": {
        "columns_with_outliers": 5,
        "total_outliers": 110
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "For the dataset, consider only the numeric columns. Define an outlier as a value whose absolute deviation from its column mean exceeds 2.5 times the column's standard deviation. Determine (1) how many numeric columns contain at least one outlier, and (2) the total number of outlier values across all numeric columns. Provide your answer as a JSON object with exactly two keys: \"columns_with_outliers\" (integer) and \"total_outliers\" (integer).",
      "hint": "Compute the mean and standard deviation for each numeric column, flag rows where |value - mean| > 2.5 * std, then count columns with any flags and sum all flagged entries.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "90f31e3d5c21fd7e",
      "_ground_truth": {
        "columns_with_outliers": 11,
        "total_outliers": 492
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "What is the mean of the numeric column that has the highest variance? Provide your answer as a single number rounded to 3 decimal places.",
      "hint": "Identify the numeric column with the greatest variance, then calculate its average.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "9fc7e7cfae8ff8a1",
      "_ground_truth": 14313.103,
      "_template": "max_variance_mean"
    },
    {
      "question": "What is the standard deviation of the numeric column that has the smallest mean? Provide your answer as a single number rounded to 3 decimal places.",
      "hint": "Calculate the mean for each numeric column, find the column with the lowest mean, then compute its standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "0d425f5d0b0f7f12",
      "_ground_truth": 0.852,
      "_template": "min_mean_column_std"
    },
    {
      "question": "How many columns in the dataset have more than 5% missing values, and which columns are they? Provide your answer as a JSON object with keys \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted).",
      "hint": "Compute the percentage of missing entries for each column and compare it to a 5% threshold to identify the high\u2011missing columns.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "How many columns in the dataset have more than 10% missing values, and which columns are they? Provide your answer as a JSON object with keys \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted).",
      "hint": "Calculate the proportion of missing entries for each column and identify those exceeding the 10% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    }
  ]
}