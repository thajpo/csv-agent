{
  "dataset_columns": [
    "gender",
    "race/ethnicity",
    "parental level of education",
    "lunch",
    "test preparation course",
    "math score",
    "reading score",
    "writing score"
  ],
  "questions": [
    {
      "question": "Identify the numeric column that exhibits the highest absolute skewness. For that column, compute the absolute skewness (rounded to 4 decimal places), the original sample mean (rounded to 4 decimal places), a 95% confidence interval for the mean using bootstrap resampling with 1000 samples (report the lower and upper bounds rounded to 4 decimal places), the bootstrap standard error of the mean (rounded to 4 decimal places), and the number of bootstrap samples used. Provide your answer as a JSON object with exactly the following keys: \"column\" (the name of the most skewed column), \"skewness\", \"mean\", \"ci_lower\", \"ci_upper\", \"std_error\", and \"n_bootstrap\" (integer). Example format: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}.",
      "hint": "First calculate the skewness for each numeric column, pick the one with the largest absolute value, then use bootstrap sampling (1000 resamples) on its non\u2011missing values to estimate the mean\u2019s confidence interval and standard error.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "bootstrap_ci_discovered",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (name of most skewed column), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (lower bound of 95% CI, rounded to 4 decimals), \"ci_upper\" (upper bound, rounded to 4 decimals), \"std_error\" (bootstrap standard error, rounded to 4 decimals), and \"n_bootstrap\" (integer, the number of bootstrap samples). Example: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}",
      "ground_truth_hash": "81974b36382f60b6",
      "_ground_truth": {
        "column": "writing score",
        "skewness": 0.2894,
        "mean": 68.054,
        "ci_lower": 67.1889,
        "ci_upper": 68.9731,
        "std_error": 0.4524,
        "n_bootstrap": 1000
      },
      "_template": "bootstrap_ci_discovered"
    },
    {
      "question": "Identify the numeric column that exhibits the highest variance among all numeric columns in the dataset. Then find a categorical column that contains between 3 and 10 distinct categories. Using the selected numeric column as the response variable and the selected categorical column as the grouping factor, perform a one\u2011way ANOVA. Report the following information in a JSON object with exactly the keys listed below:\n\n- \"target_column\": name of the numeric column with highest variance (string)\n- \"grouping_column\": name of the categorical column with 3\u201110 categories (string)\n- \"n_groups\": number of distinct groups (integer)\n- \"f_statistic\": ANOVA F\u2011statistic rounded to 4 decimal places (number)\n- \"p_value\": ANOVA p\u2011value rounded to 6 decimal places (number)\n- \"significant\": whether the p\u2011value is less than 0.05 (boolean)\n- \"best_group\": category with the highest mean of the response variable (string)\n- \"best_mean\": mean of the response variable for the best group, rounded to 4 decimal places (number)\n- \"worst_group\": category with the lowest mean of the response variable (string)\n- \"worst_mean\": mean of the response variable for the worst group, rounded to 4 decimal places (number)\n- \"eta_squared\": effect size (eta\u2011squared) rounded to 4 decimal places (number)\n\nProvide the answer exactly in the specified JSON format.",
      "hint": "Start by computing the variance of each numeric column to pick the one with the largest value. Then list the categorical columns and count their unique categories to select one with 3\u201110 levels. Use these two columns to calculate group means, the ANOVA F\u2011statistic and p\u2011value, and finally compute eta\u2011squared.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "anova_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 11 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"f_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"significant\" (boolean), \"best_group\" (category with highest mean), \"best_mean\" (rounded to 4 decimals), \"worst_group\" (category with lowest mean), \"worst_mean\" (rounded to 4 decimals), and \"eta_squared\" (effect size, rounded to 4 decimals). Example: {\"target_column\": \"sales\", \"grouping_column\": \"region\", \"n_groups\": 4, \"f_statistic\": 15.2345, \"p_value\": 0.000012, \"significant\": true, \"best_group\": \"West\", \"best_mean\": 1234.5678, \"worst_group\": \"East\", \"worst_mean\": 890.1234, \"eta_squared\": 0.1523}",
      "ground_truth_hash": "776985fff70a7574",
      "_ground_truth": {
        "target_column": "writing score",
        "grouping_column": "race/ethnicity",
        "n_groups": 5,
        "f_statistic": 7.1624,
        "p_value": 1.1e-05,
        "significant": "True",
        "best_group": "group E",
        "best_mean": 71.4071,
        "worst_group": "group A",
        "worst_mean": 62.6742,
        "eta_squared": 0.028
      },
      "_template": "anova_discovered_groups"
    },
    {
      "question": "Identify the numeric column that has the highest variance and treat it as the target variable. Among the remaining numeric columns, find the one that shows the strongest absolute correlation with this target. Using rows without missing values, fit a simple ordinary least squares regression of the target on this most predictive column. Report the target column name, the predictor column name, the absolute correlation between them (rounded to 3 decimal places), the R\u2011squared of the regression (rounded to 4 decimal places), the regression coefficient for the predictor (rounded to 4 decimal places), and the p\u2011value for that coefficient (rounded to 6 decimal places). Provide your answer as a JSON object with exactly the following keys: \"target\", \"best_predictor\", \"correlation\", \"r_squared\", \"coefficient\", \"p_value\". Example format: {\"target\": \"price\", \"best_predictor\": \"sqft\", \"correlation\": 0.834, \"r_squared\": 0.6956, \"coefficient\": 135.2847, \"p_value\": 0.000001}",
      "hint": "First compute variances of all numeric columns to pick the target, then compute absolute correlations with the other numeric columns to select the best predictor, and finally run a simple OLS regression to obtain R\u2011squared, coefficient, and p\u2011value.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "regression_most_predictive",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"best_predictor\" (column name), \"correlation\" (rounded to 3 decimals), \"r_squared\" (rounded to 4 decimals), \"coefficient\" (rounded to 4 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target\": \"price\", \"best_predictor\": \"sqft\", \"correlation\": 0.834, \"r_squared\": 0.6956, \"coefficient\": 135.2847, \"p_value\": 0.000001}",
      "ground_truth_hash": "ca7a1a9e71f66d92",
      "_ground_truth": {
        "target": "writing score",
        "best_predictor": "reading score",
        "correlation": 0.955,
        "r_squared": 0.9113,
        "coefficient": 0.9935,
        "p_value": 0.0
      },
      "_template": "regression_most_predictive"
    },
    {
      "question": "Identify the numeric column that has the highest variance among all numeric columns in the dataset. Then locate a categorical column that contains exactly two distinct categories. Using these two groups, calculate the mean of the selected numeric column for each group, perform an independent two\u2011sample t\u2011test comparing the groups, and determine whether the difference is statistically significant at the 0.05 significance level. Provide your answer as a JSON object with exactly the following keys: \"target_column\" (the name of the numeric column with highest variance), \"grouping_column\" (the name of the binary categorical column), \"group1\" (the first category name), \"group2\" (the second category name), \"mean1\" (mean for group1 rounded to 4 decimal places), \"mean2\" (mean for group2 rounded to 4 decimal places), \"t_statistic\" (t\u2011statistic rounded to 4 decimal places), \"p_value\" (p\u2011value rounded to 6 decimal places), and \"significant\" (boolean, true if p < 0.05). Example format: {\"target_column\": \"score\", \"grouping_column\": \"gender\", \"group1\": \"female\", \"group2\": \"male\", \"mean1\": 72.4672, \"mean2\": 63.3112, \"t_statistic\": 9.9796, \"p_value\": 0.000000, \"significant\": true}",
      "hint": "First compute the variance of each numeric column to find the one with the largest spread, then examine the cardinality of each categorical column to locate one with exactly two unique values before performing the group\u2011wise calculations.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "ttest_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"mean1\" (rounded to 4 decimals), \"mean2\" (rounded to 4 decimals), \"t_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"significant\" (boolean, true if p < 0.05). Example: {\"target_column\": \"score\", \"grouping_column\": \"gender\", \"group1\": \"M\", \"group2\": \"F\", \"mean1\": 75.4321, \"mean2\": 78.1234, \"t_statistic\": -2.3456, \"p_value\": 0.019234, \"significant\": true}",
      "ground_truth_hash": "6ea026dfb845f916",
      "_ground_truth": {
        "target_column": "writing score",
        "grouping_column": "gender",
        "group1": "female",
        "group2": "male",
        "mean1": 72.4672,
        "mean2": 63.3112,
        "t_statistic": 9.9796,
        "p_value": 0.0,
        "significant": "True"
      },
      "_template": "ttest_discovered_groups"
    },
    {
      "question": "Identify the numeric column with the highest variance in the dataset. Assess whether this column follows a normal distribution (e.g., using the Shapiro\u2011Wilk test). If the column is judged normal (p\u2011value > 0.05), report its mean and standard deviation; otherwise, report its median and interquartile range (IQR = Q3 \u2212 Q1). Provide your answer as a JSON object with exactly three keys: for a normal distribution use {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}; for a non\u2011normal distribution use {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. Round all numeric values to three decimal places.",
      "hint": "Calculate the variance of each numeric column to find the one with the maximum variance, then perform a normality test on that column to decide which summary statistics to return.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "conditional_normality",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 3 keys. If normal: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}. If non-normal: {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. The \"iqr\" value is Q3 minus Q1 as a single number. All numeric values rounded to 3 decimal places.",
      "ground_truth_hash": "91fde79bbd84c7d6",
      "_ground_truth": {
        "distribution": "non-normal",
        "median": 69.0,
        "iqr": 21.25
      },
      "_template": "conditional_normality"
    },
    {
      "question": "Identify the numeric column that exhibits the highest variance in the dataset. Then, among the categorical columns that have a moderate number of distinct categories (between 2 and 20 unique values), select the one that, when used to group the data, yields the highest average value of the previously identified numeric column. Report the name of this grouping column, the category value with the highest mean, the name of the numeric column, and the corresponding mean (rounded to three decimal places). Provide your answer as a JSON object with exactly four keys: \"category_column\", \"best_category\", \"target_column\", and \"mean_value\".",
      "hint": "First compute variance for each numeric column to find the most variable one, then examine categorical columns for reasonable cardinality before calculating group means.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "category_highest_target_mean",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"category_column\" (the grouping column name), \"best_category\" (the category value with highest mean), \"target_column\" (the numeric column analyzed), and \"mean_value\" (rounded to 3 decimal places). Example: {\"category_column\": \"region\", \"best_category\": \"West\", \"target_column\": \"sales\", \"mean_value\": 1234.567}",
      "ground_truth_hash": "0aa523e1addc6020",
      "_ground_truth": {
        "category_column": "gender",
        "best_category": "female",
        "target_column": "writing score",
        "mean_value": 72.467
      },
      "_template": "category_highest_target_mean"
    },
    {
      "question": "Identify the pair of numeric columns that have the strongest absolute correlation in the dataset. Compute the original correlation between these two columns, then remove any rows where either column\u2019s value lies more than three standard deviations away from its mean. Report how many rows were removed and recompute the correlation on the remaining data. Provide your answer as a JSON object with exactly the following keys: \"columns\" (a list of the two column names, alphabetically sorted), \"original_correlation\" (the original correlation rounded to three decimal places), \"outliers_removed\" (the integer count of rows removed), and \"clean_correlation\" (the correlation after outlier removal rounded to three decimal places).",
      "hint": "Start by constructing the absolute correlation matrix of all numeric columns to find the most correlated pair, then apply a 3\u2011sigma filter on both columns before recalculating the correlation.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "05ff4bdea54b8b1e",
      "_ground_truth": {
        "columns": [
          "reading score",
          "writing score"
        ],
        "original_correlation": 0.955,
        "outliers_removed": 5,
        "clean_correlation": 0.952
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "Which pair of numeric columns has the strongest (absolute) correlation? Provide your answer as a JSON object with keys \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places).",
      "hint": "Calculate the correlation matrix for the numeric columns, ignore the diagonal, and locate the maximum off\u2011diagonal value.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "f700884d56c19588",
      "_ground_truth": {
        "columns": [
          "reading score",
          "writing score"
        ],
        "correlation": 0.955
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Among the numeric columns, identify the pair of columns that have the smallest non\u2011zero absolute correlation (ignore the self\u2011correlation on the diagonal). Provide your answer as a JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example format: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "hint": "Create a correlation matrix for all numeric columns, set the diagonal to missing, then locate the minimum value in the remaining entries.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "10b887e249d35065",
      "_ground_truth": {
        "columns": [
          "math score",
          "writing score"
        ],
        "correlation": 0.803
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "For the numeric columns in the dataset, determine how many of those columns contain at least one outlier (where an outlier is defined as a value whose absolute deviation from the column mean exceeds three times the column's standard deviation) and the total number of outlier values across all numeric columns. Provide your answer as a JSON object with exactly two keys: \"columns_with_outliers\" (the integer count of numeric columns that have at least one outlier) and \"total_outliers\" (the integer total count of outlier values). Example format: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "hint": "Calculate the mean and standard deviation for each numeric column, flag values beyond 3\u202f\u00d7\u202fstd, then count flagged values per column and aggregate.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "457331f5b1818d68",
      "_ground_truth": {
        "columns_with_outliers": 3,
        "total_outliers": 12
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "How many of the numeric columns contain at least one outlier, and what is the total number of outlier values across all numeric columns? Provide your answer as a JSON object with the keys \"columns_with_outliers\" (integer) and \"total_outliers\" (integer).",
      "hint": "Treat a value as an outlier if its absolute distance from the column mean exceeds a multiple (e.g., 2.5) of the column's standard deviation, then count columns with any such values and sum all outlier occurrences.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "71969c629fd0f690",
      "_ground_truth": {
        "columns_with_outliers": 3,
        "total_outliers": 32
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "What is the mean of the numeric column that has the highest variance? Provide your answer as a single number rounded to 3 decimal places.",
      "hint": "Calculate the variance for each numeric column, identify the one with the largest variance, then compute its average.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "6f38b316463e56af",
      "_ground_truth": 68.054,
      "_template": "max_variance_mean"
    },
    {
      "question": "What is the standard deviation of the numeric column that has the lowest mean? Provide your answer as a single number rounded to 3 decimal places.",
      "hint": "Compute the mean of each numeric column, find the column with the smallest mean, then calculate that column's standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "4d47fa98fb024dc1",
      "_ground_truth": 15.163,
      "_template": "min_mean_column_std"
    },
    {
      "question": "How many columns in the dataset have more than 5% missing values? Provide your answer as a JSON object with keys \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted).",
      "hint": "Calculate the missing-value percentage for each column and identify those exceeding the 5% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "How many columns have missing value percentages greater than 10%? Provide your answer as a JSON object with keys \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted).",
      "hint": "Calculate the missing\u2011value percentage for each column and count those exceeding the 10% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "2c42c4348e535666",
      "_ground_truth": {
        "count": 0,
        "columns": []
      },
      "_template": "count_high_missing_columns"
    }
  ]
}