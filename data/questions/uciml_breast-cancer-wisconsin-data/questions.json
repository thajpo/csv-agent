{
  "dataset_columns": [
    "id",
    "diagnosis",
    "radius_mean",
    "texture_mean",
    "perimeter_mean",
    "area_mean",
    "smoothness_mean",
    "compactness_mean",
    "concavity_mean",
    "concave points_mean",
    "symmetry_mean",
    "fractal_dimension_mean",
    "radius_se",
    "texture_se",
    "perimeter_se",
    "area_se",
    "smoothness_se",
    "compactness_se",
    "concavity_se",
    "concave points_se",
    "symmetry_se",
    "fractal_dimension_se",
    "radius_worst",
    "texture_worst",
    "perimeter_worst",
    "area_worst",
    "smoothness_worst",
    "compactness_worst",
    "concavity_worst",
    "concave points_worst",
    "symmetry_worst",
    "fractal_dimension_worst",
    "Unnamed: 32"
  ],
  "questions": [
    {
      "question": "Consider only the numeric columns in the dataset. Identify the numeric column with the highest variance and treat it as the target variable. Then, among the remaining numeric columns, find the three that have the largest absolute Pearson correlation with this target. Fit an ordinary least squares regression model using these three predictors to predict the target. Provide your answer as a JSON object with exactly the following keys:\n- \"target\": the name of the target column,\n- \"predictors\": a list of the three predictor column names (ordered by decreasing absolute correlation with the target),\n- \"r_squared\": the model R\u2011squared, rounded to 4 decimal places,\n- \"adj_r_squared\": the adjusted R\u2011squared, rounded to 4 decimal places,\n- \"n_significant\": the number of predictors whose p\u2011value is less than 0.05,\n- \"coefficients\": a dictionary mapping each predictor name to its regression coefficient, rounded to 4 decimal places,\n- \"p_values\": a dictionary mapping each predictor name to its p\u2011value, rounded to 6 decimal places.\nExample format: {\"target\": \"...\", \"predictors\": [\"...\", \"...\", \"...\"], \"r_squared\": 0.1234, \"adj_r_squared\": 0.1123, \"n_significant\": 2, \"coefficients\": {\"...\": 12.3456, \"...\": -0.1234, \"...\": 5.6789}, \"p_values\": {\"...\": 0.000123, \"...\": 0.045678, \"...\": 0.567890}}",
      "hint": "Start by selecting all numeric columns, compute their variances to find the target, then calculate absolute correlations of the remaining columns with the target to pick the top three predictors before fitting the OLS model.",
      "n_steps": 10,
      "difficulty": "VERY_HARD",
      "template_name": "multiple_regression_top_predictors",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"predictors\" (list of 3 column names), \"r_squared\" (rounded to 4 decimals), \"adj_r_squared\" (rounded to 4 decimals), \"n_significant\" (integer count of predictors with p < 0.05), \"coefficients\" (dict mapping predictor names to coefficients rounded to 4 decimals), and \"p_values\" (dict mapping predictor names to p-values rounded to 6 decimals). Example: {\"target\": \"price\", \"predictors\": [\"sqft\", \"bedrooms\", \"age\"], \"r_squared\": 0.7234, \"adj_r_squared\": 0.7156, \"n_significant\": 2, \"coefficients\": {\"sqft\": 123.45, \"bedrooms\": 5000.12, \"age\": -200.34}, \"p_values\": {\"sqft\": 0.000001, \"bedrooms\": 0.023456, \"age\": 0.156789}}",
      "ground_truth_hash": "067f50d0461d389a",
      "_ground_truth": {
        "target": "id",
        "predictors": [
          "area_se",
          "radius_se",
          "perimeter_se"
        ],
        "r_squared": 0.0398,
        "adj_r_squared": 0.0347,
        "n_significant": 1,
        "coefficients": {
          "area_se": 1261547.4034,
          "radius_se": -73614584.9826,
          "perimeter_se": -8300838.0709
        },
        "p_values": {
          "area_se": 0.00081,
          "radius_se": 0.424945,
          "perimeter_se": 0.457142
        }
      },
      "_template": "multiple_regression_top_predictors"
    },
    {
      "question": "Identify the numeric column that exhibits the highest absolute skewness. Using the non\u2011missing values of that column, compute its original mean, then perform 1000 bootstrap resamples (sampling with replacement) to estimate a 95% confidence interval for the mean and the bootstrap standard error of the mean. Report your findings as a JSON object with exactly the following keys: \"column\" (the name of the most skewed column), \"skewness\" (absolute skewness rounded to 4 decimals), \"mean\" (original mean rounded to 4 decimals), \"ci_lower\" (lower bound of the 95% CI rounded to 4 decimals), \"ci_upper\" (upper bound of the 95% CI rounded to 4 decimals), \"std_error\" (bootstrap standard error rounded to 4 decimals), and \"n_bootstrap\" (the integer number of bootstrap samples).",
      "hint": "First compute skewness for each numeric column (use pandas .skew()), pick the one with the largest absolute value, then apply numpy.random.choice for bootstrap sampling.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "bootstrap_ci_discovered",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (name of most skewed column), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (lower bound of 95% CI, rounded to 4 decimals), \"ci_upper\" (upper bound, rounded to 4 decimals), \"std_error\" (bootstrap standard error, rounded to 4 decimals), and \"n_bootstrap\" (integer, the number of bootstrap samples). Example: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}",
      "ground_truth_hash": "5c8fc8d5efe471db",
      "_ground_truth": {
        "column": "id",
        "skewness": 6.4738,
        "mean": 30371831.4323,
        "ci_lower": 21459994.5822,
        "ci_upper": 42233930.4375,
        "std_error": 5154077.2746,
        "n_bootstrap": 1000
      },
      "_template": "bootstrap_ci_discovered"
    },
    {
      "question": "Identify the numeric column that has the highest variance among all numeric columns, and a categorical column that contains between 3 and 10 distinct categories. Using these two columns, conduct a one\u2011way ANOVA to compare the means of the selected numeric column across the groups defined by the selected categorical column. Report the findings as a JSON object with exactly the following keys: \"target_column\" (the name of the numeric column with highest variance), \"grouping_column\" (the name of the categorical column with 3\u201110 groups), \"n_groups\" (integer count of groups), \"f_statistic\" (F\u2011statistic rounded to 4 decimal places), \"p_value\" (p\u2011value rounded to 6 decimal places), \"significant\" (boolean, true if p\u202f<\u202f0.05), \"best_group\" (the category with the highest group mean), \"best_mean\" (that mean rounded to 4 decimal places), \"worst_group\" (the category with the lowest group mean), \"worst_mean\" (that mean rounded to 4 decimal places), and \"eta_squared\" (effect size rounded to 4 decimal places).",
      "hint": "Start by computing variances of all numeric columns to find the one with the largest spread, then scan categorical columns for one with a moderate number of unique values (3\u201110). Group the data by that category, calculate each group's mean, and apply scipy's one\u2011way ANOVA. Use the group means to identify the best and worst groups and compute eta\u2011squared from the between\u2011group and total sums of squares.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "anova_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 11 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"f_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"significant\" (boolean), \"best_group\" (category with highest mean), \"best_mean\" (rounded to 4 decimals), \"worst_group\" (category with lowest mean), \"worst_mean\" (rounded to 4 decimals), and \"eta_squared\" (effect size, rounded to 4 decimals). Example: {\"target_column\": \"sales\", \"grouping_column\": \"region\", \"n_groups\": 4, \"f_statistic\": 15.2345, \"p_value\": 0.000012, \"significant\": true, \"best_group\": \"West\", \"best_mean\": 1234.5678, \"worst_group\": \"East\", \"worst_mean\": 890.1234, \"eta_squared\": 0.1523}",
      "ground_truth_hash": "f34d87df8f73ee91",
      "_ground_truth": {
        "error": "No suitable categorical column found (need 3-10 groups)"
      },
      "_template": "anova_discovered_groups"
    },
    {
      "question": "Using the given DataFrame, consider only the numeric columns. Identify the numeric column that has the highest variance and treat it as the target variable. Then, among the remaining numeric columns, find the one that shows the strongest absolute Pearson correlation with this target. Fit a simple ordinary least squares regression predicting the target from this most\u2011correlated predictor (dropping any rows containing missing values). Report the name of the target column, the name of the best predictor column, the absolute correlation between them (rounded to three decimal places), the R\u2011squared of the regression (rounded to four decimal places), the regression coefficient for the predictor (rounded to four decimal places), and the p\u2011value for that coefficient (rounded to six decimal places). Provide your answer as a JSON object with exactly the following keys: \"target\" (string), \"best_predictor\" (string), \"correlation\" (number), \"r_squared\" (number), \"coefficient\" (number), and \"p_value\" (number).",
      "hint": "Select numeric columns, compute variances to pick the target, use .corrwith() to find the most correlated predictor, drop NaNs, add a constant, and fit with statsmodels OLS.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "regression_most_predictive",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"best_predictor\" (column name), \"correlation\" (rounded to 3 decimals), \"r_squared\" (rounded to 4 decimals), \"coefficient\" (rounded to 4 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target\": \"price\", \"best_predictor\": \"sqft\", \"correlation\": 0.834, \"r_squared\": 0.6956, \"coefficient\": 135.2847, \"p_value\": 0.000001}",
      "ground_truth_hash": "b8a5122548e54a74",
      "_ground_truth": {
        "target": "id",
        "best_predictor": "area_se",
        "correlation": 0.178,
        "r_squared": 0.0316,
        "coefficient": 488478.9438,
        "p_value": 2e-05
      },
      "_template": "regression_most_predictive"
    },
    {
      "question": "Identify the numeric column that has the highest variance among all numeric columns. Then locate a categorical column that contains exactly two distinct categories. Using the two categories as groups, calculate the mean of the high\u2011variance numeric column for each group, perform an independent two\u2011sample t\u2011test comparing the groups, and determine whether the difference is statistically significant at the 0.05 level. Provide your answer as a JSON object with exactly the following keys: \"target_column\" (the name of the high\u2011variance numeric column), \"grouping_column\" (the name of the binary categorical column), \"group1\" (the first category name), \"group2\" (the second category name), \"mean1\" (mean for group1 rounded to 4 decimal places), \"mean2\" (mean for group2 rounded to 4 decimal places), \"t_statistic\" (t\u2011statistic rounded to 4 decimal places), \"p_value\" (p\u2011value rounded to 6 decimal places), and \"significant\" (true if p\u202f<\u202f0.05, otherwise false).",
      "hint": "Start by computing the variance of each numeric column to find the one with the greatest spread, then examine categorical columns for those with exactly two unique values before proceeding with group\u2011wise statistics.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "ttest_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"mean1\" (rounded to 4 decimals), \"mean2\" (rounded to 4 decimals), \"t_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"significant\" (boolean, true if p < 0.05). Example: {\"target_column\": \"score\", \"grouping_column\": \"gender\", \"group1\": \"M\", \"group2\": \"F\", \"mean1\": 75.4321, \"mean2\": 78.1234, \"t_statistic\": -2.3456, \"p_value\": 0.019234, \"significant\": true}",
      "ground_truth_hash": "3f565b335bcb3326",
      "_ground_truth": {
        "target_column": "id",
        "grouping_column": "diagnosis",
        "group1": "M",
        "group2": "B",
        "mean1": 36818050.4434,
        "mean2": 26543824.6246,
        "t_statistic": 0.9477,
        "p_value": 0.343682,
        "significant": "False"
      },
      "_template": "ttest_discovered_groups"
    },
    {
      "question": "Identify the numeric column with the highest variance in the dataset. Test whether the values in this column follow a normal distribution using the Shapiro\u2011Wilk test (if the column contains more than 5,000 observations, use a random sample of 5,000 rows). Based on the test result, return a JSON object with the exact following format: if the distribution is normal, provide {\"distribution\": \"normal\", \"mean\": <number rounded to 3 decimal places>, \"std\": <number rounded to 3 decimal places>}; if the distribution is not normal, provide {\"distribution\": \"non-normal\", \"median\": <number rounded to 3 decimal places>, \"iqr\": <number rounded to 3 decimal places>}.",
      "hint": "Compute the variance of each numeric column to find the one with the maximum variance, then apply scipy.stats.shapiro and compare the p\u2011value to 0.05 to decide normality.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "conditional_normality",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 3 keys. If normal: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}. If non-normal: {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. The \"iqr\" value is Q3 minus Q1 as a single number. All numeric values rounded to 3 decimal places.",
      "ground_truth_hash": "6d836fb322537573",
      "_ground_truth": {
        "distribution": "non-normal",
        "median": 906024.0,
        "iqr": 7943911.0
      },
      "_template": "conditional_normality"
    },
    {
      "question": "Find the categorical column (i.e., a column with non\u2011numeric values) and the specific category within it that yields the highest average of the numeric column that has the greatest variance among all numeric columns in the dataset. Report your result as a JSON object with exactly four keys: \"category_column\" (the name of the chosen categorical column), \"best_category\" (the category value with the highest mean), \"target_column\" (the name of the numeric column with the highest variance), and \"mean_value\" (the corresponding mean rounded to three decimal places).",
      "hint": "Start by calculating the variance of each numeric column to identify the one with the highest variance. Then, for each categorical column, compute the mean of that numeric column for each category and select the category with the largest mean.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "category_highest_target_mean",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"category_column\" (the grouping column name), \"best_category\" (the category value with highest mean), \"target_column\" (the numeric column analyzed), and \"mean_value\" (rounded to 3 decimal places). Example: {\"category_column\": \"region\", \"best_category\": \"West\", \"target_column\": \"sales\", \"mean_value\": 1234.567}",
      "ground_truth_hash": "369d09f4130a56b5",
      "_ground_truth": {
        "category_column": "diagnosis",
        "best_category": "M",
        "target_column": "id",
        "mean_value": 36818050.443
      },
      "_template": "category_highest_target_mean"
    },
    {
      "question": "Identify the pair of numeric columns that have the strongest absolute correlation in the dataset. Then, remove any rows where the value in either of these two columns lies more than 3 standard deviations away from its mean, and recompute the correlation on the cleaned data. Provide your answer as a JSON object with exactly four keys: \"columns\" (a list of the two column names, alphabetically sorted), \"original_correlation\" (the correlation before outlier removal, rounded to three decimal places), \"outliers_removed\" (the integer count of rows removed as outliers), and \"clean_correlation\" (the correlation after cleaning, rounded to three decimal places). Example format: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "hint": "Start by computing the absolute correlation matrix of all numeric columns, find the maximum off\u2011diagonal value to get the column pair, then apply a 3\u2011sigma filter on both columns before recalculating the correlation.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "34366b69673891c6",
      "_ground_truth": {
        "columns": [
          "perimeter_mean",
          "radius_mean"
        ],
        "original_correlation": 0.998,
        "outliers_removed": 7,
        "clean_correlation": 0.998
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "Among all numeric columns, identify the pair of columns that have the highest absolute correlation coefficient. Provide your answer as a JSON object with exactly two keys: \"columns\" (a list containing the two column names sorted alphabetically) and \"correlation\" (the correlation value rounded to 3 decimal places).",
      "hint": "Compute the correlation matrix for the numeric columns and look for the off\u2011diagonal entry with the largest absolute value.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "8798b6294605fd4d",
      "_ground_truth": {
        "columns": [
          "perimeter_mean",
          "radius_mean"
        ],
        "correlation": 0.998
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Which pair of numeric columns has the smallest non-zero absolute correlation? Provide your answer as a JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places).",
      "hint": "Compute the absolute correlation matrix for all numeric columns, ignore the diagonal and any zero correlations, then identify the minimum remaining correlation.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "cac0c10aacaa7507",
      "_ground_truth": {
        "columns": [
          "compactness_mean",
          "id"
        ],
        "correlation": 0.0
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "Considering only the numeric columns in the dataset, define an outlier as a value whose absolute difference from its column mean exceeds three times the column's standard deviation. How many numeric columns contain at least one outlier, and what is the total number of outlier values across all numeric columns? Provide your answer as a JSON object with exactly two keys: \"columns_with_outliers\" (an integer) and \"total_outliers\" (an integer).",
      "hint": "For each numeric column compute its mean and standard deviation, then count the rows where the value deviates from the mean by more than 3\u202f\u00d7\u202fstd. Aggregate these counts to get the required totals.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "bb065a0c79d2f6f2",
      "_ground_truth": {
        "columns_with_outliers": 30,
        "total_outliers": 222
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "For the dataset, consider each numeric column and define an outlier as a value whose absolute deviation from the column mean exceeds 2.5 times the column's standard deviation. Determine (1) how many numeric columns contain at least one outlier, and (2) the total number of outlier values across all numeric columns. Provide your answer as a JSON object with exactly two keys: \"columns_with_outliers\" (integer) and \"total_outliers\" (integer).",
      "hint": "Iterate over all numeric columns, compute their mean and standard deviation, count values beyond 2.5\u202f\u00d7\u202fstd, then summarize the counts.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "1d99c19644151fec",
      "_ground_truth": {
        "columns_with_outliers": 31,
        "total_outliers": 399
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "What is the mean of the numeric column that has the highest variance? Provide your answer as a single number rounded to 3 decimal places.",
      "hint": "Calculate the variance for each numeric column, find the column with the largest variance, and then compute its average.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "c713b66a46799cfd",
      "_ground_truth": 30371831.432,
      "_template": "max_variance_mean"
    },
    {
      "question": "What is the standard deviation of the numeric column that has the smallest average value? Provide your answer as a single number rounded to 3 decimal places.",
      "hint": "Compute the mean of each numeric column, find the column with the lowest mean, and then calculate that column's standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "414369a18b9a0bb5",
      "_ground_truth": 0.003,
      "_template": "min_mean_column_std"
    },
    {
      "question": "How many columns in the dataset have more than 5% missing values, and which columns are they? Provide your answer as a JSON object with exactly two keys: \"count\" (an integer) and \"columns\" (a list of column names sorted alphabetically).",
      "hint": "Compute the missing\u2011value percentage for each column and identify those exceeding the 5% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "511a2b8ffadfdde9",
      "_ground_truth": {
        "count": 1,
        "columns": [
          "Unnamed: 32"
        ]
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "Identify the columns whose proportion of missing values exceeds 10% and report how many such columns exist. Provide your answer as a JSON object with exactly two keys: \"count\" (an integer) and \"columns\" (an alphabetically sorted list of the column names).",
      "hint": "First compute the missing value percentage for each column, then filter those above the 10% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "511a2b8ffadfdde9",
      "_ground_truth": {
        "count": 1,
        "columns": [
          "Unnamed: 32"
        ]
      },
      "_template": "count_high_missing_columns"
    }
  ]
}