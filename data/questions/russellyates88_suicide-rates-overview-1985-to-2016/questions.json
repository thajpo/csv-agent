{
  "dataset_columns": [
    "country",
    "year",
    "sex",
    "age",
    "suicides_no",
    "population",
    "suicides/100k pop",
    "country-year",
    "HDI for year",
    " gdp_for_year ($) ",
    "gdp_per_capita ($)",
    "generation"
  ],
  "questions": [
    {
      "question": "Identify the numeric column with the highest variance in the dataset and treat it as the target variable. Then, among the remaining numeric columns, find the three that have the largest absolute correlation with this target. Fit an ordinary least squares (OLS) regression using these three predictors to model the target. Provide the results as a JSON object with exactly the following keys:\n- \"target\": the name of the target column (string)\n- \"predictors\": list of the three predictor column names ordered by descending absolute correlation (list of strings)\n- \"r_squared\": the model's R\u2011squared value rounded to 4 decimal places (number)\n- \"adj_r_squared\": the adjusted R\u2011squared value rounded to 4 decimal places (number)\n- \"n_significant\": count of predictors whose p\u2011value is less than 0.05 (integer)\n- \"coefficients\": mapping from each predictor name to its regression coefficient rounded to 4 decimals (object)\n- \"p_values\": mapping from each predictor name to its p\u2011value rounded to 6 decimals (object)\nExample format: {\"target\": \"...\", \"predictors\": [\"...\",\"...\",\"...\"], \"r_squared\": 0.1234, \"adj_r_squared\": 0.1230, \"n_significant\": 2, \"coefficients\": {\"...\": 12.3456, \"...\": -0.1234, \"...\": 5.6789}, \"p_values\": {\"...\": 0.000123, \"...\": 0.045678, \"...\": 0.567890}}",
      "hint": "Compute variance for each numeric column to pick the target, then calculate absolute correlations of the other numeric columns with that target to select the top three predictors before fitting the OLS model.",
      "n_steps": 10,
      "difficulty": "VERY_HARD",
      "template_name": "multiple_regression_top_predictors",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"predictors\" (list of 3 column names), \"r_squared\" (rounded to 4 decimals), \"adj_r_squared\" (rounded to 4 decimals), \"n_significant\" (integer count of predictors with p < 0.05), \"coefficients\" (dict mapping predictor names to coefficients rounded to 4 decimals), and \"p_values\" (dict mapping predictor names to p-values rounded to 6 decimals). Example: {\"target\": \"price\", \"predictors\": [\"sqft\", \"bedrooms\", \"age\"], \"r_squared\": 0.7234, \"adj_r_squared\": 0.7156, \"n_significant\": 2, \"coefficients\": {\"sqft\": 123.45, \"bedrooms\": 5000.12, \"age\": -200.34}, \"p_values\": {\"sqft\": 0.000001, \"bedrooms\": 0.023456, \"age\": 0.156789}}",
      "ground_truth_hash": "2eec58c9ff120eb0",
      "_ground_truth": {
        "target": "population",
        "predictors": [
          "suicides_no",
          "HDI for year",
          "gdp_per_capita ($)"
        ],
        "r_squared": 0.4883,
        "adj_r_squared": 0.4881,
        "n_significant": 1,
        "coefficients": {
          "suicides_no": 4076.2257,
          "HDI for year": -319075.5282,
          "gdp_per_capita ($)": 1.044
        },
        "p_values": {
          "suicides_no": 0.0,
          "HDI for year": 0.54399,
          "gdp_per_capita ($)": 0.629074
        }
      },
      "_template": "multiple_regression_top_predictors"
    },
    {
      "question": "Identify the numeric column that exhibits the greatest absolute skewness. Using that column, calculate its original mean, then perform bootstrap resampling with 1000 samples (sampling with replacement) to estimate a 95% confidence interval for the mean (using the 2.5th and 97.5th percentiles) and the bootstrap standard error of the mean. Provide your answer as a JSON object with exactly the following keys: \"column\" (the name of the most skewed column), \"skewness\" (the absolute skewness rounded to 4 decimal places), \"mean\" (the original mean rounded to 4 decimal places), \"ci_lower\" (lower bound of the 95% CI rounded to 4 decimal places), \"ci_upper\" (upper bound of the 95% CI rounded to 4 decimal places), \"std_error\" (bootstrap standard error rounded to 4 decimal places), and \"n_bootstrap\" (the integer number of bootstrap samples used). Example format: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}.",
      "hint": "First compute the skewness of each numeric column, pick the one with the highest absolute value, then apply standard bootstrap techniques on its non\u2011missing values.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "bootstrap_ci_discovered",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 7 keys: \"column\" (name of most skewed column), \"skewness\" (rounded to 4 decimals), \"mean\" (rounded to 4 decimals), \"ci_lower\" (lower bound of 95% CI, rounded to 4 decimals), \"ci_upper\" (upper bound, rounded to 4 decimals), \"std_error\" (bootstrap standard error, rounded to 4 decimals), and \"n_bootstrap\" (integer, the number of bootstrap samples). Example: {\"column\": \"income\", \"skewness\": 2.3456, \"mean\": 50000.1234, \"ci_lower\": 48500.5678, \"ci_upper\": 51500.9012, \"std_error\": 750.3456, \"n_bootstrap\": 1000}",
      "ground_truth_hash": "8f55baee8da3f21e",
      "_ground_truth": {
        "column": "suicides_no",
        "skewness": 10.3529,
        "mean": 242.5744,
        "ci_lower": 232.6241,
        "ci_upper": 253.0483,
        "std_error": 5.3426,
        "n_bootstrap": 1000
      },
      "_template": "bootstrap_ci_discovered"
    },
    {
      "question": "Identify the numeric column that exhibits the highest variance among all numeric columns. Then, find a categorical column that contains between 3 and 10 distinct categories. Using these two columns, perform a one\u2011way ANOVA to compare the means of the selected numeric column across the groups defined by the categorical column. Report the following information in a JSON object with exactly the keys and formatting specified: \n\n{\n  \"target_column\": <name of the numeric column with highest variance>,\n  \"grouping_column\": <name of the categorical column with 3\u201110 groups>,\n  \"n_groups\": <integer count of groups>,\n  \"f_statistic\": <ANOVA F\u2011statistic rounded to 4 decimal places>,\n  \"p_value\": <ANOVA p\u2011value rounded to 6 decimal places>,\n  \"significant\": <true if p\u2011value < 0.05, otherwise false>,\n  \"best_group\": <category with the highest mean of the numeric column>,\n  \"best_mean\": <mean value for the best group rounded to 4 decimal places>,\n  \"worst_group\": <category with the lowest mean>,\n  \"worst_mean\": <mean value for the worst group rounded to 4 decimal places>,\n  \"eta_squared\": <effect size (eta\u2011squared) rounded to 4 decimal places>\n}\n\nProvide the answer exactly in this JSON format.",
      "hint": "First compute variances of all numeric columns to pick the one with the largest variance. Then examine each categorical column\u2019s unique value count to select one with 3\u201110 categories. Group the data by this column and calculate group means, then run a one\u2011way ANOVA (e.g., using scipy.stats.f_oneway) to obtain the F\u2011statistic and p\u2011value, and finally compute eta\u2011squared as the ratio of between\u2011group sum of squares to total sum of squares.",
      "n_steps": 9,
      "difficulty": "VERY_HARD",
      "template_name": "anova_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 11 keys: \"target_column\", \"grouping_column\", \"n_groups\" (integer), \"f_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), \"significant\" (boolean), \"best_group\" (category with highest mean), \"best_mean\" (rounded to 4 decimals), \"worst_group\" (category with lowest mean), \"worst_mean\" (rounded to 4 decimals), and \"eta_squared\" (effect size, rounded to 4 decimals). Example: {\"target_column\": \"sales\", \"grouping_column\": \"region\", \"n_groups\": 4, \"f_statistic\": 15.2345, \"p_value\": 0.000012, \"significant\": true, \"best_group\": \"West\", \"best_mean\": 1234.5678, \"worst_group\": \"East\", \"worst_mean\": 890.1234, \"eta_squared\": 0.1523}",
      "ground_truth_hash": "6a44ce86efe61f0b",
      "_ground_truth": {
        "target_column": "population",
        "grouping_column": "age",
        "n_groups": 6,
        "f_statistic": 200.3266,
        "p_value": 0.0,
        "significant": "True",
        "best_group": "35-54 years",
        "best_mean": 3096916.8727,
        "worst_group": "75+ years",
        "worst_mean": 573735.7288,
        "eta_squared": 0.0348
      },
      "_template": "anova_discovered_groups"
    },
    {
      "question": "Identify the numeric column with the highest variance and treat it as the target variable. Among the remaining numeric columns, find the one that has the strongest absolute Pearson correlation with this target. Using only rows where both values are present, fit a simple ordinary least squares regression of the target on this most predictive feature. Report the target column name, the predictor column name, the absolute correlation (rounded to three decimal places), the regression R\u2011squared (rounded to four decimal places), the regression coefficient for the predictor (rounded to four decimal places), and the two\u2011tailed p\u2011value for that coefficient (rounded to six decimal places). Provide your answer as a JSON object with exactly the six keys: \"target\", \"best_predictor\", \"correlation\", \"r_squared\", \"coefficient\", \"p_value\".",
      "hint": "Compute variances of all numeric columns to pick the target, then compute absolute correlations with the target to select the best predictor, and finally run an OLS regression on those two columns.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "regression_most_predictive",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 6 keys: \"target\" (column name), \"best_predictor\" (column name), \"correlation\" (rounded to 3 decimals), \"r_squared\" (rounded to 4 decimals), \"coefficient\" (rounded to 4 decimals), and \"p_value\" (rounded to 6 decimals). Example: {\"target\": \"price\", \"best_predictor\": \"sqft\", \"correlation\": 0.834, \"r_squared\": 0.6956, \"coefficient\": 135.2847, \"p_value\": 0.000001}",
      "ground_truth_hash": "559177ea6cad7a5c",
      "_ground_truth": {
        "target": "population",
        "best_predictor": "suicides_no",
        "correlation": 0.616,
        "r_squared": 0.3797,
        "coefficient": 2672.0209,
        "p_value": 0.0
      },
      "_template": "regression_most_predictive"
    },
    {
      "question": "Identify the numeric column with the highest variance in the dataset. Then locate a categorical column that contains exactly two distinct categories. Using these two columns, split the data into the two groups defined by the binary categorical column and compute: (1) the mean of the high\u2011variance numeric column for each group, (2) the t\u2011statistic and p\u2011value from an independent two\u2011sample t\u2011test comparing the two groups, and (3) whether the difference is statistically significant at the 0.05 significance level. Provide your answer as a JSON object with exactly nine keys in the following format: {\"target_column\": <name of the high\u2011variance numeric column>, \"grouping_column\": <name of the binary categorical column>, \"group1\": <label of the first category>, \"group2\": <label of the second category>, \"mean1\": <mean for group1 rounded to 4 decimals>, \"mean2\": <mean for group2 rounded to 4 decimals>, \"t_statistic\": <t\u2011statistic rounded to 4 decimals>, \"p_value\": <p\u2011value rounded to 6 decimals>, \"significant\": <true if p < 0.05 else false>}.",
      "hint": "Use the variance of numeric columns to find the one with the largest value, and check categorical columns for exactly two unique values to get the binary grouping column.",
      "n_steps": 8,
      "difficulty": "HARD",
      "template_name": "ttest_discovered_groups",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 9 keys: \"target_column\", \"grouping_column\", \"group1\", \"group2\", \"mean1\" (rounded to 4 decimals), \"mean2\" (rounded to 4 decimals), \"t_statistic\" (rounded to 4 decimals), \"p_value\" (rounded to 6 decimals), and \"significant\" (boolean, true if p < 0.05). Example: {\"target_column\": \"score\", \"grouping_column\": \"gender\", \"group1\": \"M\", \"group2\": \"F\", \"mean1\": 75.4321, \"mean2\": 78.1234, \"t_statistic\": -2.3456, \"p_value\": 0.019234, \"significant\": true}",
      "ground_truth_hash": "14bfc459718c15d9",
      "_ground_truth": {
        "target_column": "population",
        "grouping_column": "sex",
        "group1": "male",
        "group2": "female",
        "mean1": 1800817.8705,
        "mean2": 1888769.3643,
        "t_statistic": -1.8752,
        "p_value": 0.060782,
        "significant": "False"
      },
      "_template": "ttest_discovered_groups"
    },
    {
      "question": "Identify the numeric column with the highest variance in the dataset. Test whether the values in this column follow a normal distribution (you may use a normality test such as Shapiro\u2011Wilk, sampling if the column is large). Based on the result, provide a JSON object in one of the following exact formats: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>} if the column is normal, or {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>} if it is not normal. Round all numeric values to three decimal places.",
      "hint": "First compute variances of all numeric columns to find the one with the maximum variance, then apply a normality test to its values.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "conditional_normality",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 3 keys. If normal: {\"distribution\": \"normal\", \"mean\": <number>, \"std\": <number>}. If non-normal: {\"distribution\": \"non-normal\", \"median\": <number>, \"iqr\": <number>}. The \"iqr\" value is Q3 minus Q1 as a single number. All numeric values rounded to 3 decimal places.",
      "ground_truth_hash": "2770207c67515f7c",
      "_ground_truth": {
        "distribution": "non-normal",
        "median": 430150.0,
        "iqr": 1388644.75
      },
      "_template": "conditional_normality"
    },
    {
      "question": "Identify the numeric column that has the highest variance among all numeric columns. Then, select a categorical column that has a moderate number of distinct categories (between 2 and 20). Compute the mean of the high\u2011variance numeric column for each category of the selected categorical column, and determine which category attains the highest mean. Provide your answer as a JSON object with exactly four keys: \"category_column\" (the name of the categorical column used), \"best_category\" (the category value with the highest mean), \"target_column\" (the name of the numeric column with the highest variance), and \"mean_value\" (the mean for that best category, rounded to three decimal places).",
      "hint": "First calculate variance for each numeric column to find the most variable one, then look for a categorical column with a reasonable number of unique values before grouping and averaging.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "category_highest_target_mean",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"category_column\" (the grouping column name), \"best_category\" (the category value with highest mean), \"target_column\" (the numeric column analyzed), and \"mean_value\" (rounded to 3 decimal places). Example: {\"category_column\": \"region\", \"best_category\": \"West\", \"target_column\": \"sales\", \"mean_value\": 1234.567}",
      "ground_truth_hash": "ef1e031814ceb99b",
      "_ground_truth": {
        "category_column": "sex",
        "best_category": "female",
        "target_column": "population",
        "mean_value": 1888769.364
      },
      "_template": "category_highest_target_mean"
    },
    {
      "question": "Identify the pair of numeric columns that have the highest absolute correlation in the dataset. For that pair, compute the original correlation (rounded to three decimal places), count how many rows would be removed if any observation more than three standard deviations away from the mean in either column is excluded, and compute the correlation between the two columns after this outlier removal (rounded to three decimal places). Provide your answer as a JSON object with exactly four keys: \"columns\" (a list of the two column names sorted alphabetically), \"original_correlation\" (the rounded original correlation), \"outliers_removed\" (the integer count of rows removed), and \"clean_correlation\" (the rounded correlation after outlier removal).",
      "hint": "First calculate the absolute correlation matrix for all numeric columns and locate the maximum off\u2011diagonal entry. Then apply a 3\u2011sigma filter on both columns of that pair, count the excluded rows, and recompute the correlation on the remaining data.",
      "n_steps": 5,
      "difficulty": "HARD",
      "template_name": "correlation_after_outlier_removal",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 4 keys: \"columns\" (list of 2 column names, alphabetically sorted), \"original_correlation\" (rounded to 3 decimals), \"outliers_removed\" (integer count), and \"clean_correlation\" (rounded to 3 decimals). Example: {\"columns\": [\"col_a\", \"col_b\"], \"original_correlation\": 0.847, \"outliers_removed\": 12, \"clean_correlation\": 0.891}",
      "ground_truth_hash": "9c8dfad02a6cd109",
      "_ground_truth": {
        "columns": [
          "HDI for year",
          "gdp_per_capita ($)"
        ],
        "original_correlation": 0.771,
        "outliers_removed": 19768,
        "clean_correlation": 0.832
      },
      "_template": "correlation_after_outlier_removal"
    },
    {
      "question": "Which pair of numeric columns exhibits the strongest absolute correlation? Provide your answer as a JSON object with exactly two keys: \"columns\" (a list of the two column names sorted alphabetically) and \"correlation\" (the correlation coefficient rounded to 3 decimal places).",
      "hint": "Calculate the absolute correlation matrix for all numeric columns and locate the largest off\u2011diagonal value.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "strongest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the correlation coefficient rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.847}",
      "ground_truth_hash": "98224b8bd5bfe47f",
      "_ground_truth": {
        "columns": [
          "HDI for year",
          "gdp_per_capita ($)"
        ],
        "correlation": 0.771
      },
      "_template": "strongest_correlation"
    },
    {
      "question": "Among all numeric columns, find the pair of columns that have the smallest non\u2011zero absolute correlation (ignore self\u2011correlations). Provide your answer as a JSON object with exactly two keys: \"columns\" (a list of the two column names, sorted alphabetically) and \"correlation\" (the absolute correlation value rounded to three decimal places).",
      "hint": "Compute the absolute correlation matrix for the numeric columns, mask the diagonal, then locate the minimum value and its corresponding column pair.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "weakest_correlation",
      "template_params": null,
      "output_type": "dict",
      "output_schema": "A JSON object with exactly two keys: \"columns\" (a list of the two column names, alphabetically sorted) and \"correlation\" (the absolute correlation value rounded to 3 decimal places). Example: {\"columns\": [\"col_a\", \"col_b\"], \"correlation\": 0.012}",
      "ground_truth_hash": "f0e22973f6084624",
      "_ground_truth": {
        "columns": [
          "gdp_per_capita ($)",
          "suicides/100k pop"
        ],
        "correlation": 0.002
      },
      "_template": "weakest_correlation"
    },
    {
      "question": "Among the numeric columns in the dataset, determine how many columns contain at least one outlier (where an outlier is defined as a value whose absolute deviation from the column mean exceeds three times the column's standard deviation) and the total number of outlier values across all numeric columns. Provide your answer as a JSON object with exactly two keys: \"columns_with_outliers\" (integer) and \"total_outliers\" (integer).",
      "hint": "For each numeric column compute its mean and standard deviation, flag values beyond 3\u202f\u00d7\u202fstd from the mean, count them per column, then summarize the number of columns with any outliers and the overall outlier count.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 3.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "52edd5a336ab1b33",
      "_ground_truth": {
        "columns_with_outliers": 5,
        "total_outliers": 2385
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "Among the numeric columns, determine how many columns contain at least one outlier (where an outlier is defined as a value whose absolute deviation from the column mean exceeds 2.5 times the column's standard deviation) and the total number of outlier values across all numeric columns. Provide your answer as a JSON object with exactly two keys: \"columns_with_outliers\" (integer) and \"total_outliers\" (integer).",
      "hint": "For each numeric column compute its mean and standard deviation, flag rows where the absolute difference from the mean is greater than 2.5\u202f\u00d7\u202fstd, then count columns with any flags and sum all flagged entries.",
      "n_steps": 4,
      "difficulty": "MEDIUM",
      "template_name": "count_outlier_columns",
      "template_params": {
        "z_threshold": 2.5
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"columns_with_outliers\" (integer count of columns containing at least one outlier) and \"total_outliers\" (integer total count of outlier values across all columns). Example: {\"columns_with_outliers\": 3, \"total_outliers\": 47}",
      "ground_truth_hash": "63ad8b334bfac1fc",
      "_ground_truth": {
        "columns_with_outliers": 5,
        "total_outliers": 3220
      },
      "_template": "count_outlier_columns"
    },
    {
      "question": "What is the mean of the numeric column with the highest variance? Provide your answer as a single number rounded to 3 decimal places.",
      "hint": "Calculate the variance for each numeric column, find the column with the maximum variance, and then compute its average.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "max_variance_mean",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the mean), rounded to 3 decimal places",
      "ground_truth_hash": "36bf841cffa07543",
      "_ground_truth": 1844793.617,
      "_template": "max_variance_mean"
    },
    {
      "question": "What is the standard deviation of the numeric column that has the smallest mean? Provide your answer as a single number rounded to 3 decimal places.",
      "hint": "Calculate the mean of each numeric column, find the column with the lowest mean, then compute its standard deviation.",
      "n_steps": 3,
      "difficulty": "MEDIUM",
      "template_name": "min_mean_column_std",
      "template_params": null,
      "output_type": "scalar",
      "output_schema": "A single number (the standard deviation), rounded to 3 decimal places",
      "ground_truth_hash": "dd7a194c8f2c5519",
      "_ground_truth": 0.093,
      "_template": "min_mean_column_std"
    },
    {
      "question": "How many columns have more than 5% missing values? Provide your answer as a JSON object with keys \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted).",
      "hint": "Calculate the missing\u2011value percentage for each column and count those that exceed the 5% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 5.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "a29347d2e8daf40c",
      "_ground_truth": {
        "count": 1,
        "columns": [
          "HDI for year"
        ]
      },
      "_template": "count_high_missing_columns"
    },
    {
      "question": "How many columns in the dataset have more than 10% missing values, and which columns are they? Provide your answer as a JSON object with exactly two keys: \"count\" (an integer) and \"columns\" (a list of column names sorted alphabetically).",
      "hint": "Calculate the missing\u2011value percentage for each column and select those exceeding the 10% threshold.",
      "n_steps": 3,
      "difficulty": "EASY",
      "template_name": "count_high_missing_columns",
      "template_params": {
        "missing_threshold": 10.0
      },
      "output_type": "dict",
      "output_schema": "A JSON object with exactly 2 keys: \"count\" (integer) and \"columns\" (list of column names, alphabetically sorted). Example: {\"count\": 3, \"columns\": [\"col_a\", \"col_b\", \"col_c\"]}",
      "ground_truth_hash": "a29347d2e8daf40c",
      "_ground_truth": {
        "count": 1,
        "columns": [
          "HDI for year"
        ]
      },
      "_template": "count_high_missing_columns"
    }
  ]
}