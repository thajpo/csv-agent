# Single Backbone Design (LLM + Template + Procedural)

This document captures the design direction and decisions we discussed. It is intended to be a shared source of truth for consolidation, contracts, and a safe refactor plan.

---

## 1) Problem statement

We have three generation avenues:

- **LLM questions** → consistency verification
- **Template synthetic questions** → verification → episodes
- **Procedural (program) synthetic questions** → optional verbalization/verification

Each path has overlapping utilities (metadata loading, submission parsing, verification loops, filters), but different entrypoints and file formats. This makes it hard to share contracts, avoid regressions, and reason about pipeline invariants.

We want a **single backbone** that preserves intentional separation (LLM vs synthetic sources), while consolidating repeated glue code and defining clear contracts.

---

## 2) Goals (what “good” looks like)

1) **Data-aware, difficult questions that make sense**
   - Questions should encode real-world context. LLM questions may include column names; synthetic verbalized questions should avoid explicit column names.
   - Avoid procedural phrasing in verbalized questions.
     - Implementation: verbalizer prompt forbids procedural wording; start without post-verbalization lint.
   - Prefer multi-step reasoning (groups, filters, comparisons, tests).
     - Implementation: enforce minimum chain length for programs + template selection that requires multi-step operators.

2) **Single backbone, multiple sources**
   - LLM / Template / Procedural are distinct sources, but share execution (synthetic), filtering (synthetic), schema, and verification stages.
      - Synthetic filtering happens before verbalization to reduce cost.
      - LLM filtering happens during consistency verification (episode generation).
      - Dataset descriptions are optional; if missing, generate a short description from data_overview rather than skipping the dataset.
      - Post-verbalization lint is deferred; rely on the verbalizer prompt only.

3) **Mechanical questions are canonical for synthetic**
   - Synthetic always emits a mechanical question
   - Verbalization is optional and writes into the same record
   - LLM questions are inherently verbalized (no mechanical form)

4) **Verification is separate**
   - Generation and verification are distinct steps.
     - LLM verification: `src/datagen/episode_gen.py` (consistency verification: gold with hint + N no-hint traces).
     - Synthetic verification: `src/datagen/validate_synthetic.py` (single trace vs ground truth).
   - Clarification: verification is the correctness gate that decides keep/drop.
     - LLM verification uses a consistency strategy (majority agreement across traces).
     - Synthetic verification uses a ground-truth strategy (single trace vs known answer).
     - Provide a shared wrapper API (`shared/verification.py`) with strategy selection.
   - Hints are optional; use them when available but do not require a pre-verification student validation step.

5) **Avoid compute waste**
   - Manifest should prevent re-execution / re-verbalization when inputs have not changed.
      - Use `code_hash + dataset_hash` for synthetic execution caching.
      - Cache verbalization keyed by `code_hash` (mechanical unchanged).
      - Record failures for expensive stages (verbalization, verification).
      - Clarification: add manifest entries with stage=status=failed so we can skip reruns unless `--retry-failed`.

### Project ethos alignment
- **Fail fast**: schema is strict, protocol is strict, and legacy formats are rejected.
- **Correctness over convenience**: verification is mandatory for keeping questions; traces and hooks are first-class artifacts.
- **No backward compatibility**: if schema changes, regenerate questions/episodes instead of adding adapters.
- **Simple contracts**: shared modules define one contract per stage; avoid hidden fallbacks.
- **Trace fidelity**: hooks must be grounded in executed code; submissions must use `submit()` wrapper.

---

## 3) Non‑goals (for now)

- No new statistical test types beyond current operators
- No automatic difficulty labeling (we will defer until we have data)
- No hard filtering beyond “obvious junk” on day one

---

## 4) Current architecture (as‑is)

### LLM (verbalized by definition)
- `src/datagen/question_gen.py` → `src/datagen/episode_gen.py`
- Questions are directly generated by the LLM in NL
- Verified via consistency traces (gold with hint + N no-hint)

### Template synthetic
- `src/datagen/synthetic/generator.py` (execute templates + optional verbalization; generation only)
- `src/datagen/validate_synthetic.py` (official verification stage)
  - Plan: generator becomes generation-only; verification stays in validate_synthetic.py but uses shared/verification.py

### Procedural (program) synthetic
- `src/datagen/synthetic/programs/program_generator.py`
- Compiles programs, executes, filters, verbalizes, verifies
- Not wired into CLI yet

---
## 5) Overlap map (duplication we can consolidate)
These are currently duplicated and can be centralized:
- Dataset name + description loading (question_gen.py, episode_gen.py, synthetic/generator.py, validate_synthetic.py, programs/program_generator.py)
  - Duplicate logic: derive dataset name from CSV path; read `meta.json` or `{dataset}.meta.json` for description.
  - Impact: inconsistent handling of missing descriptions (LLM currently skips; synthetic allows empty), and inconsistent dataset naming when `data.csv` is used.
- Question file loading / schema parsing (episode_gen.py, validate_synthetic.py)
  - Duplicate logic: accept multiple JSON shapes and fill defaults; behavior differs across pipelines.
  - Target: fail-fast unified schema (no legacy normalization).
- Submission parsing (`submit(...)` extraction) (core/environment.py, synthetic/generator.py, programs/program_generator.py)
  - Duplicate logic: parse stdout for `✓ Submitted: ...` JSON payload.
- Verification loop wrappers (synthetic/generator.py, validate_synthetic.py, programs/program_generator.py)
  - Duplicate logic: run teacher trace(s), compare hashes, log failures.
- Filters (scattered: profiler gates, question viability, program output filters)
  - Duplicate logic: skip IDs/near-unique columns, missingness/variance issues, tiny groups, non-informative outputs.

Centralizing these into shared modules gives us one contract for each stage and reduces accidental divergence.

### Dataset name + description
- Current: Each pipeline re-derives dataset name and loads meta.json sidecars in slightly different ways.
- Overlap: same logic, different missing-description behavior and naming conventions.
- Consolidation: `shared/dataset_meta.py` returns `(dataset_name, dataset_description)` from a CSV path.
  - dataset_name: if filename is `data.csv`, use parent folder name; otherwise use the file stem.
  - dataset_description: read from `meta.json` (same folder) or `{dataset}.meta.json` using `description`/`subtitle`/`title` keys; empty string if missing.
  - If description is empty, LLM question generation should synthesize a short description from `data_overview` instead of skipping the dataset.
  - Persist synthesized descriptions alongside generated questions (for reproducibility) and do not overwrite source `meta.json` files.

### Question loading / schema parsing
- Current: LLM and synthetic questions are loaded with separate parsers, each tolerating different JSON shapes.
- Overlap: both read JSON and fill defaults in slightly different ways.
- Consolidation: `shared/questions_io.py` loads the unified schema and fails fast if the file does not match the contract.
  - Contract: `load_questions()` returns a list of QuestionRecord or raises validation errors.
  

### Submission parsing
- Current: `parse_submitted_answer` exists in core env; synthetic/program pipelines parse stdout separately.
- Overlap: all parse the same `✓ Submitted: {...}` format.
- Consolidation: `shared/submission.py` with one parser used everywhere, enforcing the protocol wrapper.

### Verification wrappers
- Current: verification logic lives in multiple files with small variations.
- Overlap: same call pattern to `execute_teacher_trace` with different evidence strategies.
- Consolidation: `shared/verification.py` wraps the call and standardizes the output structure.
  - `verify_synthetic()` = one teacher trace + compare to ground truth (use hint if present).
  - `verify_llm()` = consistency verification (gold with hint + N no-hint traces).
  - Both return `VerificationResult` so callers do not re-implement comparisons.

### Filters
- Current: profiler has dataset filters; program pipeline has its own filters.
- Overlap: same “obvious junk” rules, applied in different places.
- Consolidation: `shared/filters.py` for dataset viability + question viability, with program output filters documented as a separate subsection.

Intentional separations to keep:

- LLM exploration flow
- Template selection logic (domain-specific; keep local, but follow shared execution/filter schema)
- Program sampling logic

---

## 6) Target architecture (single backbone)

### Backbone stages

```
Source (LLM | Template | Program)
  → Execution (only for synthetic)
  → Filter (synthetic-only: profiler gates + program output filters)
  - Same DataProfiler for template + program; LLM uses verification as its filter
  → Mechanical Question (always for synthetic)
  → Verbalization (optional)
  → Verification (correctness gate)
  - Verification = episode generation (LLM: consistency verification; Synthetic: ground-truth verification)

```

### How each source fits

**LLM**
- Emits `question_text` directly (already verbalized)
- No mechanical question or ground truth
- Prompt context uses dataset_description (optional) + data_overview (programmatic summary)

**Synthetic (template + program)**
- Always emits `question_mechanical`
- Optional verbalizer writes `question_text` + `hint`
- Ground truth stored for verification
- Hints are optional; verification uses them when present but does not require them as a pre-verification step

---

## 7) Unified question schema (single record)

One schema, with optional fields depending on source:

Required (all sources):
- `id`
- `source`: `synthetic` | `llm`
- `subtype`: `template` | `program` | `llm`
- `dataset`

Required for synthetic:
- `question_mechanical`
- `code`
- `code_hash`
- `ground_truth`
- `ground_truth_hash`
- `output_schema`
- `n_steps` (program chain length or template-defined)

Required for LLM:
- `question_text`

Optional:
- `question_text` (synthetic if verbalized)
- `hint`
- `difficulty` (LLM)
- `n_steps` (LLM if provided)

Notes:
- Mechanical is canonical for synthetic.
- Verbalizer reads mechanical + dataset profile and writes back to the same record.

---

## 8) Verbalization policy

- Synthetic verbalized questions should avoid explicit column names; use role-based language (target column, grouping column, etc.).
- LLM questions may include column names when they arise naturally during exploration.
- Hints are optional; for synthetic, keep them high-level and avoid explicit column names.
- Verbalizer may mention exclusions if the code already excludes ID-like columns.
- Verbalized questions should avoid procedural wording ("calculate", "group by", "use pandas", etc.).
  - Future refinement: if the lint fails, re-prompt the verbalizer with lint feedback or fall back to mechanical.

---

## 9) Filters (minimal, profiler‑based)

Filters should be centralized and lenient at first (drop only obvious junk). They apply to synthetic only; LLM questions are filtered later by verification.

Filter scopes (synthetic):

1) Dataset viability (profiler-based)
   - Minimum rows/columns (for example rows >= 50, cols >= 2)
   - Not all columns are >= 95% missing
   - At least one eligible numeric or categorical column after ID-like exclusion

2) Column eligibility (profiler-based)
   - Exclude ID-like columns (name patterns + sequential integers + >= 98% unique)
   - Exclude columns with missing_pct >= 95
   - Exclude columns with unique_count <= 1
   - For group-based templates, require moderate cardinality and avoid tiny groups

3) Verbalized question viability (synthetic only)
   - No method terms (reuse `FORBIDDEN_METHOD_TERMS`)
   - No explicit column names
   - Keep prompts concise (<= 3 sentences)

4) Program output filters (program pipeline)
   - Must submit an answer; no NaN or empty answers
   - Minimum row count for scalar stats (for example >= 30 rows)
   - p-value threshold for group-diff programs (for example <= 0.05)
   - Unique winners for ranking outputs (no ties)
   - If too many pass, keep top-K by deterministic interestingness score

Thresholds are parameters, not hard-coded in the contract. Always log drop reasons for tuning.

---

## 10) Manifest / caching policy

Purpose: avoid re‑execution and re‑verbalization when nothing changed.

Recommendations:
- **Synthetic (template/program)** fingerprint = `code_hash + dataset_hash`
- Verbalization cache: separate entry keyed by `code_hash` to avoid re‑verbalizing if mechanical is unchanged
- Record failures for expensive stages (verbalization, verification)
- Allow `--retry-failed` to re‑run entries previously marked as failure

Notes:
- Mechanical generation is cheap; caching is most valuable for verbalization and verification

---

## 11) Difficulty metadata

We are deferring difficulty labeling until we have error data.

Temporary rule:
- `n_steps = chain length` for program‑based questions

---

## 12) CLI integration

- Add a **CLI flag** for procedural questions
- Program questions should flow through the **same episode pipeline** as templates

---

## 13) Consolidation plan (file‑level)
This plan corresponds directly to the overlap map in Section 5.

### New shared modules
- `src/datagen/shared/dataset_meta.py` (dataset name + description)
- `src/datagen/shared/questions_io.py` (load/save unified schema)
- `src/datagen/shared/submission.py` (parse submit output)
- `src/datagen/shared/filters.py` (central minimal filters)
- `src/datagen/shared/verification.py` (small wrapper around verification)

### Modify
- `src/datagen/synthetic/generator.py`: generate mechanical only + optional verbalize
- `src/datagen/validate_synthetic.py`: single synthetic verification entrypoint
- `src/datagen/question_gen.py` / `episode_gen.py`: use shared loaders
- `src/datagen/synthetic/programs/program_generator.py`: adapt to shared schema or fold into synthetic generator

### Deprecate (eventually remove)
- Local `parse_submissions` helpers in multiple files
- Duplicated dataset metadata loaders

---

## 14) Testing (minimal acceptance)

Minimum checks per change:

1) Generate 1 template question → load via unified schema
2) Generate 1 program question → load via unified schema
3) Run 1 synthetic verification trace
4) LLM question load still works

These are smoke tests to prevent regressions while refactoring.

---

## 15) Open decisions

1) Where should program questions be saved?
   - Recommendation: `data/questions_synthetic/` with subtype = program
   - Alternative: `data/questions_programs/`

2) Default verification question for synthetic:
   - Mechanical by default (current preference)
   - Verbalized if available

3) Strict vs lenient filters:
   - Start lenient, log drop reasons
   - Tighten later if needed

4) Manifest failures:
   - Record failures for verbalization/verification (expensive steps) so we can skip or `--retry-failed` intentionally

---

## 16) Decision log (current)

- Program generation is **first‑class** and **synthetic** (subtype = program).
- Mechanical questions are canonical for synthetic; verbalization is optional.
- LLM questions are already verbalized; no mechanical form.
- Verification is a separate step.
- Synthetic verbalized questions should avoid explicit column names.
- Hints are optional; no pre-verification student validation gate.
- Dataset descriptions are optional; missing descriptions should be synthesized from data_overview for LLM generation.
- Filters should be centralized (profiler-based) and initially lenient.

---

## 17) Concrete JSON examples

### Synthetic question (template or program)

```json
{
  "id": "syn_001_titanic_groupby_mean",
  "source": "synthetic",
  "subtype": "program",
  "dataset": "titanic",
  "question_mechanical": "group by 'Pclass', compute mean of 'Fare', return max group",
  "question_text": "Which passenger class paid the highest average fare?",
  "hint": "Consider grouping passengers by their class.",
  "code": "df.groupby('Pclass')['Fare'].mean().idxmax()",
  "code_hash": "a3f9c1...",
  "ground_truth": 1,
  "ground_truth_hash": "c4ca42...",
  "output_schema": "scalar:int",
  "n_steps": 3
}
```

### LLM question

```json
{
  "id": "llm_042_imdb_sentiment",
  "source": "llm",
  "subtype": "llm",
  "dataset": "imdb_reviews",
  "question_text": "What percentage of reviews are positive?",
  "hint": null,
  "difficulty": "medium",
  "n_steps": 2
}
```

### Minimal synthetic (no verbalization)

```json
{
  "id": "syn_002_iris_filter",
  "source": "synthetic",
  "subtype": "template",
  "dataset": "iris",
  "question_mechanical": "filter rows where 'sepal_length' > 5.0, count",
  "question_text": null,
  "code": "len(df[df['sepal_length'] > 5.0])",
  "code_hash": "b7f3d2...",
  "ground_truth": 118,
  "ground_truth_hash": "e3a7b1...",
  "output_schema": "scalar:int",
  "n_steps": 2
}
```

---

## 18) Function signatures for shared modules

### `src/datagen/shared/dataset_meta.py`

```python
def load_dataset_meta(csv_path: str) -> tuple[str, str]:
    """
    Load dataset name and description from a CSV path.

    Dataset name:
    - If filename is `data.csv`, use the parent folder name.
    - Otherwise use the file stem.

    Description lookup:
    - `meta.json` in the same folder, else `{dataset}.meta.json`
    - Fields: `description`, `subtitle`, `title` (first non-empty)
    - Empty string if unavailable

    If description is empty, LLM question generation should synthesize a short
    description from `data_overview` rather than skipping the dataset.

    Args:
        csv_path: Absolute path to the CSV file.
    
    Returns:
        (dataset_name, dataset_description)
    """
    ...
```

### `src/datagen/shared/questions_io.py`

```python
from typing import TypedDict, Literal

class QuestionRecord(TypedDict, total=False):
    id: str
    source: Literal["synthetic", "llm"]
    subtype: Literal["template", "program", "llm"]
    dataset: str
    question_text: str | None
    question_mechanical: str | None
    hint: str | None
    code: str | None
    code_hash: str | None
    ground_truth: Any
    ground_truth_hash: str | None
    output_schema: str | None
    n_steps: int | None
    difficulty: str | None

def load_questions(path: str) -> list[QuestionRecord]:
    """
    Load questions from JSON file in the unified schema.
    Fail fast if the file does not match the schema (no legacy normalization).

    Works for both LLM and synthetic question files (unified schema).
    """
    ...

def save_questions(questions: list[QuestionRecord], path: str) -> None:
    """
    Save questions to JSON file in unified schema.
    """
    ...

def validate_question(q: QuestionRecord) -> list[str]:
    """
    Return list of validation errors (empty if valid).
    Validation = schema/contract checks (required fields + source-specific fields).

    Checks:
    - Required fields present
    - Source-specific fields present (mechanical/code/ground_truth for synthetic, question_text for LLM)
    """
    ...
```

### `src/datagen/shared/submission.py`

```python
def parse_submission(stdout: str) -> tuple[Any, bool]:
    """
    Parse submitted answer from execution stdout.

    Looks for "✓ Submitted: {...}" pattern and expects the protocol wrapper
    (for example `{ "__csv_agent_answer__": value, "hooks": [...] }`).

    Args:
        stdout: Raw stdout from code execution.
    
    Returns:
        (submission_dict, success)
        success=False if no submission found or parse error.
    """
    ...
```

### `src/datagen/shared/filters.py`

```python
from dataclasses import dataclass

@dataclass
class FilterResult:
    passed: bool
    reason: str | None  # None if passed, explanation if dropped

def apply_filters(
    question: QuestionRecord,
    profile: dict,  # profiler output for the dataset
) -> FilterResult:
    """
    Apply minimal 'obvious junk' filters for synthetic questions.

    Scopes:
    - Dataset viability (min rows/cols, not all missing, has eligible columns)
    - Column eligibility (ID-like/near-unique, missing_pct >= 95, unique_count <= 1)
    - Verbalized question viability (no method terms, no column names, <= 3 sentences)

    Program output filters live in `synthetic/programs/filter.py` and are
    documented separately; shared filters should surface common drop reasons.

    Returns FilterResult with a drop reason if applicable.
    """
    ...

def log_drop(question_id: str, reason: str) -> None:
    """Log dropped question for later analysis."""
    Used to audit filter impact and tune thresholds without losing data on drop reasons.
    ...
```

### `src/datagen/shared/verification.py`

```python
from dataclasses import dataclass
from typing import Literal

@dataclass
class VerificationResult:
    success: bool
    match: bool | None  # None if execution failed
    trace: dict | None  # single trace for ground-truth verification
    traces: list[dict]  # multiple traces for consistency verification
    majority_answer_hash: str | None
    error: str | None

def verify_question(
    question: QuestionRecord,
    csv_path: str,
    strategy: Literal["ground_truth", "consistency"],
    n_traces: int = 1,
) -> VerificationResult:
    """
    Run verification for a question.

    - ground_truth: run one (or n) teacher trace(s) and compare to ground truth
    - consistency: run n teacher traces and require majority agreement

    Hints are optional. In consistency mode, use hint for the gold trace when present.
    """
    ...

def verify_synthetic(
    question: QuestionRecord,
    csv_path: str,
) -> VerificationResult:
    """Convenience wrapper for ground-truth verification (synthetic)."""
    ...

def verify_llm(
    question: QuestionRecord,
    csv_path: str,
    n_traces: int = 3,
) -> VerificationResult:
    """Convenience wrapper for consistency verification (LLM)."""
    ...
```

---

## 19) Dependency graph (implementation order)

```
shared/dataset_meta.py (no deps)
         │
         ▼
shared/questions_io.py (imports dataset_meta for validation)
         │
         ▼
shared/submission.py (no deps, pure parsing)
         │
         ▼
shared/filters.py (imports profiler)
         │
         ▼
shared/verification.py (imports teacher.py, submission.py)
         │
         ▼
synthetic/generator.py (uses all shared modules)
         │
         ▼
episode_gen.py (uses shared/verification.py, shared/questions_io.py)
```

**Recommended order:**
1. `dataset_meta.py` - zero deps, quick win
2. `questions_io.py` - defines the schema, enables testing
3. `submission.py` - pure function, easy to extract
4. `filters.py` - needs profiler, but self-contained
5. `verification.py` - depends on teacher, most integration

---

## 20) Before/after code snippets

### Dataset metadata loading

**Before (duplicated in 5 files):**

```python
# question_gen.py
dataset_name = Path(csv_path).stem
desc_path = Path(csv_path).with_suffix('.json')
if desc_path.exists():
    desc = json.load(open(desc_path))['description']
else:
    desc = ""

# episode_gen.py
csv_name = os.path.basename(csv_path).replace('.csv', '')
meta_path = csv_path.replace('.csv', '_meta.json')
...

# synthetic/generator.py
name = csv_path.split('/')[-1].split('.')[0]
...
```

**After (single source of truth):**

```python
# All files now do:
from datagen.shared.dataset_meta import load_dataset_meta

dataset_name, dataset_description = load_dataset_meta(csv_path)
```

### Submission parsing

**Before (duplicated):**

```python
# core/environment.py
match = re.search(r'✓ Submitted: (.+)$', stdout, re.MULTILINE)
if match:
    try:
        return json.loads(match.group(1))
    except:
        return match.group(1)

# synthetic/generator.py
for line in stdout.split('\n'):
    if 'Submitted:' in line:
        raw = line.split('Submitted:')[1].strip()
        ...
```

**After:**

```python
# All files now do:
from datagen.shared.submission import parse_submission

answer, success = parse_submission(stdout)
if not success:
    # handle failure
```

### Question loading

**Before:**

```python
# episode_gen.py
with open(questions_path) as f:
    questions = json.load(f)
if isinstance(questions, dict):
    questions = [questions]
for q in questions:
    q.setdefault('hint', None)
    ...

# validate_synthetic.py
data = json.load(open(path))
questions = data if isinstance(data, list) else [data]
...
```

**After:**

```python
# All files now do:
from datagen.shared.questions_io import load_questions

questions = load_questions(path)  # returns list, validated (fail-fast)
```

---

## 21) Failure handling rules

### Verbalization failure

```
Condition: Verbalizer LLM call fails or returns empty/invalid output

Action:
1. Keep the mechanical question (it's always valid)
2. Set question_text = None
3. Log to manifest: { "stage": "verbalization", "error": "<message>", "timestamp": ... }
4. Question is still usable for verification (mechanical is canonical)
```

### Verification failure

```
Condition: Execution times out, crashes, or produces no submission

Action:
1. Mark in manifest: { "stage": "verification", "error": "<message>", "question_id": "..." }
2. Do NOT re-run automatically on next batch
3. Use --retry-failed to explicitly re-attempt failed verifications
```

### Lint failure (procedural wording detected)

```
Condition: Post-verbalization lint detects forbidden terms

Action:
1. Option A (default): Log warning, keep verbalization anyway
2. Option B (strict): Re-prompt verbalizer with lint feedback
3. Option C (fallback): Discard verbalization, use mechanical only

Current recommendation: Start with Option A, tighten later.
```

### Manifest entry format

```json
{
  "question_id": "syn_001_...",
  "stages": {
    "execution": { "status": "ok", "timestamp": "2024-01-15T..." },
    "verbalization": { "status": "failed", "error": "timeout", "timestamp": "..." },
    "verification": { "status": "skipped" }
  },
  "code_hash": "a3f9c1...",
  "dataset_hash": "b7d2e4..."
}
```

### `--retry-failed` behavior

```
When --retry-failed is passed:
1. Load manifest
2. Find entries where any stage has status = "failed"
3. Re-run only those entries, starting from the failed stage
4. Update manifest with new results

This avoids re-running successful entries while giving explicit
control over retrying failures.
```

---

## 22) Quick reference: what goes where

| Task | Module | Notes |
|------|--------|-------|
| Get dataset name + description | `shared/dataset_meta.py` | From CSV path |
| Load/save questions | `shared/questions_io.py` | Unified schema |
| Parse submitted answer | `shared/submission.py` | From stdout |
| Apply junk filters | `shared/filters.py` | Profiler-based |
| Run verification trace | `shared/verification.py` | Wraps teacher |
| Generate templates | `synthetic/generator.py` | Local logic |
| Generate programs | `synthetic/programs/` | Local logic |
| LLM question gen | `question_gen.py` | Local logic |
| Episode generation | `episode_gen.py` | Uses shared/verification |

---

## 23) Executive summary (concrete, specific)

This is the shortest “do‑this” version of the plan.

### What we will unify
- **Dataset meta loading**: `question_gen.py`, `episode_gen.py`, `synthetic/generator.py`, `validate_synthetic.py`, `synthetic/programs/program_generator.py`
- **Question IO + schema enforcement (fail-fast)**: `episode_gen.py`, `validate_synthetic.py`
- **Submission parsing**: `core/environment.py`, `synthetic/generator.py`, `synthetic/programs/program_generator.py`
- **Verification wrappers**: `synthetic/generator.py`, `validate_synthetic.py`, `synthetic/programs/program_generator.py`
- **Minimal filters**: profiler + program filters

### What stays separate
- LLM exploration (`question_gen.py`)
- Template selection logic (`synthetic/templates.py`)
- Program sampling/grammar (`synthetic/programs/*`)

### Files to add
- `src/datagen/shared/dataset_meta.py`
- `src/datagen/shared/questions_io.py`
- `src/datagen/shared/submission.py`
- `src/datagen/shared/filters.py`
- `src/datagen/shared/verification.py`

### Files to modify (first pass)
- `src/datagen/synthetic/generator.py` (mechanical only + optional verbalize)
- `src/datagen/validate_synthetic.py` (single synthetic verification entrypoint)
- `src/datagen/episode_gen.py` (use shared loaders)
- `src/datagen/question_gen.py` (use shared dataset meta)
- `src/datagen/synthetic/programs/program_generator.py` (use shared schema + parser)

### Outputs by source
- **LLM**: `data/questions/` (already verbalized)
- **Synthetic (template + program)**: `data/questions_synthetic/` with `subtype=template|program`

---

## 24) Concrete overlaps (file + function level)

### Dataset name + description
- `src/datagen/question_gen.py`: derives dataset name, loads meta.json
- `src/datagen/episode_gen.py`: derives dataset name, loads meta.json
- `src/datagen/validate_synthetic.py`: `load_dataset_description`
- `src/datagen/synthetic/generator.py`: loads meta.json
- `src/datagen/synthetic/programs/program_generator.py`: loads meta.json

**Shared contract**: `load_dataset_meta(csv_path) -> (dataset_name, dataset_description)`

### Question loading / schema parsing
- `src/datagen/episode_gen.py`: `load_questions`
- `src/datagen/validate_synthetic.py`: `load_questions`

**Shared contract**: `load_questions(path) -> list[QuestionRecord]` (fail-fast unified schema)

### Submission parsing
- `src/core/environment.py`: `parse_submitted_answer`
- `src/datagen/synthetic/generator.py`: `_parse_all_submissions`
- `src/datagen/synthetic/programs/program_generator.py`: `_parse_submissions`

**Shared contract**: `parse_submission(stdout) -> (value, success)`

### Verification wrappers
- `src/datagen/validate_synthetic.py`: single-trace verification
- `src/datagen/synthetic/programs/program_generator.py`: verification loop

**Shared contract**: `verify_synthetic(question, csv_path) -> VerificationResult`

### Filters
- `src/datagen/synthetic/profiler.py`: dataset viability + column heuristics
- `src/datagen/synthetic/programs/filter.py`: program‑level filters

**Shared contract**: `apply_filters(question, profile) -> FilterResult`

---

## 25) Unified question record (minimal, explicit)

This is the minimal schema we will enforce across sources.

Required for all:
- `id`, `source`, `subtype`, `dataset`

Required for synthetic:
- `question_mechanical`, `output_schema`, `code`, `code_hash`, `ground_truth`, `ground_truth_hash`, `n_steps`

Required for LLM:
- `question_text`

Optional:
- `question_text` (synthetic if verbalized)
- `hint`
- `difficulty` (LLM)
- `n_steps` (LLM if provided)

---

## 26) Mechanical vs verbalized (explicit rules)

- **LLM**: only `question_text` exists (already verbalized)
- **Synthetic**: always `question_mechanical`
- **Verbalization**: optional; if it exists, it writes `question_text` + `hint` into the same record
- **Verification default**: synthetic uses mechanical unless a flag says otherwise

---

## 27) Filters (exact scope)

Filters apply **only to synthetic** prior to verbalization:

1) Dataset viability (min rows/cols, not all missing, at least one eligible column)
2) Column eligibility (ID-like/near-unique, missing_pct >= 95, unique_count <= 1, moderate cardinality for group tasks)
3) Verbalized question viability (no method terms, no explicit column names, <= 3 sentences)
4) Program output filters (no NaN/empty, min rows for scalar stats, p-value threshold, unique winners, top-K by interestingness)

LLM questions are not filtered here; they are filtered by consistency verification.

---

## 28) Manifest policy (explicit)

### What is cached
- Synthetic execution results keyed by `code_hash + dataset_hash`
- Verbalization results keyed by `code_hash`

### What is recorded as failure
- Verbalization failure (LLM error, empty output)
- Verification failure (trace fails or answer mismatch)

### How `--retry-failed` works
- Re‑run only entries with `status=failed` in the manifest
- Do not re‑run successes

---

## 29) Implementation order (thin‑slice)

1) Add `shared/dataset_meta.py` and replace callers
   - Acceptance: all three sources still load dataset description

2) Add `shared/submission.py` and replace parsers
   - Acceptance: synthetic + program pipelines still parse submit output

3) Add `shared/questions_io.py` and adapt loaders
   - Acceptance: `episode_gen.py` + `validate_synthetic.py` load unified schema and fail fast on legacy

4) Add `shared/filters.py`
   - Acceptance: synthetic generation logs filter drops, no new failures

5) Add `shared/verification.py`
   - Acceptance: `validate_synthetic.py` runs via shared verification helper

6) Wire program questions into synthetic generation path + CLI flag
   - Acceptance: program questions appear in `data/questions_synthetic/` with subtype

---

## 30) Risks / assumptions (so you don’t have to read code)

- Question schema differences across sources are real; unified schema is required and legacy files will be rejected.
- LLM consistency verification uses container pool; synthetic uses local env; keep separation.
- Dataset descriptions may be missing; LLM question generation should synthesize a short description from data_overview instead of skipping.
- Some “before/after” snippets above are illustrative; actual refactor should use real paths/logic.
- Verbalization is slow; caching matters most there.

---

## 34) Brainstorm additions (proposed enhancements)

These are optional but high‑leverage ideas to improve quality, diversity, and robustness. Add/remove as needed.

### Verification / correctness
- **Single verification entrypoint** with two evidence modes:
  - `compare_ground_truth=True` (synthetic) → majority match to ground truth
  - `compare_ground_truth=False` (LLM) → majority agreement across traces
- **Adaptive traces**: start with `n=1`, add traces only if disagreement appears
- **Confidence score**: agreement ratio or stability score, not just pass/fail
- **Synthetic “stress”**: optionally run >1 trace to catch fragile questions

### Question quality (data‑aware)
- Reject questions about **ID‑like columns** (mean/variance/count on IDs)
- Reject **near‑constant numeric** columns
- Reject **tiny / heavily imbalanced groups** for comparisons
- Reject **heavy missingness** targets
- Add **column name heuristics** (units, dates, IDs) for sanity checks
- Store **drop reason codes** for later tuning

### Verbalization
- Two‑pass verbalization:
  1) Generate question + hint without explicit column names (role-based)
  2) Check for procedural phrasing; re‑prompt only if needed
- Allow exclusion language ("excluding ID‑like columns") if code does it

### Diversity controls
- Program signature dedupe (op chain + column roles + output schema)
- Text similarity dedupe for verbalized questions (avoid near‑duplicates)
- Diversity metrics per dataset (unique templates, unique ops, etc.)

### Pipeline artifacts + caching
- Persist stage artifacts:
  1) program spec
  2) execution record
  3) question record
  4) verification record
- Cache at stage boundaries:
  - execution cache (`code_hash + dataset_hash`)
  - verbalization cache (`code_hash`)
  - verification cache (`question_id + model`)

### Observability
- Log counts in/out per stage
- Log timings per stage
- Log filter drop reasons

### Testing
- Minimal smoke tests per stage (1 question per source)
- End-to-end: one synthetic + one LLM episode passes through verification

---

## 31) Decision update: no legacy schema support

We will **not** normalize or support old question formats. Once the new schema is in place, we will regenerate all questions.

Implications:
- `questions_io.load_questions()` should **fail fast** if the file is not in the new schema.
- No migration adapters or legacy parsing layers.
- Simpler contracts, fewer edge cases.

---

## 32) Verification vs consistency (clarified)

These are the same stage (correctness gate), but use different evidence:

- **Synthetic verification**: compare one teacher run to known ground truth (single trace).
- **LLM consistency verification**: no ground truth, so compare multiple traces and require majority agreement.

We should expose a single verification API with two strategies:

- `strategy="ground_truth"` (synthetic)
- `strategy="consistency"` (LLM, N traces)

So it is conceptually the same stage, with different inputs and call counts.

---

## 33) Unified verification API (single entrypoint)

We can expose one verification function that handles both cases:

```python
from typing import Literal

def verify_question(
    question: QuestionRecord,
    csv_path: str,
    n_traces: int,
    strategy: Literal["ground_truth", "consistency"],
) -> VerificationResult:
    ...
```

Behavior:

- If `strategy == "ground_truth"`:
  - Run `n_traces` teacher traces
  - Compare each trace to ground truth
  - Decide success by majority match to ground truth (n=1 for synthetic default)

- If `strategy == "consistency"`:
  - Run `n_traces` teacher traces
  - Decide success by majority agreement across traces

This gives a single API with two modes and lets us use the same verification machinery for both LLM and synthetic, while preserving the intended difference in evidence.
