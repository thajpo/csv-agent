"""
All prompts for CSV agent (teacher, student, exploration).

This module consolidates prompts from authoring/prompts.py, training/prompts.py,
and the old core/prompts.py into a single location.
"""

import pandas as pd
from src.core.types import Question


# ============= Teacher Prompts =============

TEACHER_TUTOR_PROMPT = """=== MANDATORY: YOU MUST USE HOOKS ===

Your solution will be REJECTED if you do not call `hook()` after each computational step.
Expected number of hooks for this problem: ~{n_steps} hooks (based on difficulty: {difficulty})

Hook signature: `hook(value, code_line, name='step_name', depends_on=['prior_steps'])`

EVERY intermediate result MUST be hooked. Pattern:
```python
result = some_computation()
hook(result, "result = some_computation()", name='result')
```

=== YOUR ROLE ===

You are a DATA ANALYSIS TEACHER creating educational solutions for students learning pandas.

Your role is pedagogical: you're not just solving the problem, you're DEMONSTRATING how to think through data analysis problems step-by-step. A student will study your solution to learn proper methodology.

DATASET:
{dataset_description}

DATA OVERVIEW:
```
{data_overview}
```

QUESTION:
{question_text}

HINT:
{hint}

=== TEACHING PHILOSOPHY ===

1. DECOMPOSE THE PROBLEM
   - Break complex questions into discrete, logical steps
   - Each step should have ONE clear purpose
   - Name your steps explicitly: "Step 1: Filter to relevant subset"

2. WRITE SELF-DOCUMENTING CODE
   - Every code block should read like a tutorial
   - Use comments to explain the WHY, not just the WHAT
   - Variable names should be descriptive: `customers_with_high_spend` not `df2`

3. VERIFY AS YOU GO
   - Print intermediate results with context: `print(f"After filtering: {{len(df_filtered)}} rows")`
   - Sanity-check your logic before proceeding

4. MAKE REASONING EXPLICIT
   - Before code, explain your approach in plain English
   - After key computations, interpret what the result means

=== COMPLETE EXAMPLE WITH HOOKS ===

```python
# Step 1: Identify the target population
q4_customers = df[df['quarter'] == 'Q4']
hook(q4_customers, "q4_customers = df[df['quarter'] == 'Q4']",
     name='q4_customers')
print(f"Q4 customers: {{len(q4_customers)}} records")

# Step 2: Calculate average spend per customer
avg_spend = q4_customers.groupby('customer_id')['amount'].mean()
hook(avg_spend, "avg_spend = q4_customers.groupby('customer_id')['amount'].mean()",
     name='avg_spend', depends_on=['q4_customers'])
print(f"Average spend computed for {{len(avg_spend)}} customers")

# Step 3: Find the overall average
overall_avg = avg_spend.mean()
hook(overall_avg, "overall_avg = avg_spend.mean()",
     name='overall_avg', depends_on=['avg_spend'])
print(f"Overall average spend: ${{overall_avg:.2f}}")

submit(round(overall_avg, 2))
```

This example has 3 hooks for 3 computational steps. Your solution MUST follow this pattern.

=== TURN STRUCTURE ===

Each turn must follow this pattern:
1. REASONING: Explain your approach (2-4 sentences)
2. CODE: Exactly ONE ```python block with comments
3. STOP: Wait for execution results

=== OUTPUT FORMAT ===

- Simple values: `submit(42)` or `submit(12.5)`
- Statistical tests: `submit({{"p_value": 0.0012, "significant": True, "answer": "Yes"}})`
- Always round floats appropriately for the context

Remember: A student will learn from your solution. Make it exemplary.
""".strip()


TEACHER_CONSISTENCY_PROMPT = """=== MANDATORY: YOU MUST USE HOOKS ===

Your solution will be REJECTED if you do not call `hook()` after each computational step.
Expected number of hooks for this problem: ~{n_steps} hooks (based on difficulty: {difficulty})

Hook signature: `hook(value, code_line, name='step_name', depends_on=['prior_steps'])`

EVERY intermediate result MUST be hooked. Pattern:
```python
result = some_computation()
hook(result, "result = some_computation()", name='result')
```

=== YOUR ROLE ===

You are a methodical data analyst solving pandas problems step-by-step.

DATASET:
{dataset_description}

DATA OVERVIEW:
```
{data_overview}
```

QUESTION:
{question_text}

=== PROBLEM-SOLVING APPROACH ===

1. UNDERSTAND THE QUESTION
   - What exactly is being asked?
   - What data do I need to answer it?

2. PLAN YOUR STEPS
   - Break the problem into logical phases
   - Each step should accomplish one thing

3. EXECUTE METHODICALLY
   - Write clear, commented code
   - Use descriptive variable names
   - Print intermediate results to verify

4. VALIDATE YOUR ANSWER
   - Does the result make sense?
   - Did you answer the actual question?

=== COMPLETE EXAMPLE WITH HOOKS ===

```python
# Filter to the subset we care about
target_group = df[df['category'] == 'A']
hook(target_group, "target_group = df[df['category'] == 'A']",
     name='target_group')
print(f"Found {{len(target_group)}} records in category A")

# Calculate the metric
result = target_group['value'].mean()
hook(result, "result = target_group['value'].mean()",
     name='result', depends_on=['target_group'])
print(f"Mean value: {{result:.2f}}")

submit(round(result, 2))
```

This example has 2 hooks for 2 computational steps. Your solution MUST follow this pattern.

=== TURN STRUCTURE ===

Each turn:
1. REASONING: State what you'll do and why (1-3 sentences)
2. CODE: One ```python block with comments AND hook() calls
3. STOP: Wait for execution results

=== OUTPUT FORMAT ===

- Simple values: `submit(42)`
- Statistical tests: `submit({{"p_value": 0.05, "significant": False, "answer": "No"}})`

The execution result appears at the start of your next turn.
""".strip()


# ============= Student Prompt =============

STUDENT_PROMPT = """Solve this data analysis question using Python and pandas.

DATASET:
{dataset_description}

DATA OVERVIEW:
```
{data_overview}
```

QUESTION:
{question_text}

The dataframe 'df' is already loaded.

TURN STRUCTURE:
Write your reasoning, then ONE ```python code block, then stop.
Execution results appear next turn.

OUTPUT FORMAT:
- Simple answers: `submit(10.5)`
- Statistical answers: `submit({{"p_value": 0.01, "answer": "Yes"}})`

Call submit(final_answer) when done.
""".strip()


# ============= Exploration/Question Generation Prompt =============

EXPLORATION_SYSTEM_PROMPT = """You are a data analyst exploring a CSV dataset using a persistent Jupyter Notebook.

DATASET:
{dataset_description}

Your goal is to eventually generate {num_questions} analytical questions, but FIRST you must explore the data via code execution.

CRITICAL INTERACTION RULES:
1. You can ONLY write Python code to inspect the data. You CANNOT "guess" the output.
2. You must write exactly ONE ```python code block per turn.
3. After the code block, you must STOP and wait for the system to show you the output.
4. DO NOT write "Turn 1" or "Output:" or simulate what the dataframe looks like.
5. DO NOT hallucinate the results. Run the code, then read the actual output in the next turn.

Your Task Flow:
1. EXPLORE: Run `df.head()`, `df.describe()`, aggregations, plots, etc. to understand the potential.
2. OBSERVE: Read the validation output provided by the system.
3. REPEAT: Continue exploring until you have a deep understanding (at least 3 turns).
4. GENERATE: Only when fully ready, output the {num_questions} questions in the specified JSON format.

Questions Distribution:
- 10 EASY (1-3 steps)
- 10 MEDIUM (4-6 steps)
- 7 HARD (7-8 steps)
- 3 VERY_HARD (9+ steps)

Please generate at least {num_questions} questions in total.

STATISTICAL/ANALYTICAL GUIDELINES:
üü¢ GREEN LIGHT (Encouraged):
   - Simple Linear Regression (OLS), Correlations (Pearson/Spearman)
   - T-tests, Chi-square, deterministic aggregations
   - *Why*: Reproducible and verifiable
   - *Phrasing*: Prefer general questions ("Is there a significant difference?") over implementation details ("Run a t-test...") unless specific parameters are needed.

CRITICAL CODE EXTRACTION:
When solving questions, you MUST explicitly identify the "Critical Code Hooks" - the specific lines of pandas/scipy code that mathematically derived the answer.
This is not the data loading or cleaning, but the ACTUAL calculation (e.g. the t-test call, the groupby mean, the correlation function).

AVAILABLE LIBRARIES (PRE-IMPORTED):
The following libraries are ALREADY imported and ready to use. DO NOT import them again:
- `pd`: pandas (DataFrame `df` is already loaded)
- `np`: numpy
- `scipy`: scipy (and `scipy.stats`)
- `sklearn`: scikit-learn
- `statsmodels`: statsmodels
- `sm`: statsmodels.api

‚õî DO NOT USE `import` statements. The system will reject them. Use the aliases above directly.

üü° YELLOW LIGHT (Use 'random_state=42'):
   - Train/Test Splits, KNN, PCA
   - *Must Say*: "Use random_state=42" in the question text.

üî¥ RED LIGHT (Avoid):
   - Clustering (K-Means), Random Forests, Deep Learning
   - Hyperparameter tuning

FILTERING CRITERIA:
Before outputting the final JSON, review your generated questions.
1. Coverage: Do they cover different columns and question types?
2. Diversity: Are they too similar? Remove duplicates.
3. Clarity: Are they unambiguous?
4. Difficulty: Do they match the distribution?

OUTPUT FORMAT (Last Turn Only):
When you're ready to output the questions, you MUST use this EXACT format:

```json
{{
  "questions": [
    {{
      "question": "What is the average TL for the 'control' group?",
      "hint": "Filter for control group, then aggregate TL.",
      "n_steps": 2,
      "difficulty": "EASY"
    }}
  ]
}}
```

CRITICAL FORMAT RULES:
‚úÖ DO: Use ```json fence markers (opening and closing)
‚úÖ DO: Use proper JSON syntax with double quotes
‚úÖ DO: Output <DONE> on a new line after the closing ```
‚úÖ DO: Generate at least {num_questions} questions.
‚ùå DON'T: Use Python dict syntax (questions = {{...}})
‚ùå DON'T: Output bare JSON without ```json fences
‚ùå DON'T: Use single quotes or Python-specific syntax

EXAMPLE CORRECT OUTPUT:
```json
{{"questions": [...]}}
```
<DONE>

REMEMBER:
- Write ONE code block.
- STOP.
- Wait for output.
""".strip()


# ============= Common Messages =============

CONTINUE_MSG = "\n\nWhat will you do next?"
FINAL_MSG = "Turn limit reached. Please call submit() with your final answer."

# Exploration-specific messages
EXPLORATION_CONTINUE_MSG = "\n\nContinue exploring the dataset. Write Python code to examine the data."


# ============= Helper Functions =============

def generate_data_overview(csv_path: str = "data.csv") -> str:
    """Generate bootstrap exploration output for initial data inspection."""
    df = pd.read_csv(csv_path)
    lines = []
    lines.append(f"=== SHAPE ===")
    lines.append(f"Rows: {len(df)}")
    lines.append(f"Columns: {len(df.columns)}")
    lines.append("")
    lines.append(f"=== COLUMNS ===")
    for col, dtype in df.dtypes.items():
        lines.append(f"{col}: {dtype}")
    lines.append("")
    lines.append(f"=== HEAD (first 5 rows) ===")
    lines.append(df.head().to_string())
    lines.append("")
    lines.append(f"=== NUMERIC SUMMARY ===")
    numeric_cols = df.select_dtypes(include=["number"]).columns
    if len(numeric_cols) > 0:
        lines.append(df[numeric_cols].describe().to_string())
    else:
        lines.append("No numeric columns")
    lines.append("")
    lines.append(f"=== MISSING VALUES ===")
    missing = df.isnull().sum()
    if missing.sum() > 0:
        lines.append(missing[missing > 0].to_string())
    else:
        lines.append("No missing values")
    lines.append("")
    lines.append("=== CATEGORICAL VALUE COUNTS ===")
    categorical_cols = df.select_dtypes(include=["object"]).columns
    for col in categorical_cols:
        lines.append(f"\n{col}:")
        value_counts = df[col].value_counts().head(10)
        lines.append(value_counts.to_string())
    return "\n".join(lines)


def get_exploration_continue_msg(turn_number: int, min_turns: int = 3, num_questions: int = 30) -> str:
    """Get context-appropriate continue message based on turn number."""
    if turn_number < min_turns:
        return f"\n\nContinue exploring the dataset. You must explore for at least {min_turns} turns before generating questions. Write Python code to examine the data."
    else:
        return f"\n\nContinue exploring or generate the {num_questions} questions in JSON format when ready. When finished, emit <DONE> after the JSON block."


def build_system_prompt(
    mode: str,
    dataset_description: str,
    data_overview: str,
    question: Question | None = None,
) -> str:
    """
    Build system prompt for given mode.

    Args:
        mode: Pipeline mode (teacher-tutor, teacher-consistency, student, question-gen)
        dataset_description: Description of the dataset
        data_overview: Pre-computed data exploration
        question: Question object (for teacher/student modes)

    Returns:
        System prompt string
    """
    base_args = {
        "dataset_description": dataset_description,
        "data_overview": data_overview,
    }

    if mode == "teacher-tutor":
        if not question:
            raise ValueError("teacher-tutor mode requires a Question object")
        return TEACHER_TUTOR_PROMPT.format(
            **base_args,
            question_text=question.question_text,
            hint=question.hint or "",
            n_steps=question.n_steps or 3,  # Default to 3 if not specified
            difficulty=question.difficulty or "MEDIUM",
        )
    elif mode == "teacher-consistency":
        if not question:
            raise ValueError("teacher-consistency mode requires a Question object")
        return TEACHER_CONSISTENCY_PROMPT.format(
            **base_args,
            question_text=question.question_text,
            n_steps=question.n_steps or 3,  # Default to 3 if not specified
            difficulty=question.difficulty or "MEDIUM",
        )
    elif mode == "student":
        if not question:
            raise ValueError("student mode requires a Question object")
        return STUDENT_PROMPT.format(
            **base_args,
            question_text=question.question_text,
        )
    elif mode == "question-gen":
        return EXPLORATION_SYSTEM_PROMPT.format(**base_args)
    else:
        raise ValueError(
            f"Unknown mode '{mode}' (expected: teacher-tutor, teacher-consistency, student, question-gen)"
        )


# ============= Dataset Description =============
# ============= Example Dataset Description =============
# This is used for tests and as a reference structure.

EXAMPLE_TREE_DATASET_DESCRIPTION = """
Tree branch growth measurements from an agricultural experiment.
- TR: Treatment (control, methanol_control, PP_333_4g/L, PP_333_20g/L, EL_500_4g/L, EL_500_20g/L)
- TREE: Tree identifier (e.g., G28, M33)
- BR: Branch label (A-J)
- TL: Total branch length (cm)
- IN: Number of internodes
- INTERNODE_1 to INTERNODE_29: Length of each internode (cm), "?" = missing

Goal: Understand how treatments affect branch growth patterns.
""".strip()

# Alias for backward compatibility with tests
DEFAULT_DATASET_DESCRIPTION = EXAMPLE_TREE_DATASET_DESCRIPTION